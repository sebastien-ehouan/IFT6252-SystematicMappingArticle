"Title","Authors","Link","Year","Source title","Abstract","Author Keywords","Sponsors","Publisher","Conference name","Conference date","Conference location","Conference code","Document Type","Source"
"Variable provenance in software systems","Chittimalli P.K., Naik R.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942523680&doi=10.1145%2f2593822.2593826&partnerID=40&md5=610dad3ee13ceb61a12498e10dee2b9b",2014,"4th International Workshop on Recommendation Systems for Software Engineering, RSSE 2014 - Proceedings","Data Provenance is defined as lineage or history of the given dataitem. Knowing the source of the data or the transformation of the data-source to compute the given data is critical for analyzing the quality of the data. Many transformations of data are done in software (source code). We introduce and define the concept of Variable Provenance for source code. We argue that determining the origin(s) of the data held by a variable and the history of modifications of the variable can provide critical information along many dimensions about what happens in the source code. We use understanding of source code and creating business rules from source code as use-cases to illustrate our view-point. To compute the variable provenance, we combine program slicing techniques and operational rules associated with mathematical operators in computations to propagate the annotations. We predict that the solution to the problem of variable provenance can lead to many use-cases in the software engineering community, effective discovery of processes and business rules from existing systems, and powerful development, debugging and evolution techniques for the software industry. Â© 2014 ACM.","Business rules; Data dictionary; Program analysis; Program comprehension; Provenance","ACM Special Interest Group on Software Engineering (SIGSOFT);IEEE Computer Society's Tech. Council on Software Engin. (TCSE)","Association for Computing Machinery, Inc","4th International Workshop on Recommendation Systems for Software Engineering, RSSE 2014","3 June 2014",,113495,"Conference Paper","Scopus"
"Towards standardized evaluation of developer-assistance tools","Proksch S., Amann S., Mezini M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942546579&doi=10.1145%2f2593822.2593827&partnerID=40&md5=ae67c01393afc41abbd59c65601d6a36",2014,"4th International Workshop on Recommendation Systems for Software Engineering, RSSE 2014 - Proceedings","Over the last years, researchers proposed a variety of assistance tools to support developers in their development environments. Many of the respective publications introduce new evaluation strategies or use custom datasets. Size and quality of the performed evaluations differ. Additionally, the strategies often use metrics that are tailored to the respective tools. As a result, comparing different assistance tools is very dificult. In this work, we present a framework for the standardized evaluation of assistance tools, on the example of code recommenders. The framework combines different ideas and demands from previous work. Furthermore, we discuss how the community could jointly realize the framework. Â© 2014 ACM.","Content Assistance Tools; Evaluation; Integrated Development Environment; Repository; Standardization","ACM Special Interest Group on Software Engineering (SIGSOFT);IEEE Computer Society's Tech. Council on Software Engin. (TCSE)","Association for Computing Machinery, Inc","4th International Workshop on Recommendation Systems for Software Engineering, RSSE 2014","3 June 2014",,113495,"Conference Paper","Scopus"
"Code recommendation based on a degree-of-interest model","Murakami N., Masuhara H., Aotani T.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942522119&doi=10.1145%2f2593822.2593828&partnerID=40&md5=2f44711decd8e6c1ff5ecc49e0d6b483",2014,"4th International Workshop on Recommendation Systems for Software Engineering, RSSE 2014 - Proceedings","Code recommendation systems predict and present what the user is likely to write next by using the user's editing context, namely textual and semantic information about the programs being edited in a programming editor. Most existing systems however use information merely around the cursor position|i.e., the class/method definition at the cursor position|as the editing context. By including the code related to the current method/class, like the callers and callees of the method, recommendation could become more appropriate. We propose to use the user's editing activity for identifying code relevant to the current method/class. Specifically, we use a modified degree-of-interest model in the Mylyn task management tool, and incorporated the model in our repository-based code recommendation system, Selene. This paper reports the design of the system and the results of our initial experiments. Â© 2014 ACM.","Keyword search; Similarity","ACM Special Interest Group on Software Engineering (SIGSOFT);IEEE Computer Society's Tech. Council on Software Engin. (TCSE)","Association for Computing Machinery, Inc","4th International Workshop on Recommendation Systems for Software Engineering, RSSE 2014","3 June 2014",,113495,"Conference Paper","Scopus"
"Automated support for human resource allocation in software process by cluster analysis","Santos T.J.A., Lima A.M., Reis C.A.L., Reis R.Q.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931039265&doi=10.1145%2f2593822.2593830&partnerID=40&md5=64d84a0ae6669c08581907751804d215",2014,"4th International Workshop on Recommendation Systems for Software Engineering, RSSE 2014 - Proceedings","It is widely recognized the potential of using organizational data analysis to enable automated tools supporting process management task. The organizational repositories should be used in an active way to accordingly support dynamic decision-making process in software project management. In this paper, we briefly describe a research aiming to support the human resource allocation process in the software process context based on the analysis of organizational repositories. It intends to provide an organizational data analysis as a mean to take empirical evidence to perform fact-based decisions upon historical and ongoing organizational experiences. As the work is in its beginning, we also present some differences from other already existing approaches and the main challenges to be overcome through completion of this work. Â© 2014 ACM.","Data mining; Human resource management; Mining software repositories; Software process","ACM Special Interest Group on Software Engineering (SIGSOFT);IEEE Computer Society's Tech. Council on Software Engin. (TCSE)","Association for Computing Machinery, Inc","4th International Workshop on Recommendation Systems for Software Engineering, RSSE 2014","3 June 2014",,113495,"Conference Paper","Scopus"
"Using developer conversations to resolve uncertainty in software development: A position paper","Mashiyat A.S., Famelis M., Salay R., Chechik M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942523733&doi=10.1145%2f2593822.2593823&partnerID=40&md5=e6d37daf5beed638968410355399d743",2014,"4th International Workshop on Recommendation Systems for Software Engineering, RSSE 2014 - Proceedings","Software development is a social process: Tasks such as implementing a requirement or fixing a bug typically spark conversations between the stakeholders of a software project, where they identify points of uncertainty in the solution space and explore proposals to resolve them. Due to the fluid nature of these interactions, it is hard for project man- Agers to maintain an overall understanding of the state of the discussion and to know when and how to intervene. We pro- pose an approach for extracting the uncertainty information from developer conversations in order to provide managers with analytics. Using these allows us to recommend specificactions that managers can take to better facilitate the resolution of uncertainty. Â© 2014 ACM.","Natural language processing; Uncertainty management","ACM Special Interest Group on Software Engineering (SIGSOFT);IEEE Computer Society's Tech. Council on Software Engin. (TCSE)","Association for Computing Machinery, Inc","4th International Workshop on Recommendation Systems for Software Engineering, RSSE 2014","3 June 2014",,113495,"Conference Paper","Scopus"
"Towards a visualized code recommendation for APIs enriched with specification mining","Ghafari M., Heydarnoori A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942520430&doi=10.1145%2f2593822.2593825&partnerID=40&md5=45c58a2eb4d083804594c459c851d907",2014,"4th International Workshop on Recommendation Systems for Software Engineering, RSSE 2014 - Proceedings","This paper positions an idea for an interactive code recommendation system. In this work, candidate recommendations are abstracted as a graph-based visualization of the API usages that are decorated with the API specifications and the usage rules mined from the unit test cases of the given API and its usage examples. The user can then progressively explore this graph to obtain her desired code with- out delving into the implementation details. Â© 2014 ACM.","Code recommendation; Specification; Unit test; Visualization","ACM Special Interest Group on Software Engineering (SIGSOFT);IEEE Computer Society's Tech. Council on Software Engin. (TCSE)","Association for Computing Machinery, Inc","4th International Workshop on Recommendation Systems for Software Engineering, RSSE 2014","3 June 2014",,113495,"Conference Paper","Scopus"
"Recommending a starting point for a programming task: An initial investigation","Thompson C.A., Murphy G.C.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942522608&doi=10.1145%2f2593822.2593824&partnerID=40&md5=1c0a905392f77cca1aaf8a80736481d1",2014,"4th International Workshop on Recommendation Systems for Software Engineering, RSSE 2014 - Proceedings","When starting a new task, a software developer must typically find one or more starting points amongst many re- sources (e.g., source code and other files) forming the soft- ware system. In this paper, we consider how we might recommend one resource as an initial starting point, saving the developer the effort of having to search or use other means to find the point. Using data from the open source Eclipse Mylyn project, we investigate whether resources considered and changed for other tasks may be used to recommend a starting point for a current task. Â© 2014 ACM.","Program history; Repository mining; Task similarity","ACM Special Interest Group on Software Engineering (SIGSOFT);IEEE Computer Society's Tech. Council on Software Engin. (TCSE)","Association for Computing Machinery, Inc","4th International Workshop on Recommendation Systems for Software Engineering, RSSE 2014","3 June 2014",,113495,"Conference Paper","Scopus"
"Recommendation system to enhance planning of software development using R","Rao J.J., Kelappan R., Pallath P.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942513344&doi=10.1145%2f2593822.2593831&partnerID=40&md5=16a3e2e17d64df18e75ece2e344da239",2014,"4th International Workshop on Recommendation Systems for Software Engineering, RSSE 2014 - Proceedings","Today, planning phase of software development is very important since it involves crucial decision making on resource allocation, task management and delivery timelines. It is extremely important for employees and administrative decision makers (managers) to understand the data (e.g., his- Toric software artifacts, resource availability) and arrive at a decision. We try to apply data mining concepts like regression, classification and hybrid approaches to effectively plan the future allocations using open source R-scripts. These results could be correlated to recommend the best practices to be implemented, thus helping the management to gain better visibility. These custom R-scripts could be eficiently reused in the future to predict the further planning of the task. Our recommendation system aims at providing which- Task-to-whom feature with the help of proposed automated estimation process for planning of software development. Â© 2014 ACM.","Artificial Intelligence; Data Mining; Predictive Analysis; R-script; Recommendation System; Software Development; Software Planning","ACM Special Interest Group on Software Engineering (SIGSOFT);IEEE Computer Society's Tech. Council on Software Engin. (TCSE)","Association for Computing Machinery, Inc","4th International Workshop on Recommendation Systems for Software Engineering, RSSE 2014","3 June 2014",,113495,"Conference Paper","Scopus"
"Recommending process improvement package using direct and indirect relationships of activities","Choi S., Kim D.-K., Park S., Lee J., Park S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942521950&doi=10.1145%2f2593822.2593829&partnerID=40&md5=ffaaff370558e58bceb8a3474c91d700",2014,"4th International Workshop on Recommendation Systems for Software Engineering, RSSE 2014 - Proceedings","Software process improvement (SPI) has been widely prac- Ticed in various domains. SPI uses a process reference model (e.g., CMMI) for planning improvement actions by identifying weaknesses and strengths of the current exercise. Identified findings are analyzed for their relationships to increase the synergy of improvement actions. However, the current practice is monotonic focusing on the identification of weaknesses and strengths. In this work, we present a CMMI- based recommendation method for analyzing correlations of assessment findings. In the model, we define a process correlation model capturing relationships of practices in CMMI. The model is then used for inferring relationships of given findings where findings are viewed as instances of CMMI practices. We take into account both direct and indirect relationships and analyze the precision and recall of the correlation model by different levels of relationship depth. We evaluate the method using industrial data and the results show the potential of the method. Â© 2014 ACM.","Knowledge transfer; Process correlation analysis; Process improvement planning; Software process improvement recommendation","ACM Special Interest Group on Software Engineering (SIGSOFT);IEEE Computer Society's Tech. Council on Software Engin. (TCSE)","Association for Computing Machinery, Inc","4th International Workshop on Recommendation Systems for Software Engineering, RSSE 2014","3 June 2014",,113495,"Conference Paper","Scopus"
"Annotation support for generic patches","Dotzler G., Veldema R., Philippsen M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864682962&doi=10.1109%2fRSSE.2012.6233400&partnerID=40&md5=6b12a61e2f7a1ad915eef83ffe41fcae",2012,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings","In large projects parallelization of existing programs or refactoring of source code is time consuming as well as error-prone and would benefit from tool support. However, existing automatic transformation systems are not extensively used because they either require tedious definitions of source code transformations or they lack general adaptability. In our approach, a programmer changes code inside a project, resulting in before and after source code versions. The difference (the generated transformation) is stored in a database. When presented with some arbitrary code, our tool mines the database to determine which of the generalized transformations possibly apply. Our system is different from a pure compiler based (semantics preserving) approach as we only suggest code modifications. Our contribution is a set of generalizing annotations that we have found by analyzing recurring patterns in open source projects. We show the usability of our system and the annotations by finding matches and applying generated transformations in real-world applications. Â© 2012 IEEE.","code-refactoring; optimizations; patches; programming tools",,,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012","4 June 2012 through 4 June 2012","Zurich",91695,"Conference Paper","Scopus"
"Recommendations as learning: From discrepancies to software improvement","Schneider K., GÃ¤rtner S., Wehrmaker T., BrÃ¼gge B.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864650189&doi=10.1109%2fRSSE.2012.6233405&partnerID=40&md5=d546b130128c49559ccf2068797b4e8d",2012,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings","Successful software development requires software engineering skills as well as domain and user knowledge. This knowledge is difficult to master. Increasing complexity and fast evolving technologies cause deficits in development and system behavior. They cause discrepancies between expectations and observations. We propose using discrepancies as a trigger for recommendations to developers. Discrepancies in using a software application are combined with discrepancies between development artifacts. To efficiently support software engineers, recommendations must consider knowledge bases of discrepancies and resolution options. They evolve over time along with evolving experience. Hence, recommendations and organizational learning are intertwined. Â© 2012 IEEE.","end-user feedback; heuristics; recommendation",,,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012","4 June 2012 through 4 June 2012","Zurich",91695,"Conference Paper","Scopus"
"Automatically detecting architecturally-relevant code anomalies","Arcoverde R., Macia I., Garcia A., Von Staa A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864705125&doi=10.1109%2fRSSE.2012.6233419&partnerID=40&md5=88b05eea5baf13789ea4c2ee7217f56c",2012,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings","Software architecture degradation is a longstanding problem in software engineering. Previous studies have shown that certain code anomalies - or patterns of code anomalies - are likely to be harmful to architecture design, although their identification is far from trivial. This study presents a system for not only detecting architecturally-relevant code anomalies, but also helping developers to prioritize their removal by ranking them. We detect code anomaly patterns based on static analysis that also exploit architecture information. Â© 2012 IEEE.","architectural problem; code anomaly; refactoring",,,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012","4 June 2012 through 4 June 2012","Zurich",91695,"Conference Paper","Scopus"
"A cost-benefit approach to recommending conflict resolution for parallel software development","Niu N., Yang F., Cheng J.-R.C., Reddivari S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864691696&doi=10.1109%2fRSSE.2012.6233403&partnerID=40&md5=520c5f98bad56abd5f5695a4dd275388",2012,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings","Merging parallel versions of source code is a common and essential activity during the lifespan of large-scale software systems. When a non-trivial number of conflicts is detected, there is a need to support the maintainer in investigating and resolving these conflicts. In this paper, we contribute a cost-benefit approach to ranking the conflicting software entities by leveraging both structural and semantic information of the source code. We present a study by applying our approach to a legacy system developed by computational scientists. The study not only demonstrates the feasibility of our approach, but also sheds light on the future development of conflict resolution recommenders. Â© 2012 IEEE.","conflict resolution; cost-benefit analysis; recommendation; software merging",,,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012","4 June 2012 through 4 June 2012","Zurich",91695,"Conference Paper","Scopus"
"Harnessing Stack Overflow for the IDE","Bacchelli A., Ponzanelli L., Lanza M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864646113&doi=10.1109%2fRSSE.2012.6233404&partnerID=40&md5=53e7e9b50e9200f38c9f964565a4e234",2012,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings","Developers often consult online tutorials and message boards to find solutions to their programming issues. Among the many online resources, Question & Answer websites are gaining popularity. This is no wonder if we consider a case like Stack Overflow, where more than 92% questions on expert topics are answered in a median time of 11 minutes. This new resource has scarcely been acknowledged by any Integrated Development Environment (IDE): Even though developers spend a large part of their working time in IDEs, and the usage of Q&A services has dramatically increased, developers can only use such resources using external applications. We introduce Seahawk, an Eclipse plugin to integrate Stack Overflow crowd knowledge in the IDE. It allows developers to seamlessly access Stack Overflow data, thus obtaining answers without switching the context. We present our preliminary work on Seahawk: It allows users to (1) retrieve Q&A from Stack Overflow, (2) link relevant discussions to any source code in Eclipse, and (3) attach explanative comments to the links. Â© 2012 IEEE.","Q&A websites; Seahawk; Stack Overflow",,,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012","4 June 2012 through 4 June 2012","Zurich",91695,"Conference Paper","Scopus"
"Interaction histories mining for software change guide","Kobayashi T., Kato N., Agusa K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864660209&doi=10.1109%2fRSSE.2012.6233415&partnerID=40&md5=d173e21e00cb4e310abadde340c21a60",2012,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings","This paper presents a prediction model for change propagation based on the developers' interaction history. Since artifacts have internal and external dependencies, a change will cause some changes on related artifacts. In order to guide change operations in software development, our proposed method generates a change guide graph by mining developers' interaction histories which consist of write and read accesses to artifacts. Using a change guide graph, we can guide change using the context of previous changes. To evaluate proposed change guide method, we perform a case study with an open-source software. We show that the context information is effective for file level and method level change predictions. Â© 2012 IEEE.",,,,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012","4 June 2012 through 4 June 2012","Zurich",91695,"Conference Paper","Scopus"
"Example overflow: Using social media for code recommendation","Zagalsky A., Barzilay O., Yehudai A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864720675&doi=10.1109%2fRSSE.2012.6233407&partnerID=40&md5=0901e0604cd500bb43521d0e769cd272",2012,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings","Modern Q&A websites, such as Stack Overflow, use social media to provide concise answers, and offer rich technical context with quality assessment capabilities. Although some of the answers may include executable code snippets, they are entangled in free text and are not easily extracted. Q&A websites are not designed for such direct code reuse. We present Example Overflow, a code search and recommendation tool which brings together social media and code recommendation systems. Example Overflow enables crowd-sourced software development by utilizing both textual and social information, which accompany source code on the Web. Its browsing mechanism minimizes the context switch associated with other code search tools. In this paper we describe the development of the tool, provide preliminary evaluation, and discuss its contribution to an example centric programming paradigm. Â© 2012 IEEE.","code repository; code search; crowd-sourced software development; example centric programming; example embedding; example overflow; social recommendations; stack overflow",,,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012","4 June 2012 through 4 June 2012","Zurich",91695,"Conference Paper","Scopus"
"Actionable identification of emergent teams in software development virtual organizations","Valetto G., Blincoe K., Goggins S.P.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864656981&doi=10.1109%2fRSSE.2012.6233401&partnerID=40&md5=587e60679569691067440af671413770",2012,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings","We present a method for identifying emergent teams of developers who need to work together and coordinate, within larger software development organizations. Our goal is to identify these socio-technical constructs as they emerge, so that we can provide timely awareness and actionable recommendations to managers, technical leads and members of the emergent team alike. Our technique is rooted in the analysis of Social Networks, which are constructed from real-time traces of the activity of each individual developer within her development environment, contextualized with respect to her assigned tasks and the corresponding artifact working set. Â© 2012 IEEE.","developers' coordination; Emergent teams; IDE interactions; Social Network Analysis; task contexts",,,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012","4 June 2012 through 4 June 2012","Zurich",91695,"Conference Paper","Scopus"
"Recommending mentors to software project newcomers","Steinmacher I., Wiese I.S., Gerosa M.A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864720070&doi=10.1109%2fRSSE.2012.6233413&partnerID=40&md5=cafdebe50e55ee3539aa11919475dbf8",2012,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings","Open Source Software projects success depends on the continuous influx of newcomers and their contributions. Newcomers play an important role as they are the potential future developers, but they face difficulties and obstacles when initiating their interaction with a project, resulting in a high amount of withdrawals. This paper presents a recommendation system aiming to support newcomers finding the most appropriate project member to mentor them in a technical task. The proposed system uses temporal and social aspects of developer's behavior, in addition to recent contextual information to recommend the most suitable mentor at the moment. Â© 2012 IEEE.","mentor recommendation; newcomers; open source software; recommendation system",,,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012","4 June 2012 through 4 June 2012","Zurich",91695,"Conference Paper","Scopus"
"Group recommendation algorithms for requirements prioritization","Felfernig A., Ninaus G.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864717777&doi=10.1109%2fRSSE.2012.6233412&partnerID=40&md5=b5730d40b1cabac4225d2ef96f888a52",2012,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings","Group recommendation is successfully applied in different domains such as Interactive Television, Ambient Intelligence, and e-Tourism. The focus of this paper is to analyze the applicability of group recommendation to requirements prioritization. We provide an overview of relevant group recommendation heuristics and report the results of an empirical study which focused on the analysis of the prediction quality of these heuristics. Â© 2012 IEEE.","Group Recommendation; Recommender Systems; Requirements Engineering; Requirements Prioritization",,,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012","4 June 2012 through 4 June 2012","Zurich",91695,"Conference Paper","Scopus"
"Recommendation system for design patterns in software development: An DPR overview","Palma F., Farzin H., GuÃ©hÃ©neuc Y.-G., Moha N.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864668269&doi=10.1109%2fRSSE.2012.6233399&partnerID=40&md5=d526860fc49860be856302353dac8b6f",2012,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings","Software maintenance can become monotonous and expensive due to ignorance and misapplication of appropriate design patterns during the early phases of design and development. To have a good and reusable system, designers and developers must be aware of large information set and many quality concerns, e.g., design patterns. Systems with correct design pattern may ensure easy maintenance and evolution. However, without assistance, designing and development of software systems following certain design patterns is difficult for engineers. Recommendation systems for software engineering can assist designers and developers with a wide range of activities including suggesting design patterns. With the help of pattern recommenders, designers can come up with a reusable design. We provide a Design Pattern Recommender (DPR) process overview for software design to suggest design patterns, based on a simple Goal-Question-Metric (GQM) approach. Our prototype provides two-fold solution. In the primary-level, DPR only proposes one or more design patterns for a problem context, and in the secondary level, for a initial set of design, DPR refactors models and suggests design patterns. Our preliminary evaluation shows that DPR has a good trade-off between accuracy and procedural complexity, comparing to other state-of-the-art approaches. Â© 2012 IEEE.","Design pattern; Recommendation system; Software reuse",,,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012","4 June 2012 through 4 June 2012","Zurich",91695,"Conference Paper","Scopus"
"A comparison of recommender systems for mashup composition","Cremonesi P., Picozzi M., Matera M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864658224&doi=10.1109%2fRSSE.2012.6233411&partnerID=40&md5=98c09c055ccf941f950299b45b0cd3c6",2012,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings","Web mashups are a new generation of applications created by composing contents and functions available through Web services and APIs. A central activity in mashup development is the retrieval and selection of components to be included in the composition. The adoption of recommender systems can alleviate some of the difficulties arising in this activity. Based on the results of an empirical study, this paper tries to shed light on the application of recommender systems to the mashup composition domain, and discusses the performance of different recommendation systems when applied to a very large collection of mashups and mashup components. Â© 2012 IEEE.","APIs; Recommender systems; Web mashups",,,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012","4 June 2012 through 4 June 2012","Zurich",91695,"Conference Paper","Scopus"
"Context-based recommendation to support problem solving in software development","Cordeiro J., Antunes B., Gomes P.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864699655&doi=10.1109%2fRSSE.2012.6233418&partnerID=40&md5=3be5d46e3e840fd9a694baa45ff1069b",2012,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings","During the software development process, developers are often faced with problem solving situations. For instance, it is common the occurrence of exceptions, that originate stack traces in the Console View of the IDE. These situations motivate the developer to use the Web to search for information. However, there is a gap between the IDE and the Web, requiring developers to spend significant time searching for relevant information and navigating through web pages in a Web browser. We propose to process the information of exception stack traces and retrieve question-answering web resources to help developers. We developed a tool that integrates recommendation of question/answer web resources in Eclipse, according to the context of these exception stack traces. The results of a preliminary experimentation are promising, showing that our approach performs better than a simple keyword-based approach. Â© 2012 IEEE.","Context Modelling; Problem Solving; Recommendation Systems; Software Development",,,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012","4 June 2012 through 4 June 2012","Zurich",91695,"Conference Paper","Scopus"
"Combining activity metrics and contribution topics for software recommendations","Christidis K., Paraskevopoulos F., Panagiotou D., Mentzas G.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864678135&doi=10.1109%2fRSSE.2012.6233408&partnerID=40&md5=8a627e2ac5838445be087dbecdcf8f7d",2012,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings","In this paper we outline work in progress for the development of a recommender system for open source software development communities that takes into account information from multiple sources. Specifically our approach combines latent semantics of contributed information artifacts with quantitative metrics that indicate developer activity. Â© 2012 IEEE.","Recommender System; Semantic Analysis; Software Development; Topic Models",,,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012","4 June 2012 through 4 June 2012","Zurich",91695,"Conference Paper","Scopus"
"Context-aware recommender systems for non-functional requirements","Danylenko A., LÃ¶we W.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864706752&doi=10.1109%2fRSSE.2012.6233417&partnerID=40&md5=6882b409011f6ec6e933e18cf49b46a0",2012,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings","For large software projects, system designers have to adhere to a significant number of functional and non-functional requirements, which makes software development a complex engineering task. If these requirements change during the development process, complexity even increases. In this paper, we suggest recommendation systems based on context-aware composition to enable a system designer to postpone and automate decisions regarding efficiency non-functional requirements, such as performance, and focus on the design of the core functionality of the system instead. Context-aware composition suggests the optimal component variants of a system for different static contexts (e.g., software and hardware environment) or even different dynamic contexts (e.g., actual parameters and resource utilization). Thus, an efficiency non-functional requirement can be automatically optimized statically or dynamically by providing possible component variants. Such a recommender system reduces time and effort spent on manually developing optimal applications that adapts to different (static or dynamic) contexts and even changes thereof. Â© 2012 IEEE.","context-aware composition; context-aware recommender systems; nonfunctional requirements",,,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012","4 June 2012 through 4 June 2012","Zurich",91695,"Conference Paper","Scopus"
"Connecting User Stories and code for test development","LandhÃ¤uÃŸer M., Genaid A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864720289&doi=10.1109%2fRSSE.2012.6233406&partnerID=40&md5=5eb8d7de4c55d3608d3086af74be31dd",2012,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings","User Stories are short feature descriptions from the user's point of view. Functional tests ensure that the feature described by a User Story is fully implemented. We present a tool that builds an ontology for code and links completed User Stories in natural language with the related code artifacts. The ontology also contains links to API components that were used to implement the functional tests. Preliminary results show that these links can be used to recommend reusable test steps for new User Stories. Â© 2012 IEEE.","code mining; functional testing; ontology; reasoning; traceability",,,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012","4 June 2012 through 4 June 2012","Zurich",91695,"Conference Paper","Scopus"
"Extending recommendation systems with software maps","TrÃ¼mper J., DÃ¶llner J.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864685249&doi=10.1109%2fRSSE.2012.6233420&partnerID=40&md5=c8c2dc0cdc0abbc9d31b0b06a66096f1",2012,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings","In practice, recommendation systems have evolved as helpful tools to facilitate and optimize software engineering processes. Serving both developers and managers, specific recommendation systems address their individual problems. Yet, in a number of cases complementing them with other techniques can enhance their use and extend their scope. In this paper, we first discuss different perspectives on software-engineering processes and examples of recommendation systems that support representatives of these perspectives. We then identify how select software-map techniques can extend recommendation systems to facilitate decision making by addressing the perspectives' information and communication needs. Â© 2012 IEEE.","Computer aided analysis; Context; Decision making; Visualization",,,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012","4 June 2012 through 4 June 2012","Zurich",91695,"Conference Paper","Scopus"
"Optimizing a search-based code recommendation system","Murakami N., Masuhara H.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864700684&doi=10.1109%2fRSSE.2012.6233414&partnerID=40&md5=f826a93ac894c2c799aa8ca3a767e21b",2012,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings","Search-based code recommendation systems with a large-scale code repository can provide the programmers example code snippets that teach them not only names in application programming interface of libraries and frameworks, but also practical usages consisting of multiple steps. However, it is not easy to optimize such systems because usefulness of recommended code is indirect and hard to be measured. We propose a method that mechanically evaluates usefulness for our recommendation system called Selene. By using the proposed method, we adjusted several search and user-interface parameters in Selene for better recall factor, and also learned characteristics of those parameters. Â© 2012 IEEE.","associative text search; example code recommendation; integrated development environment",,,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012","4 June 2012 through 4 June 2012","Zurich",91695,"Conference Paper","Scopus"
"Facilitating reuse in model-based development with context-dependent model element recommendations","Heinemann L.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864696086&doi=10.1109%2fRSSE.2012.6233402&partnerID=40&md5=5094f92e56f137fb4aa8ca72bb510a5a",2012,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings","Reuse recommendation systems suggest code entities useful for the task at hand within the IDE. Current approaches focus on code-based development. However, model-based development poses similar challenges to developers regarding the identification of useful elements in large and complex reusable modeling libraries. This paper proposes an approach for recommending library elements for domain specific languages. We instantiate the approach for Simulink models and evaluate it by recommending library blocks for a body of 165 Simulink files from a public repository. We compare two alternative variants for computing recommendations: association rules and collaborative filtering. Our results indicate that the collaborative filtering approach performs better and produces recommendations for Simulink models with satisfactory precision and recall. Â© 2012 IEEE.","data mining; model-based development; recommendation system; software reuse",,,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012","4 June 2012 through 4 June 2012","Zurich",91695,"Conference Paper","Scopus"
"Recommending relevant code artifacts for change requests using multiple predictors","Denninger O.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864646418&doi=10.1109%2fRSSE.2012.6233416&partnerID=40&md5=ae3c0a5ddee7a98438aa61aa5b2c7de1",2012,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings","Finding code artifacts affected by a given change request is a time-consuming process in large software systems. Various approaches have been proposed to automate this activity, e.g., based on information retrieval. The performance of a particular prediction approach often highly depends on attributes like coding style or writing style of change request. Thus, we propose to use multiple prediction approaches in combination with machine learning. First experiments show that machine learning is well suitable to weight different prediction approaches for individual software projects and hence improve prediction performance. Â© 2012 IEEE.","recommendation systems; software maintenance",,,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012","4 June 2012 through 4 June 2012","Zurich",91695,"Conference Paper","Scopus"
"Software feature model recommendations using data mining","Sayyad A.S., Ammar H., Menzies T.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864691697&doi=10.1109%2fRSSE.2012.6233409&partnerID=40&md5=ffb3e3fb5eaf153e2492bcedb3e17460",2012,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings","Feature Models are popular tools for describing software product lines. Analysis of feature models has traditionally focused on consistency checking (yielding a yes/no answer) and product selection assistance, interactive or offline. In this paper, we describe a novel approach to identify the most critical decisions in product selection/configuration by taking advantage of a large pool of randomly generated, generally inconsistent, product variants. Range Ranking, a data mining technique, is utilized to single out the most critical design choices, reducing the job of the human designer to making less consequential decisions. A large feature model is used as a case study; we show preliminary results of the new approach to illustrate its usefulness for practical product derivation. Â© 2012 IEEE.","design decisions; Feature Models; range ranking",,,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012","4 June 2012 through 4 June 2012","Zurich",91695,"Conference Paper","Scopus"
"Automated assessment of correctness of recommendation systems","Lozano A., Kellens A., Mens K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864718592&doi=10.1109%2fRSSE.2012.6233410&partnerID=40&md5=71fbe85feee6f3a776affca30c47fcaa",2012,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012 - Proceedings","Using a concrete example, this position paper makes a case for evaluating the correctness of software recommendation systems in an automated way, prior to conducting user studies, in order to assess the validity of the results and ideal configuration of the system to be evaluated. Â© 2012 IEEE.",,,,"2012 3rd International Workshop on Recommendation Systems for Software Engineering, RSSE 2012","4 June 2012 through 4 June 2012","Zurich",91695,"Conference Paper","Scopus"
"The weHelp reference architecture for community-driven recommender systems","Sheth S., Arora N., Murphy C., Kaiser G.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954962804&doi=10.1145%2f1808920.1808930&partnerID=40&md5=316875e6c1cfc12966ac1b6c7f28eaff",2010,"Proceedings - International Conference on Software Engineering","Recommender systems have become increasingly popular. Most research on recommender systems has focused on recommendation algorithms. There has been relatively little research, however, in the area of generalized system architectures for recommendation systems. In this paper, we introduce weHelp - a reference architecture for social recommender systems. Our architecture is designed to be application and domain agnostic, but we briey discuss here how it applies to recommender systems for software engineering. Copyright Â© 2010 ACM.","Recommender systems; Reference architecture","ACM SIG on Software Engineering (SIGSOFT);IEEE CS",,"2nd International Workshop on Recommendation Systems for Software Engineering, RSSE 2010, in Conjunction with the 32nd ACM/IEEE International Conference on Software Engineering, ICSE 2010","4 May 2010 through 4 May 2010","Cape Town",81248,"Conference Paper","Scopus"
"A recommendation framework for allocating global software teams in software product line projects","Pereira T.A.B., Dos Santos V.S., Ribeiro B.L., Elias G.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955002132&doi=10.1145%2f1808920.1808928&partnerID=40&md5=bb0dbc1c83c1e83eb4923cc9c96d05df",2010,"Proceedings - International Conference on Software Engineering","In order to improve software quality and reduce costs and deadlines, many companies are adopting Software Product Line approaches. As a consequence of globalization, another common practice is the adoption of Global Software Development approaches, which seek to find more qualified workforce and more attractive costs in companies distributed around the world. Taking into account the benefits of both approaches, the ramework proposed in this paper has the goal of aiding the management of global software teams involved in the implementation phase of an SPL project, providing recommendations on how to allocate the teams to the set of software components, which are initially specified in the SPL architecture and must be subsequently implemented. Copyright Â© 2010 ACM.","Global Software Development; Global software teams; Recommendation systems; Software Product Line","ACM SIG on Software Engineering (SIGSOFT);IEEE CS",,"2nd International Workshop on Recommendation Systems for Software Engineering, RSSE 2010, in Conjunction with the 32nd ACM/IEEE International Conference on Software Engineering, ICSE 2010","4 May 2010 through 4 May 2010","Cape Town",81248,"Conference Paper","Scopus"
"Recommendation and decision technologies for requirements engineering","Felfernig A., Schubert M., Mandl M., Ricci F., Maalej W.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954969560&doi=10.1145%2f1808920.1808923&partnerID=40&md5=582ea6e079a624d0fc37a31576a37c57",2010,"Proceedings - International Conference on Software Engineering","Requirements engineering (RE) is considered as one of the most critical phases in the software life-cycle, and poorly implemented RE processes are among the major risks for project failure. Stakeholders are often faced with the challenge that the complexity of information outstrips their capability to survey it and to decide about which requirements should be taken into account. Additionally, preferences regarding a set of requirements are typically not known before-hand but constructed within the scope of a decision making process. In this paper we introduce a simple application scenario and discuss recommendation and decision technologies which can be exploited for proactively supporting stakeholders in their decision making.",,"ACM SIG on Software Engineering (SIGSOFT);IEEE CS",,"2nd International Workshop on Recommendation Systems for Software Engineering, RSSE 2010, in Conjunction with the 32nd ACM/IEEE International Conference on Software Engineering, ICSE 2010","4 May 2010 through 4 May 2010","Cape Town",81248,"Conference Paper","Scopus"
"Recommending source code examples via API call usages and documentation","McMillan C., Poshyvanyk D., Grechanik M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954979703&doi=10.1145%2f1808920.1808925&partnerID=40&md5=1abd300afe525058a35677d7af2856c5",2010,"Proceedings - International Conference on Software Engineering","Online source code repositories contain software projects that already implement certain requirements that developers must fulfill. Programmers can reuse code from these existing projects if they can find relevant code without significant effort. We propose a new method to recommend source code examples to developers by querying against Application Programming Interface (API) calls and their documentations that are fused with structural information about the code. We conducted an empirical evaluation that suggests that our approach is lightweight and accurate. Copyright Â© 2010 ACM.",,"ACM SIG on Software Engineering (SIGSOFT);IEEE CS",,"2nd International Workshop on Recommendation Systems for Software Engineering, RSSE 2010, in Conjunction with the 32nd ACM/IEEE International Conference on Software Engineering, ICSE 2010","4 May 2010 through 4 May 2010","Cape Town",81248,"Conference Paper","Scopus"
"Towards a Better Code Completion system by API grouping, filtering, and popularity-based ranking","Hou D., Pletcher D.M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954978191&doi=10.1145%2f1808920.1808926&partnerID=40&md5=285e87671a31fc17d0bc1020f847a907",2010,"Proceedings - International Conference on Software Engineering","Nowadays, programmers spend much of their workday dealing with code libraries and frameworks that are bloated with APIs. One common way of interacting with APIs is through Code Completion inside the code editor. By default, Code Completion presents in a popup pane, in alphabetical order or by relevance, all accessible members available in the apparent type and supertypes of a receiver expression. This default behavior for Code Completion should and can be further improved because (1) not all public methods are APIs and presenting non-API public members to a programmer is misleading, (2) certain APIs are meant to be accessible only in some limited contexts but not others, (3) the alphabetical order separates otherwise logically related APIs, making it hard to see their connection and to work with, and (4) commonly used APIs are often presented long after much less used APIs due to suboptimal API sorting strategies. BCC (Better Code Completion) addresses these problems by enhancing Code Completion so that programmers can control how specific API elements should be sorted, filtered, and grouped. We report our preliminary validation results from testing BCC with Java projects that make use of the AWT/Swing APIs. For one large project, the BCC approach reduces by over ninety percent the total number of APIs that a programmer would have to scroll through using Eclipse's Code Completion before settling on the desired ones.",,"ACM SIG on Software Engineering (SIGSOFT);IEEE CS",,"2nd International Workshop on Recommendation Systems for Software Engineering, RSSE 2010, in Conjunction with the 32nd ACM/IEEE International Conference on Software Engineering, ICSE 2010","4 May 2010 through 4 May 2010","Cape Town",81248,"Conference Paper","Scopus"
"KAdvice: Infering synchronization patterns from an existing codebase","Schmidt A., Polze A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954976031&doi=10.1145%2f1808920.1808927&partnerID=40&md5=946554f43e78398178c53b9ae23cabcc",2010,"Proceedings - International Conference on Software Engineering","Operating system kernels are complex software systems. The kernels of todays mainstream OSs, such as Linux or Windows, are composed from a number of modules, which contain code and data. Even when providing synchronous interfaces (APIs) to the programmer, large portions of the OS kernel operate in an asynchronous manner. Synchronizing access to kernel data structures therefore is a central problem in OS kernels running on todays multicore and multiprocessor hardware. With the need to utilize future multi- and manycore processors, managing the synchronization problem becomes central to all multithreaded control-parallel applications. Since only little software is written from scratch, understanding the intended use of locking constructs and their relation to shared data structures will become critical to all programmers. Built upon our experiences with developing code inside the Windows kernel, we have developed the KAdvice approach, which helps to analyze locking structures in an existing code- base. KAdvice applies static analysis to call graphs and code dependencies to recommend appropriate locking patterns when accessing certain data structures. KAdvice has itself proven very useful in context of students' programming projects based upon the Windows Research Kernel (WRK). However, our approach is more general and applicable not only to OS kernels but to control-parallel software in general.","Data-flow analysis; KAdvice; Lock patterns","ACM SIG on Software Engineering (SIGSOFT);IEEE CS",,"2nd International Workshop on Recommendation Systems for Software Engineering, RSSE 2010, in Conjunction with the 32nd ACM/IEEE International Conference on Software Engineering, ICSE 2010","4 May 2010 through 4 May 2010","Cape Town",81248,"Conference Paper","Scopus"
"Component recommendation for cloud applications","Zheng Z., Lyu M.R.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954980766&doi=10.1145%2f1808920.1808931&partnerID=40&md5=f4bd0e5eebdba5f5645d0f98580c460f",2010,"Proceedings - International Conference on Software Engineering","Cloud computing is becoming popular nowadays. Building highly reliable cloud applications is a great challenge since the cloud applications are usually large-scale, complex, and include a lot of distributed components. In this paper, we propose a significant component recommendation framework for the cloud applications to attack this challenge. Our approach employs the component invocation relationship to compute significant values of components. The most significant components can be identified efficiently and effectively by our approach. Copyright Â© 2010 ACM.",,"ACM SIG on Software Engineering (SIGSOFT);IEEE CS",,"2nd International Workshop on Recommendation Systems for Software Engineering, RSSE 2010, in Conjunction with the 32nd ACM/IEEE International Conference on Software Engineering, ICSE 2010","4 May 2010 through 4 May 2010","Cape Town",81248,"Conference Paper","Scopus"
"Proposing software design recommendations based on component interface intersecting","Hummel O., Janjic W., Atkinson C.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954991359&doi=10.1145%2f1808920.1808936&partnerID=40&md5=bcf860f1ae8183a0c8969a7c91f28b46",2010,"Proceedings - International Conference on Software Engineering","The open source movement had a tremendous impact on software engineering in recent years. It not only established serious competition for many commercial software vendors, it also led to the availability of millions of source code artifacts on the Internet. For the time being there exists a fledgling community working on software search solutions and associated recommendation engines. However, the potential for reusing knowledge contained in internet-scale software repositories is far from being exhausted. While existing systems are limited to retrieving existing artifacts during the coding phase, in this position paper we propose a novel idea for determining the "" intersection"" of multiple similar artifacts that allows creating design recommendations for a developer even earlier in the development lifecycle.","CASE-tools; Data mining; Design recommendation; Software reuse","ACM SIG on Software Engineering (SIGSOFT);IEEE CS",,"2nd International Workshop on Recommendation Systems for Software Engineering, RSSE 2010, in Conjunction with the 32nd ACM/IEEE International Conference on Software Engineering, ICSE 2010","4 May 2010 through 4 May 2010","Cape Town",81248,"Conference Paper","Scopus"
"Recommending experts using communication history","Moraes A., Silva E., Da Trindade C., Barbosa Y., Meira S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955000618&doi=10.1145%2f1808920.1808929&partnerID=40&md5=c57c86c93731cad27da10d240de787c7",2010,"Proceedings - International Conference on Software Engineering","In distributed software development the communication is inefficient because of geographical and temporal distances, affecting the team's performance and awareness. The low level of awareness makes hard the task of finding the expert of a piece of source code, delaying the implementation whenever a developer needs help. To identify and to recommend the people with right knowledge to people in trouble during the implementation can improve the collaboration and awareness of the team because it can reduce the waiting time for an answer, since the expert can be contacted directly. In this paper we propose recommender system for expert location with the aim to reduce delays of finding the right person whenever somebody needs assistance during coding. Our approach uses the communication history of the project (the developer's mailing list) in addition to usual source code history. We also present results which show the practical potential of our approach. Copyright Â© 2010 ACM.","Distributed software development; Expert recommender system; Global software engineering; Knowledge management","ACM SIG on Software Engineering (SIGSOFT);IEEE CS",,"2nd International Workshop on Recommendation Systems for Software Engineering, RSSE 2010, in Conjunction with the 32nd ACM/IEEE International Conference on Software Engineering, ICSE 2010","4 May 2010 through 4 May 2010","Cape Town",81248,"Conference Paper","Scopus"
"Utilizing recommender systems to support software requirements elicitation","Carlos C.-H., Jane C.-H.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955004574&doi=10.1145%2f1808920.1808922&partnerID=40&md5=34b12d941ec30c82bfe457eb068abb70",2010,"Proceedings - International Conference on Software Engineering","Requirements Engineering involves a number of human intensive activities designed to help project stakeholders discover, analyze, and specify the functional and non-functional needs for a software intensive system. Recommender systems can support several different areas of this process including identifying potential subject matter experts for a topic, keeping individual stakeholders informed of relevant issues, and even recommending possible features for stakeholders to consider and explore. This position paper summarizes an extensive series of experiments that were conducted to identify best-of-breed algorithms for recommending forums to stakeholders and recommending unexplored topics to project managers. Copyright Â© 2010 ACM.","Recommender systems; Requirements elicitation; Stakeholders; Subject matter experts","ACM SIG on Software Engineering (SIGSOFT);IEEE CS",,"2nd International Workshop on Recommendation Systems for Software Engineering, RSSE 2010, in Conjunction with the 32nd ACM/IEEE International Conference on Software Engineering, ICSE 2010","4 May 2010 through 4 May 2010","Cape Town",81248,"Conference Paper","Scopus"
"On recommending meaningful names in source and UML","Kuhn A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954977922&doi=10.1145%2f1808920.1808932&partnerID=40&md5=1532b5eb5786bf05a05017b9a4841aee",2010,"Proceedings - International Conference on Software Engineering","Meaningful method names are crucial for the readability and maintainability of software. Existing naming conventions focus on syntactic details, leaving programmers with little or no support in choosing meaningful (domain) names. In this paper we propose to build a recommendation system that supports software developers and software architects when naming identifiers in source code as well as when naming elements in UML diagrams. We discuss related work, outline the design of such a recommendation system and discuss possible evaluation strategies. Copyright Â© 2010 ACM.",,"ACM SIG on Software Engineering (SIGSOFT);IEEE CS",,"2nd International Workshop on Recommendation Systems for Software Engineering, RSSE 2010, in Conjunction with the 32nd ACM/IEEE International Conference on Software Engineering, ICSE 2010","4 May 2010 through 4 May 2010","Cape Town",81248,"Conference Paper","Scopus"
"Predicting the fix time of bugs","Giger E., Pinzger M., Gall H.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954974536&doi=10.1145%2f1808920.1808933&partnerID=40&md5=6da31979323333fbe8b9ef5e7886a38c",2010,"Proceedings - International Conference on Software Engineering","Two important questions concerning the coordination of development effort are which bugs to fix first and how long it takes to fix them. In this paper we investigate empirically the relationships between bug report attributes and the time to fix. The objective is to compute prediction models that can be used to recommend whether a new bug should and will be fixed fast or will take more time for resolution. We examine in detail if attributes of a bug report can be used to build such a recommender system. We use decision tree analysis to compute and 10-fold cross validation to test prediction models. We explore prediction models in a series of empirical studies with bug report data of six systems of the three open source projects Eclipse, Mozilla, and Gnome. Results show that our models perform significantly better than random classification. For example, fast fixed Eclipse Platform bugs were classified correctly with a precision of 0.654 and a recall of 0.692. We also show that the inclusion of postsubmission bug report data of up to one month can further improve prediction models.",,"ACM SIG on Software Engineering (SIGSOFT);IEEE CS",,"2nd International Workshop on Recommendation Systems for Software Engineering, RSSE 2010, in Conjunction with the 32nd ACM/IEEE International Conference on Software Engineering, ICSE 2010","4 May 2010 through 4 May 2010","Cape Town",81248,"Conference Paper","Scopus"
"What is trust in a recommender for software development?","Murphy G.C., Murphy-Hill E.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954971982&doi=10.1145%2f1808920.1808934&partnerID=40&md5=dc6720ad1a9b0082b6f3d285e1212b19",2010,"Proceedings - International Conference on Software Engineering","Many recommendation systems have been built to aid software developers. Few have been evaluated using human-based evaluation strategies. In studying situations where recommendation systems have been used or might be used, we have observed that issues related to trust are important to a developer's acceptance of recommendations. In this position paper, we outline the trust issues that we have identified and suggest some mechanisms for promoting trust in recommendation systems aimed at software developers.","Recommendation; Software development; Trust","ACM SIG on Software Engineering (SIGSOFT);IEEE CS",,"2nd International Workshop on Recommendation Systems for Software Engineering, RSSE 2010, in Conjunction with the 32nd ACM/IEEE International Conference on Software Engineering, ICSE 2010","4 May 2010 through 4 May 2010","Cape Town",81248,"Conference Paper","Scopus"
"Towards knowledge assisted agile requirements evolution","Kumar M., Ajmeri N., Ghaisas S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954974537&doi=10.1145%2f1808920.1808924&partnerID=40&md5=06c7d657c240044bdb1af3c996ab995a",2010,"Proceedings - International Conference on Software Engineering","This paper presents work on a recommendation system for Knowledge assisted Agile Requirements Evolution (K-gileRE). We treat requirements engineering as a special case of knowledge engineering and emphasize the fact that providing a domain knowledge edge can impart agility to the requirements definition exercise. The approach differs from existing agile methods in that it seamlessly incorporates a domain knowledge base into an agile requirements definition framework and explicitly provides to requirement analysts, relevant online domain specific recommendations based on underlying ontologies. The framework presents a 'domain knowledge seed' to requirement analysts. The seed provides a view of core features in a given domain and associated knowledge elements such as business processes, rules, policies, partial data models, use cases and test cases,. These in turn are mapped with agile requirements elements such as user stories, features, tasks, product backlog, sprints and prototype plans. The requirement analyst can evolve the seed to suit her specific project needs. As she modifies and evolves the seed specification, she receives domain-specific online recommendations to improve the correctness, consistency and completeness of her requirement specification documents and executable models. Using the domain knowledge seed as a point of departure provides a jump-start to her project. Each exercise of requirements definition thus becomes an evolution from the seed instead of the traditional 'clean slate' Requirements Engineering (RE) that typically starts from the scratch. Hence, the term KgileRE. We elaborate how K-gileRE helps in practicing the essence of agile doctrines while defining software requirements by providing just-in-time recommendations. Copyright Â© 2010 ACM.","Collaborative and semantic requirements definition; Domain-specific recommendations; Knowledge assisted agile","ACM SIG on Software Engineering (SIGSOFT);IEEE CS",,"2nd International Workshop on Recommendation Systems for Software Engineering, RSSE 2010, in Conjunction with the 32nd ACM/IEEE International Conference on Software Engineering, ICSE 2010","4 May 2010 through 4 May 2010","Cape Town",81248,"Conference Paper","Scopus"
"Assisting engineers in switching artifacts by using task semantic and interaction history","Maalej W., Sahm A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954960338&doi=10.1145%2f1808920.1808935&partnerID=40&md5=190fc8b689369ec16307dbe7fe2468bf",2010,"Proceedings - International Conference on Software Engineering","Recent empirical studies show that software engineers use 5 tools and 14 artifacts on average for a single task. As development work is frequently interrupted and several simultaneous tasks are performed in parallel, engineers need to switch many times between these tools and artifacts. A lot of time gets wasted in repeatedly locating, reopening or selecting the right artifacts needed next. To address this problem we introduce Switch!, a context-aware artifact recommendation and switching tool. Switch! assists engineers in switching artifacts based on the type of the development task and the interaction history. Copyright Â© 2010 ACM.",,"ACM SIG on Software Engineering (SIGSOFT);IEEE CS",,"2nd International Workshop on Recommendation Systems for Software Engineering, RSSE 2010, in Conjunction with the 32nd ACM/IEEE International Conference on Software Engineering, ICSE 2010","4 May 2010 through 4 May 2010","Cape Town",81248,"Conference Paper","Scopus"
"Recommending rename refactorings","Thies A., Roth C.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954992166&doi=10.1145%2f1808920.1808921&partnerID=40&md5=69caad7f3539bac7a4881f53a9d0b2af",2010,"Proceedings - International Conference on Software Engineering","Variable names play a major role in program comprehension. However, their choice is often subject to the intuition (or intention) of individual programmers: although code conventions and style guides may constrain identifier usage, programmers are individuals naming program concepts individually. Especially if different parts of a program are written by different programmers, inconsistent naming of program entities may follow. This is unfortunate, since consistent naming would aid program comprehension, in particular if references pointing to same objects used in similar ways are named equally. As a first approach, we focus on assignments to discover possible inconsistency of naming, exploiting that a variable assigned to another likely points to same objects and, if declared with the same type, is likely used for the same purpose. To explore the feasibility of our approach, we implemented a tool recommending rename refactorings to harmonize variable names based on an analysis of assignments and static type information. Evaluated on some open source projects the results seem promising enough to aim for some extensions, such as application to method names, inferred type information, and weakly type-checked languages. Copyright Â© 2010 ACM.","Assignment analysis; Identifiers; Rename refactoring; Variable naming","ACM SIG on Software Engineering (SIGSOFT);IEEE CS","IEEE Computer Society","2nd International Workshop on Recommendation Systems for Software Engineering, RSSE 2010, in Conjunction with the 32nd ACM/IEEE International Conference on Software Engineering, ICSE 2010","4 May 2010 through 4 May 2010","Cape Town",81248,"Conference Paper","Scopus"
"Visual augmentation of source code editors: A systematic mapping study","SulÃ­r M., BaÄÃ­kovÃ¡ M., Chodarev S., PorubÃ¤n J.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056170967&doi=10.1016%2fj.jvlc.2018.10.001&partnerID=40&md5=6cbcb49d2b97ed1cafa976ed64a353ad",2018,"Journal of Visual Languages and Computing","Source code written in textual programming languages is typically edited in integrated development environments (IDEs) or specialized code editors. These tools often display various visual items, such as icons, color highlights or more advanced graphical overlays directly in the main editable source code view. We call such visualizations source code editor augmentation. In this paper, we present a first systematic mapping study of source code editor augmentation tools and approaches. We manually reviewed the metadata of 5553 articles published during the last twenty years in two phases â€“ keyword search and references search. The result is a list of 103 relevant articles and a taxonomy of source code editor augmentation tools with seven dimensions, which we used to categorize the resulting list of the surveyed articles. We also provide the definition of the term source code editor augmentation, along with a brief overview of historical development and augmentations available in current industrial IDEs. Â© 2018","In situ visualization; Integrated development environment (IDE); Source code editor augmentation; Survey; Systematic review",,"Academic Press",,,,,"Article","Scopus"
"Evaluating the suitability of state-based formal methods for industrial deployment","Mashkoor A., Kossak F., Egyed A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053482365&doi=10.1002%2fspe.2634&partnerID=40&md5=cac93e9f71b8a2e31acb2a9be30b9257",2018,"Software - Practice and Experience","After a number of success stories in safety-critical domains, we are starting to witness applications of formal methods in contemporary systems and software engineering. However, one thing that is still missing is the evaluation criteria that help software practitioners choose the right formal method for the problem at hand. In this paper, we present the criteria for evaluating and comparing different formal methods. The criteria were chosen through a literature review, discussions with experts from academia and practitioners from industry, and decade-long personal experience with the application of formal methods in industrial and academic projects. The criteria were then evaluated on several model-oriented state-based formal methods. Our research shows that besides technical grounds (eg, modeling capabilities and supported development phases), formal methods should also be evaluated from social and industrial perspectives. We also found out that it is not possible to generate a matrix that renders the selection of the right formal method an automatic process. However, we can generate several pointers, which make this selection process a lot less cumbersome. Â© 2018 The Authors. Software:Â PracticeÂ andÂ Experience Published by John Wiley & Sons Ltd.","evaluation criteria; formal methods",,"John Wiley and Sons Ltd",,,,,"Article","Scopus"
"Improving regression test efficiency with an awareness of refactoring changes","Chen Z., Guo H.-F., Song M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050199696&doi=10.1016%2fj.infsof.2018.07.003&partnerID=40&md5=3b0be3252677f4b0ddb23861a8ed3fc7",2018,"Information and Software Technology","Context. Developers often improve software quality through refactoringsâ€”the practice of behavior-preserving changes to existing code. Recent studies showed that, despite their awareness of tool support for automated refactorings, developers prefer manual refactorings. This practice can be often error-prone and increase testing cost. Objective. To address the problem, we present the Refactorings Investigation and Testing technique, called RIT. RIT improves the testing efficiency for validating refactoring changes and providing confidence that changed parts behave as intended. As testing is expensive for developers of high-assurance software, RIT reduces a considerable amount of its costs by only identifying dependent statements on a failure in each test and by detecting specific refactoring edits responsible for testing failures. Method. Our approach identifies refactorings by analyzing original and edited versions of a program. It then uses the semantic impact of a set of identified refactoring changes to detect tests whose behavior may have been affected and modified by refactoring edits. Given each failed asserts after running regression test suites, RIT helps developers focus their attention on logically related program statements by applying program slicing for minimizing each test. For debugging purposes, RIT determines specific failure-inducing refactoring edits, separating from other changes that only affect other asserts or tests. Results. We evaluated RIT on three open source projects, and found that RIT detected tests affected by refactorings with 80.9% accuracy on average. Furthermore, it identified and formed partitions relating program statements only dependent on failed asserts with 97.2% accuracy on average. Conclusion. RIT, which combines a refactoring reconstruction technique with change impact analysis to localize failure-inducing program edits, helps developers localize fault causes by focusing on refactoring changes as opposed to all the code fragments in the new version. Â© 2018 Elsevier B.V.","Change impact analysis; Fault localization; Refactorings; Software evolution; Test selection",,"Elsevier B.V.",,,,,"Article","Scopus"
"Measuring Program Comprehension: A Large-Scale Field Study with Professionals","Xia X., Bao L., Lo D., Xing Z., Hassan A.E., Li S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028931582&doi=10.1109%2fTSE.2017.2734091&partnerID=40&md5=fca07ad04aede7b5d3ce0f7ff22fd4df",2018,"IEEE Transactions on Software Engineering","During software development and maintenance, developers spend a considerable amount of time on program comprehension activities. Previous studies show that program comprehension takes up as much as half of a developer's time. However, most of these studies are performed in a controlled setting, or with a small number of participants, and investigate the program comprehension activities only within the IDEs. However, developers' program comprehension activities go well beyond their IDE interactions. In this paper, we extend our ActivitySpace framework to collect and analyze Human-Computer Interaction (HCI) data across many applications (not just the IDEs). We follow Minelli et al.'s approach to assign developers' activities into four categories: navigation, editing, comprehension, and other. We then measure the comprehension time by calculating the time that developers spend on program comprehension, e.g., inspecting console and breakpoints in IDE, or reading and understanding tutorials in web browsers. Using this approach, we can perform a more realistic investigation of program comprehension activities, through a field study of program comprehension in practice across a total of seven real projects, on 78 professional developers, and amounting to 3,148 working hours. Our study leverages interaction data that is collected across many applications by the developers. Our study finds that on average developers spend âˆ¼ 58 percent of their time on program comprehension activities, and that they frequently use web browsers and document editors to perform program comprehension activities. We also investigate the impact of programming language, developers' experience, and project phase on the time that is spent on program comprehension, and we find senior developers spend significantly less percentages of time on program comprehension than junior developers. Our study also highlights the importance of several research directions needed to reduce program comprehension time, e.g., building automatic detection and improvement of low quality code and documentation, construction of software-engineering-specific search engines, designing better IDEs that help developers navigate code and browse information more efficiently, etc. Â© 1976-2012 IEEE.","field study; inference model; Program comprehension",,"Institute of Electrical and Electronics Engineers Inc.",,,,,"Article","Scopus"
"Augmenting and structuring user queries to support efficient free-form code search","Sirres R., BissyandÃ© T.F., Kim D., Lo D., Klein J., Kim K., Traon Y.L.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040983742&doi=10.1007%2fs10664-017-9544-y&partnerID=40&md5=0e690f45467afe9f98b194e650416290",2018,"Empirical Software Engineering","Source code terms such as method names and variable types are often different from conceptual words mentioned in a search query. This vocabulary mismatch problem can make code search inefficient. In this paper, we present COde voCABUlary (CoCaBu), an approach to resolving the vocabulary mismatch problem when dealing with free-form code search queries. Our approach leverages common developer questions and the associated expert answers to augment user queries with the relevant, but missing, structural code entities in order to improve the performance of matching relevant code examples within large code repositories. To instantiate this approach, we build GitSearch, a code search engine, on top of GitHub and Stack Overflow Q&A data. We evaluate GitSearch in several dimensions to demonstrate that (1) its code search results are correct with respect to user-accepted answers; (2) the results are qualitatively better than those of existing Internet-scale code search engines; (3) our engine is competitive against web search engines, such as Google, in helping users solve programming tasks; and (4) GitSearch provides code examples that are acceptable or interesting to the community as answers for Stack Overflow questions. Â© 2018, Springer Science+Business Media, LLC.","Code search; Free-form search; GitHub; Query augmentation; StackOverflow; Vocabulary mismatch",,"Springer New York LLC",,,,,"Article","Scopus"
"Automated modelling assistance by integrating heterogeneous information sources","Ãngel M.S., de Lara J., Neubauer P., Wimmer M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043506483&doi=10.1016%2fj.cl.2018.02.002&partnerID=40&md5=7150739735b2e92564faee72d2b595cc",2018,"Computer Languages, Systems and Structures","Model-Driven Engineering (MDE) uses models as its main assets in the software development process. The structure of a model is described through a meta-model. Even though modelling and meta-modelling are recurrent activities in MDE and a vast amount of MDE tools exist nowadays, they are tasks typically performed in an unassisted way. Usually, these tools cannot extract useful knowledge available in heterogeneous information sources like XML, RDF, CSV or other models and meta-models. We propose an approach to provide modelling and meta-modelling assistance. The approach gathers heterogeneous information sources in various technological spaces, and represents them uniformly in a common data model. This enables their uniform querying, by means of an extensible mechanism, which can make use of services, e.g., for synonym search and word sense analysis. The query results can then be easily incorporated into the (meta-)model being built. The approach has been realized in the EXTREMO tool, developed as an Eclipse plugin. EXTREMO has been validated in the context of two domains â€“ production systems and process modelling â€“ taking into account a large and complex industrial standard for classification and product description. Further validation results indicate that the integration of EXTREMO in various modelling environments can be achieved with low effort, and that the tool is able to handle information from most existing technological spaces. Â© 2018 Elsevier Ltd","(Meta-)modelling; Domain-specific languages; Language engineering; Modelling; Modelling assistance",,"Elsevier Ltd",,,,,"Article","Scopus"
"A novel rule based machine translation scheme from Greek to Greek Sign Language: Production of different types of large corpora and Language Models evaluation","Kouremenos D., Ntalianis K., Kollias S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046446621&doi=10.1016%2fj.csl.2018.04.001&partnerID=40&md5=5412191c0902785f604ea950ed31f28b",2018,"Computer Speech and Language","One of the aims of assistive technologies is to help people with disabilities to communicate with others and to provide means of access to information. As an aid to Deaf people, in this work we present a novel prototype Rule Based Machine Translation (RBMT) system for the creation of large and quality written Greek Sign Language (GSL) glossed corpora from Greek text. In particular, the proposed RBMT system assists the professional GSL translator in speeding up the production of different kinds of GSL glossed corpora. Then each glossed corpus is used for the production/creation of Language Model (LM) n-grams. With the GSL glossed corpus from Greek text, we can build, test and evaluate different kinds of Language Models for different kinds of glossed GSL corpora. Here, it should be noted that it does not require grammar knowledge of GSL but only very basic GSL phenomena covered by manual RBMT rules as it assists the professional human translator. Furthermore, it should also be stressed that Language Models for written GSL gloss are missing from the scientific literature, thus this work is pioneer in this field. Evaluation of the proposed scheme is carried out for the weather reports domain, where 20,284 tokens and 1000 sentences have been produced. By using the BiLingual Evaluation Understudy (BLEU) metric score, our prototype RBMT system achieves a relative score of 0.84 (84%) for 4-grams and 0.9 (90%) for 1-grams. Â© 2018 Elsevier Ltd","Deaf people communication; Greek Sign Language; GSL; Machine translation",,"Academic Press",,,,,"Article","Scopus"
"Personal digital bodyguards for e-security, e-learning and e-health: A prospective survey","Plamondon R., Pirlo G., Anquetil Ã‰., RÃ©mi C., Teulings H.-L., Nakagawa M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046169565&doi=10.1016%2fj.patcog.2018.04.012&partnerID=40&md5=fc6ed6df5343f381f1ac73bd4f844007",2018,"Pattern Recognition","The widespread availability of hand-held devices like tablets, phablets and smart phones, along with their new handwriting digitizing and their increased computing powers, enable these to process the graphomotor dimension and the lognormal trends of human handwriting. By exploiting such capacity, it becomes possible to extend these mobile devices into Personal Digital Bodyguards (PDBs). PDBs will be able to supplement people's sensitive data protection with signature verification, equipment use security with writer authentication and handwritten CAPTCHAs processing (e-security), and to enhance human-machine interaction performances through words spotting and handwriting recognition (e-recognition). For young children, these tools will turn into interactive toys helping them to learn and master their fine motor control and become better writers. For advanced students they will enable sophisticated systems for (e-learning) and (e-testing). Moreover, PDBs will also be able to provide the user with fine motor control monitoring, which can detect stress, aging and health problems (e-health). This paper presents a prospective survey of various projects dealing with these five e-fields of investigation, focussing on state of the art results and providing directions in research and development, under the theoretical umbrella of the Kinematic Theory of human movements and its Lognormality Principle. From a practical point of view, the concept of lognormality provides a fundamental common thread, an integrative psychophysical standpoint to track the graphomotor problems of signature verification, writer identification, handwriting generation, recognition and learning. Â© 2018","E-health; E-interaction; E-learning; E-recognition; E-security; E-testing tools; Handwriting learning and recognition; Lognormality principle; Mobile devices; Neuromuscular disorder; Personal digital body guard; Signature verification",,"Elsevier Ltd",,,,,"Article","Scopus"
"Learning from the past: A process recommendation system for video game projects using postmortems experiences","Politowski C., Fontoura L.M., Petrillo F., GuÃ©hÃ©neuc Y.-G.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046135039&doi=10.1016%2fj.infsof.2018.04.003&partnerID=40&md5=6a5dae139c40037f6c0d67d1adecd11a",2018,"Information and Software Technology","Context: The video game industry is a billion dollar industry that faces problems in the way games are developed. One method to address these problems is using developer aid tools, such as Recommendation Systems. These tools assist developers by generating recommendations to help them perform their tasks. Objective: This article describes a systematic approach to recommend development processes for video game projects, using postmortem knowledge extraction and a model of the context of the new project, in which â€œpostmortemsâ€ are articles written by video game developers at the end of projects, summarizing the experience of their game development team. This approach aims to provide reflections about development processes used in the game industry as well as guidance to developers to choose the most adequate process according to the contexts they're in. Method: Our approach is divided in three separate phases: in the first phase, we manually extracted the processes from the postmortems analysis; in the second one, we created a video game context and algorithm rules for recommendation; and finally in the third phase, we evaluated the recommended processes by using quantitative and qualitative metrics, game developers feedback, and a case study by interviewing a video game development team. Contributions: This article brings three main contributions. The first describes a database of developersâ€™ experiences extracted from postmortems in the form of development processes. The second defines the main attributes that a video game project contain, which it uses to define the contexts of the project. The third describes and evaluates a recommendation system for video game projects, which uses the contexts of the projects to identify similar projects and suggest a set of activities in the form of a process. Â© 2018 Elsevier B.V.","Recommendation system; Software development process; Video game development",,"Elsevier B.V.",,,,,"Article","Scopus"
"A model-driven software engineering workflow and tool architecture for servitised manufacturing","Ntanos E., Dimitriou G., Bekiaris V., Vassiliou C., Kalaboukas K., Askounis D.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056397297&doi=10.1007%2fs10257-018-0371-5&partnerID=40&md5=b489ce9df05a0c91b8211782f7dc13a5",2018,"Information Systems and e-Business Management","Modern manufacturing businesses increasingly engage in servitisation, by offering advanced services along with physical products, and creating â€œproductâ€“service systemsâ€. Information Technology infrastructures, and especially software, are a critical part of modern service provision. However, software development in this context has not been investigated and there are no development methods or tools specifically adapted to the task of creating software for servitised businesses in general, or manufacturing in particular. In this paper, we define the requirements for software engineering in servitised manufacturing. Based on these, we describe a model-driven software engineering workflow for servitised manufacturing, supporting both structural and behavioural modelling of the service system. Furthermore, we elaborate on the architecture of an appropriate model-driven Integrated Development Environment (IDE). The proposed workflow and a prototype implementation of the IDE were evaluated in a set of industrial pilots, demonstrating improved communication and collaboration between participants in the software engineering process. Â© 2018, Springer-Verlag GmbH Germany, part of Springer Nature.","Integrated Development Environment; Manufacturing; Model driven; Product-service system; Software engineering; Tool integration",,"Springer Verlag",,,,,"Article","Scopus"
"Systematic literature review: Self-Regulated Learning strategies using e-learning tools for Computer Science","Garcia R., Falkner K., Vivian R.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047006409&doi=10.1016%2fj.compedu.2018.05.006&partnerID=40&md5=6dbff6d958958389d36933c164a0272f",2018,"Computers and Education","In 1986, Barry Zimmerman and Manuel Martinez-Pons presented a taxonomy containing 14 categories on Self-Regulated Learning (SRL) strategies performed by high school students when studying. Since this study, researchers have used the taxonomy as a framework for their research on studentsâ€™ SRL strategies and behaviours. When the taxonomy was constructed in the mid-1980s, these categories did not consider studentsâ€™ SRL behaviours while using digital technologies to study. The goals of this systematic literature review are to understand if the original SRL strategies are addressed in modern Computer Science e-learning tools and to determine if tools have emerged that support SRL strategies not captured by the original taxonomy. This review organises the e-learning tools within the SRL categories. It shows that a preponderance of research has been done on certain SRL skills, with limited focus on other categories. The systematic literature review concludes with suggestions for future research opportunities pertaining to tools that support the original categories, as well as tools that support SRL strategies. Â© 2018","e-learning Tools; Metacognitive Skills; Self-Regulated Learning",,"Elsevier Ltd",,,,,"Article","Scopus"
"Understanding semi-structured merge conflict characteristics in open-source Java projects","Accioly P., Borba P., Cavalcanti G.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038628515&doi=10.1007%2fs10664-017-9586-1&partnerID=40&md5=0ff6c8798c2b0b941587337bc37fa534",2018,"Empirical Software Engineering","Empirical studies show that merge conflicts frequently occur, impairing developersâ€™ productivity, since merging conflicting contributions might be a demanding and tedious task. However, the structure of changes that lead to conflicts has not been studied yet. Understanding the underlying structure of conflicts, and the involved syntactic language elements might shed light on how to better avoid merge conflicts. To this end, in this paper we derive a catalog of conflict patterns expressed in terms of the structure of code changes that lead to merge conflicts. We focus on conflicts reported by a semistructured merge tool that exploits knowledge about the underlying syntax of the artifacts. This way, we avoid analyzing a large number of spurious conflicts often reported by typical line based merge tools. To assess the occurrence of such patterns in different systems, we conduct an empirical study reproducing 70,047 merges from 123 GitHub Java projects. Our results show that most semistructured merge conflicts in our sample happen because developers independently edit the same or consecutive lines of the same method. However, the probability of creating a merge conflict is approximately the same when editing methods, class fields, and modifier lists. Furthermore, we noticed that most part of conflicting merge scenarios, and merge conflicts, involve more than two developers. Also, that copying and pasting pieces of code, or even entire files, across different repositories is a common practice and cause of conflicts. Finally, we discuss how our results reveal the need for new research studies and suggest potential improvements to tools supporting collaborative software development. Â© 2017, Springer Science+Business Media, LLC, part of Springer Nature.","Awareness tools; Collaborative software development; Empirical software engineering; Merge conflicts",,"Springer New York LLC",,,,,"Article","Scopus"
"A novel Big Data analytics and intelligent technique to predict driver's intent","Birek L., Grzywaczewski A., Iqbal R., Doctor F., Chang V.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044955737&doi=10.1016%2fj.compind.2018.03.025&partnerID=40&md5=9589fd865f96ffa32ea0f872e8c9386c",2018,"Computers in Industry","Modern age offers a great potential for automatically predicting the driver's intent through the increasing miniaturization of computing technologies, rapid advancements in communication technologies and continuous connectivity of heterogeneous smart objects. Inside the cabin and engine of modern cars, dedicated computer systems need to possess the ability to exploit the wealth of information generated by heterogeneous data sources with different contextual and conceptual representations. Processing and utilizing this diverse and voluminous data, involves many challenges concerning the design of the computational technique used to perform this task. In this paper, we investigate the various data sources available in the car and the surrounding environment, which can be utilized as inputs in order to predict driver's intent and behavior. As part of investigating these potential data sources, we conducted experiments on e-calendars for a large number of employees, and have reviewed a number of available geo referencing systems. Through the results of a statistical analysis and by computing location recognition accuracy results, we explored in detail the potential utilization of calendar location data to detect the driver's intentions. In order to exploit the numerous diverse data inputs available in modern vehicles, we investigate the suitability of different Computational Intelligence (CI) techniques, and propose a novel fuzzy computational modelling methodology. Finally, we outline the impact of applying advanced CI and Big Data analytics techniques in modern vehicles on the driver and society in general, and discuss ethical and legal issues arising from the deployment of intelligent self-learning cars. Â© 2018 Elsevier B.V.","Big Data; Big Data analytics; Computational intelligence; Driver's intent prediction; E-calendar; Geo referencing",,"Elsevier B.V.",,,,,"Article","Scopus"
"VPL-based big data analysis system: UDAS","Choi H., Gim J., Seo Y.-D., Baik D.-K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050407551&doi=10.1109%2fACCESS.2018.2857845&partnerID=40&md5=32172d17bd548b9b198a4966cfd681c6",2018,"IEEE Access","Over the past five years, research on big data analysis has been actively conducted, and many services have been developed to find valuable data. However, low quality of raw data and data loss problem during data analysis make it difficult to perform accurate data analysis. With the enormous generation of both unstructured and structured data, refinement of data is becoming increasingly difficult. As a result, data refinement plays an important role in data analysis. In addition, as part of efforts to ensure research reproducibility, the importance of reuse of researcher data and research methods is increasing; however, the research on systems supporting such roles has not been conducted sufficiently. Therefore, in this paper, we propose a big data analysis system named the unified data analytics suite (UDAS) that focuses on data refinement. UDAS performs data refinement based on the big data platform and ensures the reusability and reproducibility of refinement and analysis through the visual programming language interface. It also recommends open source and visualization libraries to users for statistical analysis. The qualitative evaluation of UDAS using the functional evaluation factor of the big data analysis platform demonstrated that the average satisfaction of the users is significantly high. Â© 2013 IEEE.","Clouds; Data analysis; Data refinement; Data visualization; R; Reproducibility of results",,"Institute of Electrical and Electronics Engineers Inc.",,,,,"Article","Scopus"
"Quick fixing ATL transformations with speculative analysis","Cuadrado J.S., Guerra E., de Lara J.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976467674&doi=10.1007%2fs10270-016-0541-1&partnerID=40&md5=e2a6e32c847b9ee81d2cc8c00dbcdde1",2018,"Software and Systems Modeling","Model transformations are central components of most model-based software projects. While ensuring their correctness is vital to guarantee the quality of the solution, current transformation tools provide limited support to statically detect and fix errors. In this way, the identification of errors and their correction are nowadays mostly manual activities which incur in high costs. The aim of this work is to improve this situation. Recently, we developed a static analyser that combines program analysis and constraint solving to identify errors in ATL model transformations. In this paper, we present a novel method and system that uses our analyser to propose suitable quick fixes for ATL transformation errors, notably some non-trivial, transformation-specific ones. Our approach supports speculative analysis to help developers select the most appropriate fix by creating a dynamic ranking of fixes, reporting on the consequences of applying a quick fix, and providing a pre-visualization of each quick fix application. The approach integrates seamlessly with the ATL editor. Moreover, we provide an evaluation based on existing faulty transformations built by a third party, and on automatically generated transformation mutants, which are then corrected with the quick fixes of our catalogue. Â© 2016, Springer-Verlag Berlin Heidelberg.","ATL; Model transformation; Quick fixes; Speculative analysis; Transformation static analysis",,"Springer Verlag",,,,,"Article","Scopus"
"Collaboration framework for software development based on question and answer sites","Kao C.H.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050270181&doi=10.1109%2fICASI.2018.8394595&partnerID=40&md5=02a368429a64734a800bdea6e7d4c355",2018,"Proceedings of 4th IEEE International Conference on Applied System Innovation 2018, ICASI 2018","Recently, question and answer sites (Q&A sites) have become a popular and useful resource for software developers to learn and share their knowledge in order to support the complex software development and maintenance tasks. Through the contributions all around the world, Q&A sites accumulate a large amount of valuable information (e.g., experience, suggestions, solutions, explanations, code snippets, and so on) and can be used as associated resources for software development and maintenance nowadays. In order to improve the management of information on Q&A sites for software development and maintenance tasks, a collaboration framework for software development based on Q&A sites is proposed in this paper. Through the collaboration framework, the valuable information on Q&A sites can be managed and leveraged efficiently in order to facilitate the development and maintenance tasks in complex software systems. Â© 2018 IEEE.","Collaboration Framework; Question and Answer Site; Software Development",,"Institute of Electrical and Electronics Engineers Inc.","4th IEEE International Conference on Applied System Innovation, ICASI 2018","13 April 2018 through 17 April 2018",,137425,"Conference Paper","Scopus"
"JIT feedback: What experienced developers like about static analysis","Tymchuk Y., Ghafari M., Nierstrasz O.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051624373&doi=10.1145%2f3196321.3196327&partnerID=40&md5=545e2abaad204272f155c0b822d46859",2018,"Proceedings - International Conference on Software Engineering","Although software developers are usually reluctant to use static analysis to detect issues in their source code, our automatic just-in-time static analysis assistant was integrated into an Integrated Development Environment, and was evaluated positively by its users. We conducted interviews to understand the impact of the tool on experienced developers, and how it performs in comparison with other static analyzers. We learned that the availability of our tool as a default IDE feature and its automatic execution are the main reasons for its adoption. Moreover, the fact that immediate feedback is provided directly in the related development context is essential to keeping developers satisfied, although in certain cases feedback delivered later was deemed more useful. We also discovered that static analyzers can play an educational role, especially in combination with domain-specific rules. Â© 2018 ACM.","just-in-time feedback; software quality; static analysis","ACM Special Interest Group on Software Engineering (SIGSOFT);IEEE Computer Society;IEEE Technical Council on Software Engineering (TCSE)","IEEE Computer Society","ACM/IEEE 26th International Conference on Program Comprehension, ICPC 2018, collocated with the 40th International Conference on Software Engineering, ICSE 2018","27 May 2018 through 28 May 2018",,138316,"Conference Paper","Scopus"
"Un-break my build: Assisting developers with build repair hints","Vassallo C., Proksch S., Zemp T., Gall H.C.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051634711&doi=10.1145%2f3196321.3196350&partnerID=40&md5=31292666684a91d925237ee65066d611",2018,"Proceedings - International Conference on Software Engineering","Continuous integration is an agile software development practice. Instead of integrating features right before a release, they are constantly being integrated in an automated build process. This shortens the release cycle, improves software quality, and reduces time to market. However, the whole process will come to a halt when a commit breaks the build, which can happen for several reasons, e.g., compilation errors or test failures, and fixing the build suddenly becomes a top priority. Developers not only have to find the cause of the build break and fix it, but they have to be quick in all of it to avoid a delay for others. Unfortunately, these steps require deep knowledge and are often time consuming. To support developers in fixing a build break, we propose Bart, a tool that summarizes the reasons of the build failure and suggests possible solutions found on the Internet. We will show in a case study with eight participants that developersfind Bart useful to understand build breaks and that using Bart substantially reduces the time to fix a build break, on average by 41%. Â© 2018 ACM.","agile software development; build break; error recovery; software development tools; software engineering; summarization","ACM Special Interest Group on Software Engineering (SIGSOFT);IEEE Computer Society;IEEE Technical Council on Software Engineering (TCSE)","IEEE Computer Society","ACM/IEEE 26th International Conference on Program Comprehension, ICPC 2018, collocated with the 40th International Conference on Software Engineering, ICSE 2018","27 May 2018 through 28 May 2018",,138316,"Conference Paper","Scopus"
"Recommending frequently encountered bugs","Zhang Y., Lo D., Xia X., Jiang J., Sun J.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051668877&doi=10.1145%2f3196321.3196348&partnerID=40&md5=445e9162c31b9192164cda525ee99724",2018,"Proceedings - International Conference on Software Engineering","Developers introduce bugs during software development which reduce software reliability. Many of these bugs are commonly occurring and have been experienced by many other developers. Informing developers, especially novice ones, about commonly occurring bugs in a domain of interest (e.g., Java), can help developers comprehend program and avoid similar bugs in the future. Unfortunately, information about commonly occurring bugs are not readily available. To address this need, we propose a novel approach named RFEB which recommends frequently encountered bugs (FEBugs) that may affect many other developers. RFEB analyzes Stack Overflow which is the largest software engineering-specific Q&amp;A communities. Among the plenty of questions posted in Stack Overflow, many of them provide the descriptions and solutions of different kinds of bugs. Unfortunately, the search engine that comes with Stack Overflow is not able to identify FEBugs well. To address the limitation of the search engine of Stack Overflow, we propose RFEB which is an integrated and iterative approach that considers both relevance and popularity of Stack Overflow questions to identify FEBugs. To evaluate the performance of RFEB, we perform experiments on a dataset from Stack Overflow which contains more than ten million posts. We compared our model with Stack Overflow's search engine on 10 domains, and the experiment results show that RFEB achieves the average NDCG10 score of 0.96, which improves Stack Overflow's search engine by 20%. Â© 2018 ACM.",,"ACM Special Interest Group on Software Engineering (SIGSOFT);IEEE Computer Society;IEEE Technical Council on Software Engineering (TCSE)","IEEE Computer Society","ACM/IEEE 26th International Conference on Program Comprehension, ICPC 2018, collocated with the 40th International Conference on Software Engineering, ICSE 2018","27 May 2018 through 28 May 2018",,138316,"Conference Paper","Scopus"
"Codecatch: Extracting source code snippets from online sources","Diamantopoulos T., Karagiannopoulos G., Symeonidis A.L.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051190077&doi=10.1145%2f3194104.3194107&partnerID=40&md5=df9280d6c020cf133474d331d09f0cee",2018,"Proceedings - International Conference on Software Engineering","Nowadays, developers rely on online sources to find example snippets that address the programming problems they are trying to solve. However, contemporary API usage mining methods are not suitable for locating easily reusable snippets, as they provide usage examples for specific APIs, thus requiring the developer to know which library to use beforehand. On the other hand, the approaches that retrieve snippets from online sources usually output a list of examples, without aiding the developer to distinguish among different implementations and without offering any insight on the quality and the reusability of the proposed snippets. In this work, we present CodeCatch, a system that receives queries in natural language and extracts snippets from multiple online sources. The snippets are assessed both for their quality and for their usefulness/preference by the developers, while they are also clustered according to their API calls to allow the developer to select among the different implementations. Preliminary evaluation of CodeCatch in a set of indicative programming problems indicates that it can be a useful tool for the developer. Â© 2018 ACM.","API usage mining; code reuse; snippet mining","ACM SIGSOFT;IEEE Computer Society;IEEE Technical Council on Software Engineering (TCSE)","IEEE Computer Society","6th ACM/IEEE International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering, RAISE 2018, co-located with the 40th International Conference on Software Engineering, ICSE 2018","27 May 2018",,137725,"Conference Paper","Scopus"
"Analyzing conflict predictors in open-source Java projects","Accioly P., Borba P., Silva L., Cavalcanti G.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051679908&doi=10.1145%2f3196398.3196437&partnerID=40&md5=539e62e957d1c50ce96cbd29340d26e3",2018,"Proceedings - International Conference on Software Engineering","In collaborative development environments integration conflicts occur frequently. To alleviate this problem, different awareness tools have been proposed to alert developers about potential conflicts before they become too complex. However, there is not much empirical evidence supporting the strategies used by these tools. Learning about what types of changes most likely lead to conflicts might help to derive more appropriate requirements for early conflict detection, and suggest improvements to existing conflict detection tools. To bring such evidence, in this paper we analyze the effectiveness of two types of code changes as conflict predictors. Namely, editions to the same method, and editions to directly dependent methods. We conduct an empirical study analyzing part of the development history of 45 Java projects from GitHub and Travis CI, including 5,647 merge scenarios, to compute the precision and recall for the conflict predictors aforementioned. Our results indicate that the predictors combined have a precision of 57.99% and a recall of 82.67%. Moreover, we conduct a manual analysis which provides insights about strategies that could further increase the precision and the recall. Â© 2018 ACM.","awareness tools; collaborative development; conflict predictors; precision and recall","ACM Special Interest Group on Software Engineering (SIGSOFT);IEEE Computer Society;IEEE Technical Council on Software Engineering (TCSE)","IEEE Computer Society","15th ACM/IEEE International Conference on Mining Software Repositories, MSR 2018, co-located with the 40th International Conference on Software Engineering, ICSE 2018","28 May 2018 through 29 May 2018",,138320,"Conference Paper","Scopus"
"SOTorrent: Reconstructing and analyzing the evolution of stack overflow posts","Baltes S., Dumani L., Treude C., Diehl S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051650034&doi=10.1145%2f3196398.3196430&partnerID=40&md5=441ea8a9a3e42d06d6e83604bc40ff28",2018,"Proceedings - International Conference on Software Engineering","Stack Overflow (SO) is the most popular question-and-answer website for software developers, providing a large amount of code snippets and free-form text on a wide variety of topics. Like other software artifacts, questions and answers on SO evolve over time, for example when bugs in code snippets are fixed, code is updated to work with a more recent library version, or text surrounding a code snippet is edited for clarity. To be able to analyze how content on SO evolves, we built SOTorrent, an open dataset based on the official SO data dump. SOTorrent provides access to the version history of SO content at the level of whole posts and individual text or code blocks. It connects SO posts to other platforms by aggregating URLs from text blocks and by collecting references from GitHub files to SO posts. In this paper, we describe how we built SOTorrent, and in particular how we evaluated 134 different string similarity metrics regarding their applicability for reconstructing the version history of text and code blocks. Based on a first analysis using the dataset, we present insights into the evolution of SO posts, e.g., that post edits are usually small, happen soon after the initial creation of the post, and that code is rarely changed without also updating the surrounding text. Further, our analysis revealed a close relationship between post edits and comments. Our vision is that researchers will use SOTorrent to investigate and understand the evolution of SO posts and their relation to other platforms such as GitHub. Â© 2018 ACM.","code snippets; open dataset; software evolution; stack overflow","ACM Special Interest Group on Software Engineering (SIGSOFT);IEEE Computer Society;IEEE Technical Council on Software Engineering (TCSE)","IEEE Computer Society","15th ACM/IEEE International Conference on Mining Software Repositories, MSR 2018, co-located with the 40th International Conference on Software Engineering, ICSE 2018","28 May 2018 through 29 May 2018",,138320,"Conference Paper","Scopus"
"Exploring the use of automated API migrating techniques in practice: An experience report on Android","Lamothe M., Shang W.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051621669&doi=10.1145%2f3196398.3196420&partnerID=40&md5=b27cf3f8c8d15f3165af077b3583442f",2018,"Proceedings - International Conference on Software Engineering","In recent years, open source software libraries have allowed developers to build robust applications by consuming freely available application program interfaces (API). However, when these APIs evolve, consumers are left with the difficult task of migration. Studies on API migration often assume that software documentation lacks explicit information for migration guidance and is impractical for API consumers. Past research has shown that it is possible to present migration suggestions based on historical code-change information. On the other hand, research approaches with optimistic views of documentation have also observed positive results. Yet, the assumptions made by prior approaches have not been evaluated on large scale practical systems, leading to a need to affirm their validity. This paper reports our recent practical experience migrating the use of Android APIs in FDroid apps when leveraging approaches based on documentation and historical code changes. Our experiences suggest that migration through historical codechanges presents various challenges and that API documentation is undervalued. In particular, the majority of migrations from removed or deprecated Android APIs to newly added APIs can be suggested by a simple keyword search in the documentation. More importantly, during our practice, we experienced that the challenges of API migration lie beyond migration suggestions, in aspects such as coping with parameter type changes in new API. Future research may aim to design automated approaches to address the challenges that are documented in this experience report. Â© 2018 ACM.","Android API; API migration; mining software repositories; software evolution","ACM Special Interest Group on Software Engineering (SIGSOFT);IEEE Computer Society;IEEE Technical Council on Software Engineering (TCSE)","IEEE Computer Society","15th ACM/IEEE International Conference on Mining Software Repositories, MSR 2018, co-located with the 40th International Conference on Software Engineering, ICSE 2018","28 May 2018 through 29 May 2018",,138320,"Conference Paper","Scopus"
"Improving integrated development environment commands knowledge with recommender systems","Gasparic M., Gurbanov T., Ricci F.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049791809&doi=10.1145%2f3183377.3183386&partnerID=40&md5=4a0f06a23f011b6cce14810009914ef1",2018,"Proceedings - International Conference on Software Engineering","Development tools have an impact on software engineers' productivity and quality of software construction. We believe that it is crucial to teach future software engineers how to exploit integrated development environment functionality, if we want to encourage the effective application of software development principles and practices. Our research shows that recommender systems can be deployed to improve integrated development environment knowledge of computer science students by automatically suggesting new and useful commands, such as buttons and shortcuts that execute different functions. While previous work focused on optimizing the algorithmic predictive capability of a recommender to identify the commands that the users will eventually use, we have addressed a set of research questions related to the overall acceptance of a complete recommender system in a real-life setting. The evaluation results show that a command recommender system can be well accepted by computer science students. In particular, when students are supported by such a system, they use a considerably larger set of commands available in their development environment. Moreover, the results show that the highest acceptance rate and the usefulness score were achieved by a non-personalized, popularity-based algorithm, while the most novel commands were suggested by a context-aware algorithm. Â© 2018 Association for Computing Machinery.",,"ACM Special Interest Group on Software Engineering (SIGSOFT);Computer Society (IEEE-CS)","IEEE Computer Society","40th ACM/IEEE International Conference on Software Engineering: Software Engineering Education and Training, ICSE-SEET 2018","30 May 2018 through 1 June 2018",,137346,"Conference Paper","Scopus"
"SATD detector: A text-mining-based self-Admitted technical debt detection tool","Liu Z., Huang Q., Xia X., Shihab E., Lo D., Li S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049683556&doi=10.1145%2f3183440.3183478&partnerID=40&md5=fc9b4f29011dee78dffc65dcb86362ba",2018,"Proceedings - International Conference on Software Engineering","In software projects, technical debt metaphor is used to describe the situation where developers and managers have to accept compromises in long-Term software quality to achieve short-Term goals. There are many types of technical debt, and self-Admitted technical debt (SATD) was proposed recently to consider debt that is introduced intentionally (e.g., through temporaryfi x) and admitted by developers themselves. Previous work has shown that SATD can be successfully detected using source code comments. However, most current state-of-The-Art approaches identify SATD comments through pattern matching, which achieve high precision but very low recall. That means they may miss many SATD comments and are not practical enough. In this paper, we propose SATD Detector, a tool that is able to (i) automatically detect SATD comments using text mining and (ii) highlight, list and manage detected comments in an integrated development environment (IDE). This tool consists of a Java library and an Eclipse plug-in. The Java library is the back-end, which provides command-line interfaces and Java APIs to re-Train the text mining model using users' data and automatically detect SATD comments using either the build-in model or a user-specified model. The Eclipse plug-in, which is the front-end, first leverages our pre-Trained composite classifier to detect SATD comments, and then highlights and marks these detected comments in the source code editor of Eclipse. In addition, the Eclipse plug-in provides a view in IDE which collects all detected comments for management. Demo URL: https://youtu.be/sn4gU2qhGm0 Java library download: https://git.io/vNdnY Eclipse plug-in download: https://goo.gl/ZzjBzp. Â© 2018 Authors.","Eclipse plug-in; SATD detection; Self-Admitted technical debt","ACM Special Interest Group on Software Engineering (SIGSOFT);Computer Society (IEEE-CS)","IEEE Computer Society","40th ACM/IEEE International Conference on Software Engineering, ICSE 2018","27 May 2018 through 3 June 2018",,137351,"Conference Paper","Scopus"
"Deep code search","Gu X., Zhang H., Kim S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049401158&doi=10.1145%2f3180155.3180167&partnerID=40&md5=e19ad8fd713785c50346e6142aa159c4",2018,"Proceedings - International Conference on Software Engineering","To implement a program functionality, developers can reuse previously written code snippets by searching through a large-scale codebase. Over the years, many code search tools have been proposed to help developers. The existing approaches often treat source code as textual documents and utilize information retrieval models to retrieve relevant code snippets that match a given query. These approaches mainly rely on the textual similarity between source code and natural language query. They lack a deep understanding of the semantics of queries and source code. In this paper, we propose a novel deep neural network named CODEnn (Code-Description Embedding Neural Network). Instead of matching text similarity, CODEnn jointly embeds code snippets and natural language descriptions into a high-dimensional vector space, in such a way that code snippet and its corresponding description have similar vectors. Using the unified vector representation, code snippets related to a natural language query can be retrieved according to their vectors. Semantically related words can also be recognized and irrelevant/noisy keywords in queries can be handled. As a proof-of-concept application, we implement a code search tool named DeepCS using the proposed CODEnn model. We empirically evaluate DeepCS on a large scale codebase collected from GitHub. The experimental results show that our approach can effectively retrieve relevant code snippets and outperforms previous techniques. Â© 2018 ACM.","Code search; Deep learning; Joint embedding",,"IEEE Computer Society","40th International Conference on Software Engineering, ICSE 2018","27 May 2018 through 3 June 2018",,137142,"Conference Paper","Scopus"
"A framework for semi-automated co-evolution of security knowledge and system models","BÃ¼rger J., StrÃ¼ber D., GÃ¤rtner S., Ruhroth T., JÃ¼rjens J., Schneider K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043281006&doi=10.1016%2fj.jss.2018.02.003&partnerID=40&md5=a6ec4e3a1775b0421f6aa02a243f64b0",2018,"Journal of Systems and Software","Security is an important and challenging quality aspect of software-intensive systems, becoming even more demanding regarding long-living systems. Novel attacks and changing laws lead to security issues that did not necessarily rise from a flawed initial design, but also when the system fails to keep up with a changing environment. Thus, security requires maintenance throughout the operation phase. Ongoing adaptations in response to changed security knowledge are inevitable. A necessary prerequisite for such adaptations is a good understanding of the security-relevant parts of the system and the security knowledge. We present a model-based framework for supporting the maintenance of security during the long-term evolution of a software system. It uses ontologies to manage the system-specific and the security knowledge. With model queries, graph transformation and differencing techniques, knowledge changes are analyzed and the system model is adapted. We introduce the novel concept of Security Maintenance Rules to couple the evolution of security knowledge with co-evolutions of the system model. As evaluation, community knowledge about vulnerabilities is used (Common Weakness Enumeration database). We show the applicability of the framework to the iTrust system from the medical care domain and hence show the benefits of supporting co-evolution for maintaining secure systems. Â© 2018 Elsevier Inc.","Co-evolution; Security impact analysis; Security requirements; Software design; Software evolution",,"Elsevier Inc.",,,,,"Article","Scopus"
"Estimating functional connectivity symmetry between oxy- and deoxy-haemoglobin: Implications for fNIRS connectivity analysis","Montero-Hernandez S., Orihuela-Espina F., Sucar L.E., Pinti P., Hamilton A., Burgess P., Tachtsidis I.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046808290&doi=10.3390%2fa11050070&partnerID=40&md5=c2b52dc96329a44cd1e5c6ddf3d03fd0",2018,"Algorithms","Functional Near InfraRed Spectroscopy (fNIRS) connectivity analysis is often performed using the measured oxy-haemoglobin (HbO2) signal, while the deoxy-haemoglobin (HHb) is largely ignored. The in-common information of the connectivity networks of both HbO2 and HHb is not regularly reported, or worse, assumed to be similar. Here we describe a methodology that allows the estimation of the symmetry between the functional connectivity (FC) networks of HbO2 and HHb and propose a differential symmetry index (DSI) indicative of the in-common physiological information. Our hypothesis is that the symmetry between FC networks associated with HbO2 and HHb is above what should be expected from random networks. FC analysis was done in fNIRS data collected from six freely-moving healthy volunteers over 16 locations on the prefrontal cortex during a real-world task in an out-of-the-lab environment. In addition, systemic data including breathing rate (BR) and heart rate (HR) were also synchronously collected and used within the FC analysis. FC networks for HbO2 and HHb were established independently using a Bayesian networks analysis. The DSI between both haemoglobin (Hb) networks with and without systemic influence was calculated. The relationship between the symmetry of HbO2 and HHb networks, including the segregational and integrational characteristics of the networks (modularity and global efficiency respectively) were further described. Consideration of systemic information increases the path lengths of the connectivity networks by 3%. Sparse networks exhibited higher asymmetry than dense networks. Importantly, our experimental connectivity networks symmetry between HbO2 and HHb departs from random (t-test: t(509) = 26.39, p &lt; 0.0001). The DSI distribution suggests a threshold of 0.2 to decide whether both HbO2 and HHb FC networks ought to be studied. For sparse FC networks, analysis of both haemoglobin species is strongly recommended. Our DSI can provide a quantifiable guideline for deciding whether to proceed with single or both Hb networks in FC analysis. Â© 2018 by the authors.","fNIRS; Functional connectivity; Prefrontal cortex; Symmetry",,"MDPI AG",,,,,"Article","Scopus"
"Doppio: Tracking UI flows and code changes for app development","Chi P.-Y., Hu S.-P., Li Y.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046933574&doi=10.1145%2f3173574.3174029&partnerID=40&md5=6eab8da51464030181ec1111f4db713d",2018,"Conference on Human Factors in Computing Systems - Proceedings","Developing interactive systems often involves a large set of callback functions for handling user interaction, which makes it challenging to manage UI behaviors, create descriptive documentation, and track code revisions. We developed Doppio, a tool that automatically tracks and visualizes UI flows and their changes based on source code. For each input event listener of a widget, e.g., onClick of an Android View class, Doppio captures and associates its UI output from a program execution with its code snippet from the codebase. It automatically generates a screenflow diagram organized by the callback methods and interaction flow, where developers can review the code and UI revisions interactively. Doppio, as an IDE plugin, is seamlessly integrated into a common development workflow. Our studies show that our tool is able to generate quality visual documentation and helped participants understand unfamiliar source code and track changes. Â© 2018 Copyright is held by the owner/author(s).","Android; Demonstrations; IDEs; Mobile apps; Screencast videos; Screenflow diagram; Software documentation","ACM SIGCHI","Association for Computing Machinery","2018 CHI Conference on Human Factors in Computing Systems, CHI 2018","21 April 2018 through 26 April 2018",,135975,"Conference Paper","Scopus"
"Towards an intelligent fault prediction code editor to improve software quality using deep learning","Jindal V.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053691244&doi=10.1145%2f3191697.3213793&partnerID=40&md5=425166249b18527f18bd4e8c97934717",2018,"ACM International Conference Proceeding Series","Software quality assurance has become the pillar for success in software companies. High quality, low maintenance programs can be achieved if fault-prone modules can be identified early in the development lifecycle. In this paper, we propose a new intelligent Integrated Development Environment (IDE) which seamlessly allow programmers to test their code for faults using prior source code databases. Our IDE is built upon deep learning models for making recommendations. The editor also gives scores to programmers on their program design. We evaluate and validate our approach using famous NASA code repositories. Â© 2018 Copyright held by the owner/author(s).","Deep learning; Fault prediction; Quality assurance; Software quality","AOSA;INRIA;Oracle;UCA","Association for Computing Machinery","2nd International Conference on Art, Science, and Engineering of Programming, Programming 2018","9 April 2018 through 12 April 2018",,137691,"Conference Paper","Scopus"
"Debugging support for big data processing applications","Marra M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053683403&doi=10.1145%2f3191697.3213801&partnerID=40&md5=8b047b9d0fca92406a953700ce508be1",2018,"ACM International Conference Proceeding Series","Current trends in Big Data processing indicate that the volume, velocity and variety of data are increasing quickly due to an explosion on diversity and number of sources of information. This poses challenges for Big Data frameworks to be able to meet the new requirements of the emerging real-time streaming data processing applications. This research project focuses on the academic study of integrated development environments and debugging tools to assist the software development of Big Data applications. Â© 2018 Copyright held by the owner/author(s).","Big Data; Debugging; Meta-Level Interfaces; Tools","AOSA;INRIA;Oracle;UCA","Association for Computing Machinery","2nd International Conference on Art, Science, and Engineering of Programming, Programming 2018","9 April 2018 through 12 April 2018",,137691,"Conference Paper","Scopus"
"RETICULA: Real-time code quality assessment","Franzio L., Lin B., Lanza M., Bavota G.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050980424&doi=10.1109%2fSANER.2018.8330256&partnerID=40&md5=e66f4d3510318e6463232bbbeb88d00a",2018,"25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings","Code metrics can be used to assess the internal quality of software systems, and in particular their adherence to good design principles. While providing hints about code quality, metrics are difficult to interpret. Indeed, they take a code component as input and assess a quality attribute (e.g., code readability) by providing a number as output. However, it might be unclear for developers whether that value should be considered good or bad for the specific code at hand. We present RETICULA (REal TIme Code qUaLity Assessment), a plugin for the IntelliJ IDE to assist developers in perceiving code quality during software development. RETICULA compares the quality metrics for a project (or a single class) under development in the IDE with those of similar open source systems (classes) previously analyzed. With the visualized results, developers can gain insights about the quality of their code. A video illustrating the features of RETICULA can be found at: https://reticulaplugin.github.io/. Â© 2018 IEEE.",,"IEEE;IEEE Computer Society;Technical Council on Software Engineering (TCSE);University of Molise","Institute of Electrical and Electronics Engineers Inc.","25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018","20 March 2018 through 23 March 2018",,135723,"Conference Paper","Scopus"
"Duplicate question detection in stack overflow: A reproducibility study","Silva R.F.G., PaixÃ£o K., De Almeida Maia M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050956978&doi=10.1109%2fSANER.2018.8330262&partnerID=40&md5=3dfd1df827f06d5ec93f2a3745aec025",2018,"25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings","Stack Overflow has become a fundamental element of developer toolset. Such influence increase has been accompanied by an effort from Stack Overflow community to keep the quality of its content. One of the problems which jeopardizes that quality is the continuous growth of duplicated questions. To solve this problem, prior works focused on automatically detecting duplicated questions. Two important solutions are DupPredictor and Dupe. Despite reporting significant results, both works do not provide their implementations publicly available, hindering subsequent works in scientific literature which rely on them. We executed an empirical study as a reproduction of DupPredictor and Dupe. Our results, not robust when attempted with different set of tools and data sets, show that the barriers to reproduce these approaches are high. Furthermore, when applied to more recent data, we observe a performance decay of our both reproductions in terms of recall-rate over time, as the number of questions increases. Our findings suggest that the subsequent works concerning detection of duplicated questions in Question and Answer communities require more investigation to assert their findings. Â© 2018 IEEE.","Classification; Duplicate questions; Question quality; Stack Overflow","IEEE;IEEE Computer Society;Technical Council on Software Engineering (TCSE);University of Molise","Institute of Electrical and Electronics Engineers Inc.","25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018","20 March 2018 through 23 March 2018",,135723,"Conference Paper","Scopus"
"Context is king: The developer perspective on the usage of static analysis tools","Vassallo C., Panichella S., Palomba F., Proksch S., Zaidman A., Gall H.C.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047084812&doi=10.1109%2fSANER.2018.8330195&partnerID=40&md5=a3b226d41b12a29769bd7e1f407af32e",2018,"25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings","Automatic static analysis tools (ASATs) are tools that support automatic code quality evaluation of software systems with the aim of (i) avoiding and/or removing bugs and (ii) spotting design issues. Hindering their wide-spread acceptance are their (i) high false positive rates and (ii) low comprehensibility of the generated warnings. Researchers and ASATs vendors have proposed solutions to prioritize such warnings with the aim of guiding developers toward the most severe ones. However, none of the proposed solutions considers the development context in which an ASAT is being used to further improve the selection of relevant warnings. To shed light on the impact of such contexts on the warnings configuration, usage and adopted prioritization strategies, we surveyed 42 developers (69% in industry and 31% in open source projects) and interviewed 11 industrial experts that integrate ASATs in their workflow. While we can confirm previous findings on the reluctance of developers to configure ASATs, our study highlights that (i) 71% of developers do pay attention to different warning categories depending on the development context, and (ii) 63% of our respondents rely on specific factors (e.g., team policies and composition) when prioritizing warnings to fix during their programming. Our results clearly indicate ways to better assist developers by improving existing warning selection and prioritization strategies. Â© 2018 IEEE.","Code Review; Continuous Integration; Development Context; Empirical Study; Static Analysis","IEEE;IEEE Computer Society;Technical Council on Software Engineering (TCSE);University of Molise","Institute of Electrical and Electronics Engineers Inc.","25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018","20 March 2018 through 23 March 2018",,135723,"Conference Paper","Scopus"
"Classifying stack overflow posts on API issues","Ahasanuzzaman M., Asaduzzaman M., Roy C.K., Schneider K.A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051020036&doi=10.1109%2fSANER.2018.8330213&partnerID=40&md5=8462f20d4e0abdd1c1d155f842703790",2018,"25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings","The design and maintenance of APIs are complex tasks due to the constantly changing requirements of its users. Despite the efforts of its designers, APIs may suffer from a number of issues (such as incomplete or erroneous documentation, poor performance, and backward incompatibility). To maintain a healthy client base, API designers must learn these issues to fix them. Question answering sites, such as Stack Overflow (SO), has become a popular place for discussing API issues. These posts about API issues are invaluable to API designers, not only because they can help to learn more about the problem but also because they can facilitate learning the requirements of API users. However, the unstructured nature of posts and the abundance of non-issue posts make the task of detecting SO posts concerning API issues difficult and challenging. In this paper, we first develop a supervised learning approach using a Conditional Random Field (CRF), a statistical modeling method, to identify API issue-related sentences. We use the above information together with different features of posts and experience of users to build a technique, called CAPS, that can classify SO posts concerning API issues. Evaluation of CAPS using carefully curated SO posts on three popular API types reveals that the technique outperforms all three baseline approaches we consider in this study. We also conduct studies to test the generalizability of CAPS results and to understand the effects of different sources of information on it. Â© 2018 IEEE.","API Issue; feature extraction; Stack Overflow; text classification; unstructured data mining","IEEE;IEEE Computer Society;Technical Council on Software Engineering (TCSE);University of Molise","Institute of Electrical and Electronics Engineers Inc.","25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018","20 March 2018 through 23 March 2018",,135723,"Conference Paper","Scopus"
"Noise in Mylyn interaction traces and its impact on developers and recommendation systems","Soh Z., Khomh F., GuÃ©hÃ©neuc Y.-G., Antoniol G.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021764946&doi=10.1007%2fs10664-017-9529-x&partnerID=40&md5=ef654e5199442a38c2629670c505c09d",2018,"Empirical Software Engineering","Interaction traces (ITs) are developersâ€™ logs collected while developers maintain or evolve software systems. Researchers use ITs to study developersâ€™ editing styles and recommend relevant program entities when developers perform changes on source code. However, when using ITs, they make assumptions that may not necessarily be true. This article assesses the extent to which researchersâ€™ assumptions are true and examines noise in ITs. It also investigates the impact of noise on previous studies. This article describes a quasi-experiment collecting both Mylyn ITs and video-screen captures while 15 participants performed four realistic software maintenance tasks. It assesses the noise in ITs by comparing Mylyn ITs and the ITs obtained from the video captures. It proposes an approach to correct noise and uses this approach to revisit previous studies. The collected data show that Mylyn ITs can miss, on average, about 6% of the time spent by participants performing tasks and can contain, on average, about 85% of false edit events, which are not real changes to the source code. The approach to correct noise reveals about 45% of misclassification of ITs. It can improve the precision and recall of recommendation systems from the literature by up to 56% and 62%, respectively. Mylyn ITs include noise that biases subsequent studies and, thus, can prevent researchers from assisting developers effectively. They must be cleaned before use in studies and recommendation systems. The results on Mylyn ITs open new perspectives for the investigation of noise in ITs generated by other monitoring tools such as DFlow, FeedBag, and Mimec, and for future studies based on ITs. Â© 2017, Springer Science+Business Media, LLC.","Editing behaviour; Mylyn Interaction traces; Noise; Recommendation systems; Software maintenance",,"Springer New York LLC",,,,,"Article","Scopus"
"Security slicing for auditing common injection vulnerabilities","ThomÃ© J., Shar L.K., Bianculli D., Briand L.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014466419&doi=10.1016%2fj.jss.2017.02.040&partnerID=40&md5=dd25e5075271bfe0d314d79bc49ed44f",2018,"Journal of Systems and Software","Cross-site scripting and injection vulnerabilities are among the most common and serious security issues for Web applications. Although existing static analysis approaches can detect potential vulnerabilities in source code, they generate many false warnings and source-sink traces with irrelevant information, making their adoption impractical for security auditing. One suitable approach to support security auditing is to compute a program slice for each sink, which contains all the information required for security auditing. However, such slices are likely to contain a large amount of information that is irrelevant to security, thus raising scalability issues for security audits. In this paper, we propose an approach to assist security auditors by defining and experimenting with pruning techniques to reduce original program slices to what we refer to as security slices, which contain sound and precise information. To evaluate the proposed approach, we compared our security slices to the slices generated by a state-of-the-art program slicing tool, based on a number of open-source benchmarks. On average, our security slices are 76% smaller than the original slices. More importantly, with security slicing, one needs to audit approximately 1% of the total code to fix all the vulnerabilities, thus suggesting significant reduction in auditing costs. Â© 2018 Elsevier Inc.","Automated code fixing; Security auditing; Static analysis; Vulnerability",,"Elsevier Inc.",,,,,"Article","Scopus"
"Design of interactive paging and locating device for GPS applications","Muraleedharan A., Bhakthavatchalu R.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046672204&doi=10.1109%2fICCSP.2017.8286507&partnerID=40&md5=af9079b7ed1c8ffeec716fa91d2b524d",2018,"Proceedings of the 2017 IEEE International Conference on Communication and Signal Processing, ICCSP 2017","This paper proposes design of an interactive paging and locating device. It is a standalone device which integrates the modern communication methods in it. It is an interactive device and it communicates with the user via the touch screen and through paging. The proposed system allows the user to send location information via the Global Positioning System (GPS) module. This device finds its application in vehicles by helping them to track the locations. The device proposed is designed using the IDE tool Atollic TrueSTUDIO which provides fast write and debug options thereby developing a software of superior quality. STM32CubeMX is the graphical tool used for configuring the microcontroller. The STM32F10xxx CortexTM-M3 processor is the hardware used to implement the proposed design which is a high performance 32-bit processor. In developing countries, other than India, where the hiring of vehicles is common, the system proposed here can be of great significance. This device can also be incorporated with other additional features and can be used as a mobile standalone device in future. Â© 2017 IEEE.","Global Positioning System; Global System for Mobile Communication; STM32 Microcontrollers",,"Institute of Electrical and Electronics Engineers Inc.","6th IEEE International Conference on Communication and Signal Processing, ICCSP 2017","6 April 2017 through 8 April 2017",,134565,"Conference Paper","Scopus"
"Detection technology and application of clone refactoring","Yao Y., Liu D., Zhang L.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047078329&doi=10.1145%2f3180374.3181332&partnerID=40&md5=1be2de3465f4fc4d17dd6e85e1943d08",2018,"ACM International Conference Proceeding Series","Clone code is a similar part of code, such code may seriously affect the maintainability of the software and reduce the quality of code. In order to eliminate the negative impact of clone code, the researchers offer many methods to eliminate clone code and refactoring is an important part in them. In this paper, several different methods and tools are introduced, the advantages and disadvantages of various methods and tools are summarized, apart from this, we also describe the important application of clone refactoring in maintaining the quality of code, at last, we discussed the challenges of the current clone refactoring detection. Â© 2018 Association for Computing Machinery.","Clone code; Maintainability; Refactoring; Software quality","Wuhan University","Association for Computing Machinery","2nd International Conference on Management Engineering, Software Engineering and Service Sciences, ICMSS 2018","13 January 2018 through 15 January 2018",,136015,"Conference Paper","Scopus"
"A fuzzy inference system to recommend skills for source code review using eye movement data","Chandrika K.R., Amudha J.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044752149&doi=10.3233%2fJIFS-169467&partnerID=40&md5=f01b06284f9a6650f6afbff2e90ca738",2018,"Journal of Intelligent and Fuzzy Systems","A quality software development is inclined to the software developer skills. The research focus on recommending the skills of individuals based on the eye movement data. The paper sketches a study conducted on students who are future developers. A fuzzy based recommendation system was implemented to recommend two skills, code coverage and debugging skills that are primary in source code review. The code coverage inference system recommends individual code coverage as maximum, average and minimum and the debugging fuzzy inference system recommends debugging skills as skilled, unskilled and expert. Â© 2018 - IOS Press and the authors. All rights reserved.","code coverage; debugging; eye tracking; recommendation system; skills; Software engineering; source cod review",,"IOS Press",,,,,"Conference Paper","Scopus"
"Educating users to formulate questions in Q&A platforms: A scaffold for google sheets","DÃ­az O., Contell J.P.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048467379&doi=10.1007%2f978-3-319-91563-0_10&partnerID=40&md5=2c63d7dc9d9cb9c54acb3ec9c26558e6",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","Different studies point out that spreadsheets are easy to use but difficult to master. When difficulties arise, home users and small-and-medium organizations might not always resort to a help desk. Alternatively, Question&Answer platforms (e.g. Stack Overflow) come in handy. Unfortunately, we can not always expect home users to properly set good questions. However, examples can be a substitute for long explanations. This is particularly important for our target audience who might lack the skills to describe their needs in abstract terms, and hence, examples might be the easiest way to get the idea through. This sustains the effort to leverage existing spreadsheet tools with example-centric inline question posting. This paper describes such an effort for Google Sheets. The extension assists users in posing their example-based questions without leaving Google Sheets. These questions are next transparently channeled to Stack Overflow. Â© Springer International Publishing AG, part of Springer Nature 2018.","Q&A platforms; Social computing; Spreadsheets; StackOverflow",,"Springer Verlag","30th International Conference on Advanced Information Systems Engineering, CAiSE 2018","11 June 2018 through 15 June 2018",,214019,"Conference Paper","Scopus"
"CHAIN: Developing model-driven contextual help for adaptive user interfaces","Akiki P.A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032440442&doi=10.1016%2fj.jss.2017.10.017&partnerID=40&md5=9caccf18b9ea5bc6c039fcd187ee3f67",2018,"Journal of Systems and Software","Adaptive user interfaces (UIs) change their presentation at runtime to remain usable in different contexts-of-use. Such changes can cause discrepancies between the UI and static help materials, e.g., videos and screenshots, thereby negatively impacting the latter's usefulness (usability and utility). This paper presents Contextual Help for Adaptive INterfaces (CHAIN), which is an approach for developing model-driven contextual help that maintains its usefulness across UI adaptations. This trait is achieved by interpreting the help models at runtime and overlaying instructions on the running adapted UI. A language called Contextual Help for Adaptive INterfaces eXtensible Markup Language (CHAINXML) and a visual notation were developed for expressing and depicting help models. A technique was also devised for presenting CHAIN help models over legacy applications, whether or not their source-code is available. A supporting tool was developed as an extension to Cedar Studio. This work was empirically evaluated in two studies. The first study performed a preliminary evaluation of CHAIN's visual notation. The second study evaluated CHAIN's strengths and shortcomings after using it to develop help for real-life adaptive UIs. The results gave a positive indication about CHAIN's technical qualities and provided insights that could inform future work. Â© 2017 Elsevier Inc.","Adaptive user interfaces; Contextual help; Design tools and techniques; Model-driven help; Software support",,"Elsevier Inc.",,,,,"Article","Scopus"
"Towards a framework for the design of quantitative experiments: Human-computer interaction and accessibility research","Sandnes F.E., Eika E., Medola F.O.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050587620&doi=10.1007%2f978-3-319-92049-8_8&partnerID=40&md5=59a4521b4e9aae83dbddd5aa5564fa6f",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","Many students and researchers struggle with the design and analysis of empirical experiments. Such issue may be caused by lack of knowledge about inferential statistics and suitable software tools. Often, students and researchers conduct experiments without having a complete plan for the entire lifecycle of the process. Difficulties associated with the statistical analysis are often ignored. Consequently, one may end up with data that cannot be easily analyzed. This paper discusses the concept sketch of a framework that intends to help students and researchers to design correct empirical experiments by making sound design decisions early in the research process. The framework consists of an IDE, i.e., Integrated (statistical experiment) Development Environment. This IDE helps the user structures an experiment by giving continuous feedback drawing the experimenterâ€™s attention towards potential problems. The output of the IDE is an experimental structure and data format that can be imported to common statistical packages such as JASP in addition to providing guidance about what tests to use. Â© Springer International Publishing AG, part of Springer Nature 2018.","Design of experiments; Human computer interaction; Inferential statistics; Integrated development environment; Universal accessibility",,"Springer Verlag","12th International Conference on Universal Access in Human-Computer Interaction, UAHCI 2018 Held as Part of HCI International 2018","15 July 2018 through 20 July 2018",,216059,"Conference Paper","Scopus"
"Active automata learning in practice: An annotated bibliography of the years 2011 to 2016","Howar F., Steffen B.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051114083&doi=10.1007%2f978-3-319-96562-8_5&partnerID=40&md5=15eef7e3cc4dcb510cdcbc3b8ed479bf",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","Active automata learning is slowly becoming a standard tool in the toolbox of the software engineer. As systems become ever more complex and development becomes more distributed, inferred models of system behavior become an increasingly valuable asset for understanding and analyzing a systemâ€™s behavior. Five years ago (in 2011) we have surveyed the then current state of active automata learning research and applications of active automata learning in practice. We predicted four major topics to be addressed in the then near future: efficiency, expressivity of models, bridging the semantic gap between formal languages and analyzed components, and solutions to the inherent problem of incompleteness of active learning in black-box scenarios. In this paper we review the progress that has been made over the past five years, assess the status of active automata learning techniques with respect to applications in the field of software engineering, and present an updated agenda for future research. Â© Springer International Publishing AG, part of Springer Nature 2018.",,,"Springer Verlag","International Dagstuhl Seminar 16172 Machine Learning for Dynamic Software Analysis: Potentials and Limits, 2016","24 April 2016 through 27 April 2016",,216439,"Conference Paper","Scopus"
"Detecting broken pointcuts using structural commonality and degree of interest","Khatchadourian R., Rashid A., Masuhara H., Watanabe T.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021855542&doi=10.1016%2fj.scico.2017.06.011&partnerID=40&md5=9adb26aa84adb8ac3495944579a1d820",2017,"Science of Computer Programming","Pointcut fragility is a well-documented problem in Aspect-Oriented Programming; changes to the base-code can lead to join points incorrectly falling in or out of the scope of pointcuts. Deciding which pointcuts have broken due to base-code changes is a daunting venture, especially in large and complex systems. We present an automated approach that recommends pointcuts that are likely to require modification due to a particular base-code change, as well as ones that do not. Our hypothesis is that join points selected by a pointcut exhibit common structural characteristics. Patterns describing such commonality are used to recommend pointcuts that have potentially broken with a degree of confidence as the developer is typing. The approach is implemented as an extension to the popular Mylyn Eclipse IDE plug-in, which maintains focused contexts of entities relevant to the task at hand using a Degree of Interest (DOI) model. We show that it is accurate in revealing broken pointcuts by applying it to multiple versions of several open source projects and evaluating the quality of the recommendations produced against actual modifications. We found that our tool made broken pointcuts 2.14 times more interesting in the DOI model than unbroken ones, with a p-value under 0.1, indicating a significant difference in final DOI value between the two kinds of pointcuts (i.e., broken and unbroken). Â© 2017 Elsevier B.V.","Software development environments; Software maintenance; Software tools",,"Elsevier B.V.",,,,,"Article","Scopus"
"Improved query reformulation for concept location using CodeRank and document structures","Rahman M.M., Roy C.K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041441954&doi=10.1109%2fASE.2017.8115655&partnerID=40&md5=a9887fa5664e9454cf3acfeab9fffd3c",2017,"ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering","During software maintenance, developers usually deal with a significant number of software change requests. As a part of this, they often formulate an initial query from the request texts, and then attempt to map the concepts discussed in the request to relevant source code locations in the software system (a.k.a., concept location). Unfortunately, studies suggest that they often perform poorly in choosing the right search terms for a change task. In this paper, we propose a novel technique-ACER-that takes an initial query, identifies appropriate search terms from the source code using a novel term weight-CodeRank, and then suggests effective reformulation to the initial query by exploiting the source document structures, query quality analysis and machine learning. Experiments with 1,675 baseline queries from eight subject systems report that our technique can improve 71% of the baseline queries which is highly promising. Comparison with five closely related existing techniques in query reformulation not only validates our empirical findings but also demonstrates the superiority of our technique. Â© 2017 IEEE.","CodeRank; concept location; data resampling; query quality analysis; Query reformulation; term weighting","College of Engineering;Denso;et al.;Microsoft;NASA;University of Minnesota, Software Engineering","Institute of Electrical and Electronics Engineers Inc.","32nd IEEE/ACM International Conference on Automated Software Engineering, ASE 2017","30 October 2017 through 3 November 2017",,132671,"Conference Paper","Scopus"
"Automatic summarization of API reviews","Uddin G., Khomh F.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041432441&doi=10.1109%2fASE.2017.8115629&partnerID=40&md5=d527c2e5b54a81d476c737622ecc8427",2017,"ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering","With the proliferation of online developer forums as informal documentation, developers often share their opinions about the APIs they use. However, given the plethora of opinions available for an API in various online developer forums, it can be challenging for a developer to make informed decisions about the APIs. While automatic summarization of opinions have been explored for other domains (e.g., cameras, cars), we found little research that investigates the benefits of summaries of public API reviews. In this paper, we present two algorithms (statistical and aspect-based) to summarize opinions about APIs. To investigate the usefulness of the techniques, we developed, Opiner, an online opinion summarization engine that presents summaries of opinions using both our proposed techniques and existing six off-the-shelf techniques. We investigated the usefulness of Opiner using two case studies, both involving professional software engineers. We found that developers were interested to use our proposed summaries much more frequently than other summaries (daily vs once a year) and that while combined with Stack Overflow, Opiner helped developers to make the right decision with more accuracy and confidence and in less time. Â© 2017 IEEE.","API informal documentation; Opinion mining; opinion summaries; study; summary quality","College of Engineering;Denso;et al.;Microsoft;NASA;University of Minnesota, Software Engineering","Institute of Electrical and Electronics Engineers Inc.","32nd IEEE/ACM International Conference on Automated Software Engineering, ASE 2017","30 October 2017 through 3 November 2017",,132671,"Conference Paper","Scopus"
"Understanding stack overflow code fragments","Treude C., Robillard M.P.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040611845&doi=10.1109%2fICSME.2017.24&partnerID=40&md5=eda9d7a081ea27dbbcf50896cb3efc9a",2017,"Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017","Code fragments posted in answers on Q&A forums can form an important source of developer knowledge. However, effective reuse of code fragments found online often requires information other than the code fragment alone. We report on the results of a survey-based study to investigate to what extent developers perceive Stack Overflow code fragments to be self-explanatory. As part of the study, we also investigated the types of information missing from fragments that were not self-explanatory. We find that less than half of the Stack Overflow code fragments in our sample are considered to be self-explanatory by the 321 participants who answered our survey, and that the main issues that negatively affect code fragment understandability include incomplete fragments, code quality, missing rationale, code organization, clutter, naming issues, and missing domain information. This study is a step towards understanding developers' information needs as they relate to code fragments, and how these needs can be addressed. Â© 2017 IEEE.",,"IEEE Computer Society;IEEE Technical Council on Software Engineering (TCSE)","Institute of Electrical and Electronics Engineers Inc.","2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017","19 September 2017 through 22 September 2017",,132283,"Conference Paper","Scopus"
"Software practitioner perspectives on merge conflicts and resolutions","McKee S., Nelson N., Sarma A., Dig D.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040567114&doi=10.1109%2fICSME.2017.53&partnerID=40&md5=d9c046eb04d3f6280e533a6f8e568097",2017,"Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017","Merge conflicts occur when software practitioners need to work in parallel and are inevitable in software development. Tool builders and researchers have focused on the prevention and resolution of merge conflicts, but there is little empirical knowledge about how practitioners actually approach and perform merge conflict resolution. Without such knowledge, tool builders might be building on wrong assumptions and researchers might miss opportunities for improving the state of the art. We conducted semi-structured interviews of 10 software practitioners across 7 organizations, including both open-source and commercial projects. We identify the key concepts and perceptions from practitioners, which we then validated via a survey of 162 additional practitioners. We find that practitioners are directly impacted by their perception of the complexity of the conflicting code, and may alter the timeline in which to resolve these conflicts, as well as the methods employed for conflict resolution based upon that initial perception. Practitioners' perceptions alter the impact of tools and processes that have been designed to preemptively and efficiently resolve merge conflicts. Understanding whether practitioners will react according to standard use cases is important when creating human-oriented tools to support development processes. Â© 2017 IEEE.",,"IEEE Computer Society;IEEE Technical Council on Software Engineering (TCSE)","Institute of Electrical and Electronics Engineers Inc.","2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017","19 September 2017 through 22 September 2017",,132283,"Conference Paper","Scopus"
"Investigating the Use of Code Analysis and NLP to Promote a Consistent Usage of Identifiers","Lin B., Scalabrino S., Mocci A., Oliveto R., Bavota G., Lanza M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046086743&doi=10.1109%2fSCAM.2017.17&partnerID=40&md5=352c02bbe9b4ea03256de6df9354b8e5",2017,"Proceedings - 2017 IEEE 17th International Working Conference on Source Code Analysis and Manipulation, SCAM 2017","Meaningless identifiers as well as inconsistent use of identifiers in the source code might hinder code readability and result in increased software maintenance efforts. Over the past years, effort has been devoted to promoting a consistent usage of identifiers across different parts of a system through approaches exploiting static code analysis and Natural Language Processing (NLP). These techniques have been evaluated in small-scale studies, but it is unclear how they compare to each other and how they complement each other. Furthermore, a full-fledged larger empirical evaluation is still missing.,We aim at bridging this gap. We asked developers of five projects to assess the meaningfulness of the recommendations generated by three techniques, two already existing in the literature (one exploiting static analysis, one using NLP) and a novel one we propose. With a total of 922 rename refactorings evaluated, this is, to the best of our knowledge, the largest empirical study conducted to assess and compare rename refactoring tools promoting a consistent use of identifiers. Our study sheds light on the current state-of-the-art in rename refactoring recommenders, and indicates directions for future work. Â© 2017 IEEE.",,"IEEE Computer Society;IEEE Technical Council on Software Engineering (TCSE)","Institute of Electrical and Electronics Engineers Inc.","17th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2017","17 September 2017 through 18 September 2017",,133891,"Conference Paper","Scopus"
"Patterns of developers behaviour: A 1000-hour industrial study","Astromskis S., Bavota G., Janes A., Russo B., Di Penta M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022015886&doi=10.1016%2fj.jss.2017.06.072&partnerID=40&md5=eff27cb76ac73df078943d3b3fda434c",2017,"Journal of Systems and Software","Monitoring developersâ€™ activity in the Integrated Development Environment (IDE) and, in general, in their working environment, can be useful to provide context to recommender systems, and, in perspective, to develop smarter IDEs. This paper reports results of a long (about 1000 h) observational study conducted in an industrial environment, in which we captured developersâ€™ interaction with the IDE, with various applications available in their workstation, and related them with activities performed on source code files. Specifically, the study involved six developers working on three software systems and investigated (i) how much time developers spent on various activities and how they shift from one activity to another (ii) how developers navigate through the software architecture during their task, and (iii) how the complexity and readability of source code may trigger further actions, such as requests for help or browsing/changing other files. Results of our study suggest that: (i) not surprisingly, developers spend most or their time (âˆ¼ 61%) in development activities while the usage of online help is limited (2%) but intensive in specific development sessions; (ii) developers often execute the system under development after working on code, likely to verify the effect of applied changes on the system's behaviour; (iii) while working on files having a high complexity, developers tend to more frequently execute the system as well as to use more online help websites. Â© 2017 Elsevier Inc.","Case study; Monitoring developersâ€™ activities",,"Elsevier Inc.",,,,,"Article","Scopus"
"Distributed computational model for shared processing on Cyber-Physical System environments","Mora H., Colom J.F., Gil D., Jimeno-Morenilla A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029422054&doi=10.1016%2fj.comcom.2017.07.009&partnerID=40&md5=bddde286c25c09791f9ddc36bbcc2f45",2017,"Computer Communications","Cyber-Physical Systems typically consist of a combination of mobile devices, embedded systems and computers to monitor, sense, and actuate with the surrounding real world. These computing elements are usually wireless, interconnected to share data and interact with each other, with the server part and also with cloud computing services. In such a heterogeneous environment, new applications arise to meet ever-increasing needs and these are an important challenge to the processing capabilities of devices. For example, automatic driving systems, manufacturing environments, smart city management, etc. To meet the requirements of said application contexts, the system can create computing processes to distribute the workload over the network and/or a cloud computing server. Multiple options arise in relation to what network nodes should support the execution of the processes. This paper focuses on this problem by introducing a distributed computational model to dynamically share these tasks among the computing nodes and considering the inherent variability of the context in these environments. Our novel approach promotes the integration of the computing resources, with externally supplied cloud services, to fulfill modern application requirements. A prototype implementation for the proposed model has been built and an application example has been designed to validate the proposal in a real working environment. Â© 2017 Elsevier B.V.","Cyber-physical systems; Distributed computation; Internet of things; Mobile computing; Modeling",,"Elsevier B.V.",,,,,"Article","Scopus"
"Can Developers' Interaction Data Improve Change Recommendation?","Yamamori A., Hagward A.M., Kobayashi T.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029840337&doi=10.1109%2fCOMPSAC.2017.79&partnerID=40&md5=d71b48ed13627c82fe4a85574a521862",2017,"Proceedings - International Computer Software and Applications Conference","One of the most common causes of bugs is overlooking changes. To prevent bugs and improve the quality of the products, numerous studies have been undertaken on change guides based on logical couplings extracted from developers' past process histories, such as change history. While valuable change rules based on logical couplings can be gleaned found from the change history, these rules often fail to find appropriate candidates because the change histories in repositories only preserve a summary of changes between commits. We recently analyzed the interaction data produced by a developer in an integrated development environment. Such interaction data contains not only a detailed change history but also reference activities between commits. In this paper, we investigate whether logical couplings extracted from interaction data could improve change recommendation performance. We used the interaction data from actual open source development, not from the project only for this study. Experimental results obtained using the interaction data from actual open source development showed a significant improvement in the efficiency of the change recommendation process. The results also indicated improvement in the number of detected artifacts that the developer had forgot to change. Â© 2017 IEEE.","Change Guide; Change Impact Analysis; Interaction Data; Mining Software Repository; Software Maintenance","Associazione Italiana per l'Informatica (AICA);IEEE;IEEE Big Data Initiative;IEEE Computer Society","IEEE Computer Society","41st IEEE Annual Computer Software and Applications Conference, COMPSAC 2017","4 July 2017 through 8 July 2017",,130854,"Conference Paper","Scopus"
"Metadata-based code example embedding","Tamla P., Feja S., Prause C.R.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052872504&doi=10.1145%2f3121257.3121258&partnerID=40&md5=733af6942d71d63456ed8fd187ae2386",2017,"SWAN 2017 - Proceedings of the 3rd ACM SIGSOFT International Workshop on Software Analytics, Co-located with FSE 2017","In practice, developers usually seek different ways to save time and effort. Thus, they use different tools (such as search engines, issue tracking, or Q&A sites) to collaborate and find code examples that meet their specific needs. However, such tools only support the traditional find-alter-embed approach of code examples while ignoring the origin and location of these sources. Such information can be very useful to assist software development tasks such as bug-fixing, teamwork, and knowledge transfer, through direct notification of critical changes made to the code example, or access to the original source including its discussions, issues, and bug reports. In this paper, we propose a new approach that consists of collecting meta information about a code example to automatically track critical changes to it and its origin and provide feedback to both developers and the online community. We report on our vision, approach and challenges, and draft a software architecture to implement our research idea. Â© 2017 Association for Computing Machinery.","Code Example Embedding; Collaborative Software Development; Development Environment; Mining Software Repositories; Software process","ACM SIGSOFT","Association for Computing Machinery, Inc","3rd ACM SIGSOFT International Workshop on Software Analytics, SWAN 2017, Co-located with 11th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2017","4 September 2017",,138677,"Conference Paper","Scopus"
"Reverse engineering reusable software components from object-oriented APIs","Shatnawi A., Seriai A.-D., Sahraoui H., Alshara Z.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002957064&doi=10.1016%2fj.jss.2016.06.101&partnerID=40&md5=c1d6513afff820e171aa3013bb284ad6",2017,"Journal of Systems and Software","Object-oriented Application Programing Interfaces (APIs) support software reuse by providing pre-implemented functionalities. Due to the huge number of included classes, reusing and understanding large APIs is a complex task. Otherwise, software components are accepted to be more reusable and understandable entities than object-oriented ones. Thus, in this paper, we propose an approach for reengineering object-oriented APIs into component-based ones. We mine components as a group of classes based on the frequency they are used together and their ability to form a quality-centric component. To validate our approach, we experimented on 100 Java applications that used four APIs. Â© 2016 Elsevier Inc.","API; Frequent usage pattern; Object-oriented; Reverse engineering; Software component; Software reuse",,"Elsevier Inc.",,,,,"Article","Scopus"
"2-step word alignment framework for Thai-English statistical machine translation","Luekhong P., Ruangrajitpakorn T., Sukhahuta R., Supnithi T.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040814274&partnerID=40&md5=17eda0f599930602be6fa072ad18fb87",2017,"Information (Japan)","This paper presents a framework of a new word alignment process for SMT and the translation table improvement method with bilingual dictionary that was the lexical probabilistic tuning methodology for translation table in SMT. First, the alignment method was designed to include the quality of using dictionary as prior knowledge and the ability of co-occurrence to fill unknown words. By testing the proposed framework against the renowned GIZA, we applied an alignment model from both systems to Moses for proving its usefulness in a practical hierarchical phrase-based translation usage and exploited a BLEU score as a measurement. The case study in this work focused on Thai to English translation. The testing results showed the proposed method can overrun the result of GIZA IBM model-4 by 2.09 BLEU points. Â© 2017 International Information Institute.","Bilingual dictionary; Hybrid alignment; Machine translation; Thai english hierarchical phrase based translation; Word alignment",,"International Information Institute Ltd.",,,,,"Article","Scopus"
"Smart Base Station-Assisted Partial-Flow Device-to-Device Offloading System for Video Streaming Services","Park G.S., Kim W., Jeong S.H., Song H.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029364279&doi=10.1109%2fTMC.2016.2626434&partnerID=40&md5=0836bcaedc84507c6c2a09064a15e5e7",2017,"IEEE Transactions on Mobile Computing","This paper presents a smart base station-assisted partial-flow device-to-device offloading system that provides seamless video streaming services to clients by effectively offloading parts of the video traffic to D2D networks in order to alleviate the cellular network traffic load. In the proposed system, the main functionalities of the content centric networking (CCN) technology are deployed for efficient and fast cached content management at the base stations and in the devices. First, a logical overlay Chord-like network is constructed among the smart base stations for effectively handling significant numbers of the distributed contents. Then, content management protocols over the Chord-like network are implemented for rapidly finding the D2D server that contains the cached content requested by the D2D client. A two-stage PID-based LTE traffic controller is proposed to determine the amount of traffic to be offloaded to the D2D network among the cellular operator, the D2D servers, and the D2D clients. The proposed system is fully implemented using a CCNx open source and C/C++. Experimental results are provided to demonstrate the performance improvement of the proposed system. Â© 2017 IEEE.","chord-like network; Content centric networking; partial-flow device-to-device offloading; two-stage PID-based LTE traffic controller",,"Institute of Electrical and Electronics Engineers Inc.",,,,,"Article","Scopus"
"Is Tom Cruise threatened? An empirical study of the impact of product variety on demand concentration","Tan T.F., Netessine S., Hitt L.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031681246&doi=10.1287%2fisre.2017.0712&partnerID=40&md5=810c248d5e5a176a73ef319049af3a3f",2017,"Information Systems Research","We empirically examine the impact of expanded product variety on demand concentration using large data sets from the movie rental industry as our test bed.We find that product variety is likely to increase demand concentration, which goes against the ""long tail effect"" theory predicting that demand will become less concentrated on ""hit"" products because of expanded product variety. We further provide evidence that this finding is not due to introducing many low-selling niche products, as the intuition might suggest. Instead, we discover that increasing product variety diversifies the demand away from each movie title, but less significantly for hits than for niche products. In particular, we find that increasing product variety by 1,000 titles may increase the Gini coefficient of DVD rentals by 0.0029, which translates to increasing the market share of the top 1% of DVDs by 1.96% and the market share of the top 10% of DVDs by 0.58%. At the same time, the market share of the bottom 1% of DVDs is reduced by 21.29%, while the market share of the bottom 10% ofDVDs is reduced by 5.28%.We rule out alternative explanations using a variety of long tail metrics, capturing movie format/distribution channel interaction and customer heterogeneity, while making use of instrumental variables. Â© 2017 INFORMS.","Demand concentration; Movie rental; Product rating; Product variety; The long tail effect",,"INFORMS Inst.for Operations Res.and the Management Sciences",,,,,"Article","Scopus"
"Logistic informatics modelling using concept of stratification (CST)","Asadabadi M.R., Saberi M., Chang E.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030157702&doi=10.1109%2fFUZZ-IEEE.2017.8015510&partnerID=40&md5=11bc3ad4ccf1e3dc8537f1150d305527",2017,"IEEE International Conference on Fuzzy Systems","The concept of stratification (CST) is a reform in problem solving approaches in computer science introduced and developed by Lotfi Zadeh [1]. In this approach, the target set is a set of the initial states. The environment around the target is then systematically identified and strata around the target are gradually built using the concept of 'incremental target enlargement'. We advocate that this approach is useful in different areas; however, it is essential to highlight the potential applicability of the approach by presenting different examples. In this paper illustrative examples in Information Dominance (ID) and in requirement elicitation in contracting, are structured using CST. These examples show the application of the concept in the contexts of logistic informatics and contracting and assists the modelling process. Of particular relevance are Example 4 and 5. Example 4 exposes situations that are not dealt with in the current form of CST. The study, therefore, extends CST by considering the possibility of occasional state repetition while using the same input. Further, a 3D version of CST is structured and examined through example 5. This proposed version of the concept illustrates how the Fuzzy Inference System (FIS) benefits the user when applying CST. Â© 2017 IEEE.","3D CST; Concept of Stratification (CST); CST extensions; Fuzzy CST",,"Institute of Electrical and Electronics Engineers Inc.","2017 IEEE International Conference on Fuzzy Systems, FUZZ 2017","9 July 2017 through 12 July 2017",,130106,"Conference Paper","Scopus"
"How do developers toggle breakpoints? observational studies","Petrillo F., Mandian H., Yamashita A., Khomh F., Gueheneuc Y.-G.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029415238&doi=10.1109%2fQRS.2017.39&partnerID=40&md5=b7a93540e5d3b1522bd04c53ba73fa2a",2017,"Proceedings - 2017 IEEE International Conference on Software Quality, Reliability and Security, QRS 2017","One of the most important tasks in software maintenance is debugging. Developers perform debugging to fix faults and implement new features. Usually they use interactive development environments to perform their debugging sessions. To start an interactive debugging session, developers must set breakpoints. Choosing where to set breakpoints is a non-trivial task, yet few studies have investigated how developers set breakpoints during interactive debugging sessions. To understand how developers set breakpoints, we analysed more than 10 hours of 45 video-recorded debugging sessions, where a total of 307 breakpoints were set. We used the videos from two independent studies involving three software systems. We could observe that: (1) considerable time is spent by developers until they are able to set the first breakpoint; (2) when developers toggle breakpoints carefully, they complete tasks faster than developers who set (potential useless) breakpoints quickly; and (3) different developers set breakpoints in similar locations while working (independently) on the same tasks or different tasks. We discuss some implications of our observations for debugging activities. Â© 2017 IEEE.","Breakpoints; Debugging; Empirical software engineering; Interactive debugging",,"Institute of Electrical and Electronics Engineers Inc.","17th IEEE International Conference on Software Quality, Reliability and Security, QRS 2017","25 July 2017 through 29 July 2017",,129990,"Conference Paper","Scopus"
"IDE-based learning analytics for computing education: A process model, critical review, and research agenda","Hundhausen C.D., Olivares D.M., Carter A.S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028590656&doi=10.1145%2f3105759&partnerID=40&md5=9a6b0f49ff8b451657f88e6480c95229",2017,"ACM Transactions on Computing Education","In recent years, learning process data have become increasingly easy to collect through computer-based learning environments. This has led to increased interest in the field of learning analytics, which is concerned with leveraging learning process data in order to better understand, and ultimately to improve, teaching and learning. In computing education, the logical place to collect learning process data is through integrated development environments (IDEs), where computing students typically spend large amounts of time working on programming assignments. While the primary purpose of IDEs is to support computer programming, they might also be used as a mechanism for delivering learning interventions designed to enhance student learning. The possibility of using IDEs both to collect learning process data, and to strategically intervene in the learning process, suggests an exciting design space for computing education research: that of IDE-based learning analytics. In order to facilitate the systematic exploration of this design space, we present an IDE-based data analytics process model with four primary activities: (1) Collect data, (2) Analyze data, (3) Design intervention, and (4) Deliver intervention. For each activity, we identify key design dimensions and review relevant computing education literature. To provide guidance on designing effective interventions, we describe four relevant learning theories, and consider their implications for design. Based on our review, we present a call-to-action for future research into IDE-based learning analytics. Â© 2017 ACM.","Learning analytics; Learning interventions; Learning process data",,"Association for Computing Machinery",,,,,"Article","Scopus"
"Software integration in global software development: Challenges for GSD vendors","Ilyas M., Khan S.U.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018645451&doi=10.1002%2fsmr.1875&partnerID=40&md5=a93df461a87aa7b68020f60c06cfaa0d",2017,"Journal of Software: Evolution and Process","Context: The advances in information and communication technologies have revolutionized the software development environment from local to global software development (GSD). This revolution also created challenges for vendor organizations. Vendors face challenges in integrating the components developed independently by GSD teams into a final working product. Objective: The objectives of the current study is to find out those critical barriers/challenges that hinder the integration process at any stage in the GSD environment for different types and size of projects. Methodology: For achieving the objectives we initially conducted a comprehensive systematic literature review (SLR). We searched 6 digital libraries and also followed the snowballing technique. The data was extracted from a total of 88 finally selected papers. Findings of the SLR study were then empirically validated through a questionnaire survey in GSD industry. A total of 96 experts from 22 different countries participated in the survey. Results: We have found a total of 16 barriers/challenges among which ten barriers are ranked as critical barriers/challenges. Some of the top ranked barriers are â€œlack of communication,â€ â€œlack of proper documentation,â€ â€œlack of compatibility,â€ and â€œarchitecture mismatch.â€ The findings of our industrial survey are mostly coherent with the SLR findings. However, there is a difference in ranks of the various barriers/challenges across the 2 data sets (SLR and industrial survey). The identified challenges need to be properly addressed by software vendors to reduce the complexity of the integration process in GSD projects. Conclusion: We found that the severity of these barriers increases in large size projects. On the other hand, bespoke products are more affected by â€œlack of communication,â€ while off-the-shelfâ€“based projects face integration problems due to â€œlack of compatibility,â€ â€œarchitecture mismatch,â€ and â€œwrong off the shelf product selection and customizationâ€. Copyright Â© 2017 John Wiley & Sons, Ltd.","barriers/challenges; empirical study; global software development; software integration; systematic literature review",,"John Wiley and Sons Ltd",,,,,"Article","Scopus"
"On code reuse from StackOverflow: An exploratory study on Android apps","Abdalkareem R., Shihab E., Rilling J.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018752207&doi=10.1016%2fj.infsof.2017.04.005&partnerID=40&md5=8d8c2e6e0a5eaa518a14977f300ecd8f",2017,"Information and Software Technology","Context: Source code reuse has been widely accepted as a fundamental activity in software development. Recent studies showed that StackOverflow has emerged as one of the most popular resources for code reuse. Therefore, a plethora of work proposed ways to optimally ask questions, search for answers and find relevant code on StackOverflow. However, little work studies the impact of code reuse from StackOverflow. Objective: To better understand the impact of code reuse from StackOverflow, we perform an exploratory study focusing on code reuse from StackOverflow in the context of mobile apps. Specifically, we investigate how much, why, when, and who reuses code. Moreover, to understand the potential implications of code reuse, we examine the percentage of bugs in files that reuse StackOverflow code. Method: We perform our study on 22 open source Android apps. For each project, we mine their source code and use clone detection techniques to identify code that is reused from StackOverflow. We then apply different quantitative and qualitative methods to answer our research questions. Results: Our findings indicate that 1) the amount of reused StackOverflow code varies for different mobile apps, 2) feature additions and enhancements in apps are the main reasons for code reuse from StackOverflow, 3) mid-age and older apps reuse StackOverflow code mostly later on in their project lifetime and 4) that in smaller teams/apps, more experienced developers reuse code, whereas in larger teams/apps, the less experienced developers reuse code the most. Additionally, we found that the percentage of bugs is higher in files after reusing code from StackOverflow. Conclusion: Our results provide insights on the potential impact of code reuse from StackOverflow on mobile apps. Furthermore, these results can benefit the research community in developing new techniques and tools to facilitate and improve code reuse from StackOverflow. Â© 2017","Code reuse; Mobile app; StackOverflow",,"Elsevier B.V.",,,,,"Article","Scopus"
"Single Image Rain Streak Decomposition Using Layer Priors","Li Y., Tan R.T., Guo X., Lu J., Brown M.S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028405092&doi=10.1109%2fTIP.2017.2708841&partnerID=40&md5=8e6162e86a0849197c091afa7d48c748",2017,"IEEE Transactions on Image Processing","Rain streaks impair visibility of an image and introduce undesirable interference that can severely affect the performance of computer vision and image analysis systems. Rain streak removal algorithms try to recover a rain streak free background scene. In this paper, we address the problem of rain streak removal from a single image by formulating it as a layer decomposition problem, with a rain streak layer superimposed on a background layer containing the true scene content. Existing decomposition methods that address this problem employ either sparse dictionary learning methods or impose a low rank structure on the appearance of the rain streaks. While these methods can improve the overall visibility, their performance can often be unsatisfactory, for they tend to either over-smooth the background images or generate-images that still contain noticeable rain streaks. To address the problems, we propose a method that imposes priors for both the background and rain streak layers. These priors are based on Gaussian mixture models learned on small patches that can accommodate a variety of background appearances as well as the appearance of the rain streaks. Moreover, we introduce a structure residue recovery step to further separate the background residues and improve the decomposition quality. Quantitative evaluation shows our method outperforms existing methods by a large margin. We overview our method and demonstrate its effectiveness over prior work on a number of examples. Â© 2017 IEEE.","Gaussian mixture models; image decomposition; image prior; Rain removal; visibility recovery",,"Institute of Electrical and Electronics Engineers Inc.",,,,,"Article","Scopus"
"Learning syntactic program transformations from examples","Rolim R., Soares G., D'Antoni L., Polozov O., Gulwani S., Gheyi R., Suzuki R., Hartmann B.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019549570&doi=10.1109%2fICSE.2017.44&partnerID=40&md5=b3a45063807a0c115be86bd635b81949",2017,"Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017","Automatic program transformation tools can be valuable for programmers to help them with refactoring tasks, and for Computer Science students in the form of tutoring systems that suggest repairs to programming assignments. However, manually creating catalogs of transformations is complex and time-consuming. In this paper, we present REFAZER, a technique for automatically learning program transformations. REFAZER builds on the observation that code edits performed by developers can be used as input-output examples for learning program transformations. Example edits may share the same structure but involve different variables and subexpressions, which must be generalized in a transformation at the right level of abstraction. To learn transformations, REFAZER leverages state-of-The-Art programming-by-example methodology using the following key components: (a) a novel domain-specific language (DSL) for describing program transformations, (b) domain-specific deductive algorithms for efficiently synthesizing transformations in the DSL, and (c) functions for ranking the synthesized transformations. We instantiate and evaluate REFAZER in two domains. First, given examples of code edits used by students to fix incorrect programming assignment submissions, we learn program transformations that can fix other students' submissions with similar faults. In our evaluation conducted on 4 programming tasks performed by 720 students, our technique helped to fix incorrect submissions for 87% of the students. In the second domain, we use repetitive code edits applied by developers to the same project to synthesize a program transformation that applies these edits to other locations in the code. In our evaluation conducted on 56 scenarios of repetitive edits taken from three large C# open-source projects, REFAZER learns the intended program transformation in 84% of the cases using only 2.9 examples on average. Â© 2017 IEEE.","Program Synthesis; Program transformation; Refactoring; Tutoring Systems","Association for Computing Machinery;et al.;Exactas UBA 150;IEEE Computer Society;IEEE Technical Council on Software Engineering (TCSE);Special Interest Group on Software Engineering (SIGSOFT)","Institute of Electrical and Electronics Engineers Inc.","39th IEEE/ACM International Conference on Software Engineering, ICSE 2017","20 May 2017 through 28 May 2017",,129335,"Conference Paper","Scopus"
"Code smells detection 2.0: Crowdsmelling and visualization","Dos Reis J.P., Abreu E.F.B., Carneiro D.F.G.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027080493&doi=10.23919%2fCISTI.2017.7975961&partnerID=40&md5=4d0c3caed7d7321b13b92450c9c4d3b2",2017,"Iberian Conference on Information Systems and Technologies, CISTI","Background: Code smells have long been catalogued with corresponding mitigating solutions called refactoring operations. However, while the latter are supported in several IDEs, code smells detection scaffolding still has many limitations. Another aspect deserving attention is code smells visualization, to increase software quality awareness, namely in large projects, where maintainability is often the dominating issue. Research problems: Researchers have pointed out that code smells detection is inherently a subjective process and that is probably the main hindrance on providing automatic support. Regarding visualization, customized views are required, because each code smell type may have a different scope. Choosing the right visualization for each code smell type is an open research topic. Expected contributions: This research work focuses on the code smells detection and awareness process, by proposing two symbiotic contributions: crowdsmelling and smelly maps. We envisage that such features will be available in a future generation of interactive development environments (aka IDE 2.0). Crowdsmelling uses the concept of collective intelligence through which programmers around the world will collaboratively contribute to the calibration of code smells detection algorithms (one per each code smell), hopefully improving the detection accuracy and mitigating the subjectivity problem. Smelly maps build upon the aforementioned code smells detection capability and on the previous experience at UNIFACS of setting up a software visualization infrastructure. We expect to represent detected code smells at different abstraction levels with the goal of increasing software quality awareness and facilitating refactoring decisions upon large software systems. Â© 2017 AISTI.","Code Smell; Code Smells Detection; Crowdsmelling; Crowdsourcing; IDE 2.0; Refactoring; Smelly Maps; Software Construction; Software Maintenance; Software Quality","Asociacion de Tecnicos de Informatica (ATI);Empreend - Associacao Portuguesa para o Empreendedorismo;et al.;FCA;IEEE Portugal Section;IEEE SMC","IEEE Computer Society","12th Iberian Conference on Information Systems and Technologies, CISTI 2017","21 June 2017 through 24 June 2017",,129105,"Conference Paper","Scopus"
"Professionals Are Not Superman: Failures beyond Motivation in Software Experiments","Dieste O., Fonseca E.R.C., Raura G., RodrÃ­guez P.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027134711&doi=10.1109%2fCESI.2017.8&partnerID=40&md5=523e56832716f7c4eeb194ce49e9c7dd",2017,"Proceedings - 2017 IEEE/ACM 5th International Workshop on Conducting Empirical Studies in Industry, CESI 2017","Background: Industry experiments are typically associated with higher external validity compared to academic experiments. However, when conducting industry experiments, dropouts and incomplete experimental tasks are quite common, which is unusual in academic experiments. To the best of our knowledge, this phenomenon has not been reported in the literature. Aim: Identify the circumstances that explain why some experimental subjects exhibit poor or null participation during experimental sessions. Method: An industry experiment with experienced programmers at the Universidad de las Fuerzas Armadas ESPE of Ecuador was performed. Several post hoc analyses of the experimental data revealed relationships that could explain the subjects' behavior. Results: A high percentage of older experienced programmers did not perform meaningful work in their task assignments, even though they were present during the entire experiment. Longer overall (i.e., not only programing) experience and poor knowledge of the programming language and integrated development environment have a negative influence in the degree of task completion as well. Conclusions: Several experienced professionals were found to live a two, mixed-factors reality: old age and technological lapse. This negatively influenced (to a greater or lesser extent, depending on the person) attitudes regarding performance of activities that differ from daily professional work. Â© 2017 IEEE.","Experiments; Old age; Performance; Professionals; Students; Technological lapse",,"Institute of Electrical and Electronics Engineers Inc.","5th IEEE/ACM International Workshop on Conducting Empirical Studies in Industry, CESI 2017","23 May 2017",,129061,"Conference Paper","Scopus"
"Recent advances in image dehazing","Wang W., Yuan X.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029183477&doi=10.1109%2fJAS.2017.7510532&partnerID=40&md5=90a131c7df665a4b5d52c871926bf32b",2017,"IEEE/CAA Journal of Automatica Sinica","Images captured in hazy or foggy weather conditions can be seriously degraded by scattering of atmospheric particles, which reduces the contrast, changes the color, and makes the object features difficult to identify by human vision and by some outdoor computer vision systems. Therefore image dehazing is an important issue and has been widely researched in the field of computer vision. The role of image dehazing is to remove the influence of weather factors in order to improve the visual effects of the image and provide benefit to post-processing. This paper reviews the main techniques of image dehazing that have been developed over the past decade. Firstly, we innovatively divide a number of approaches into three categories: image enhancement based methods, image fusion based methods and image restoration based methods. All methods are analyzed and corresponding sub-categories are introduced according to principles and characteristics. Various quality evaluation methods are then described, sorted and discussed in detail. Finally, research progress is summarized and future research directions are suggested. Â© 2014 Chinese Association of Automation.","Atmospheric scattering model; image dehazing; image enhancement; quality assessment",,"Institute of Electrical and Electronics Engineers Inc.",,,,,"Article","Scopus"
"Using eye gaze data to recognize task-relevant source code better and more fine-grained","Kevic K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026777338&doi=10.1109%2fICSE-C.2017.152&partnerID=40&md5=45edcda818042326ecbaa97753583c75",2017,"Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering Companion, ICSE-C 2017","Models to assess a source code element's relevancy for a given change task are the basis of many software engineering tools, such as recommender systems, for code comprehension. To improve such relevancy models and to aid developers in finding relevant parts in the source code faster, we studied developer's fine-grained navigation patterns with eye tracking technology. By combining the captured eye gaze data with interaction data of 12 developers working on a change task, we were able to identify relevant methods with high accuracy and improve precision and recall compared to the widely used click frequency technique by 77% and 24% respectively. Furthermore, we were able to show that the captured gaze data enables to retrace which source code lines developers found relevant. Our results thus provide evidence that eye gaze data can be used to improve existing models in terms of accuracy and granularity. Â© 2017 IEEE.","Eye-gaze; Recommender system; Relevancy model",,"Institute of Electrical and Electronics Engineers Inc.","39th IEEE/ACM International Conference on Software Engineering Companion, ICSE-C 2017","20 May 2017 through 28 May 2017",,128913,"Conference Paper","Scopus"
"Taxonomy based features in question classification using support vector machine","Sangodiah A., Ahmad R., Ahmad W.F.W.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021700847&partnerID=40&md5=8ddbf6efb7b0f9a8cb4a6f7839848194",2017,"Journal of Theoretical and Applied Information Technology","One of the areas in text mining which is text classification has attracted much attention in various industries and fields lately. This is because the text classification has the ability in labelling text documents to one or more pre-defined categories based on content similarity. As text classification emphasizes on document level, question classification works at finer level such as sentence and phrase. Several studies on question classification in respect to Bloom taxonomy to measure cognitive level of learners in higher learning institutions have been carried out in the past. But, existing feature types in the past work may work reasonably well on data sets consisting of questions that are too specific to one particular field or area which will result in having multiple classifiers to be built for questions involving various fields or areas. Certainly, feature types play an important role in improving the accuracy of classifier. Past related work emphasizes on feature types such as bag of word (BOW) and syntactic analysis in question classification. In this study, a new feature type named taxonomy based is proposed to improve the accuracy of question classification for data sets having questions from various fields. The performance of question classification using the new feature type between data sets consisting of questions from specific and various areas will be compared. Support Vector Machine classifier will be used as it is known for high accuracy in text classification. The outcome of this study shows that the taxonomy based features has the ability in improve the accuracy of classifier involving data sets of questions from various fields. Â© 2005 â€“ ongoing JATIT & LLS.","Bag-of-words; Bloom taxonomy; Feature type; Question classification; Support vector machine",,"Asian Research Publishing Network",,,,,"Article","Scopus"
"Some from Here, Some from There: Cross-Project Code Reuse in GitHub","Gharehyazie M., Ray B., Filkov V.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026520340&doi=10.1109%2fMSR.2017.15&partnerID=40&md5=889f5e4fec932c0467a48a34aca0713e",2017,"IEEE International Working Conference on Mining Software Repositories","Code reuse has well-known benefits on code quality, coding efficiency, and maintenance. Open Source Software (OSS) programmers gladly share their own code and they happily reuse others'. Social programming platforms like GitHub have normalized code foraging via their common platforms, enabling code search and reuse across different projects. Removing project borders may facilitate more efficient code foraging, and consequently faster programming. But looking for code across projects takes longer and, once found, may be more challenging to tailor to one's needs. Learning how much code reuse goes on across projects, and identifying emerging patterns in past cross-project search behavior may help future foraging efforts. To understand cross-project code reuse, here we present an in-depth study of cloning in GitHub. Using Deckard, a clone finding tool, we identified copies of code fragments across projects, and investigate their prevalence and characteristics using statistical and network science approaches, and with multiple case studies. By triangulating findings from different methods, we find that cross-project cloning is prevalent in GitHub, ranging from cloning few lines of code to whole project repositories. Some of the projects serve as popular sources of clones, and others seem to contain more clones than their fair share. Moreover, we find that ecosystem cloning follows an onion model: most clones come from the same project, then from projects in the same application domain, and finally from projects in different domains. Our results show directions for new tools that can facilitate code foraging and sharing within GitHub. Â© 2017 IEEE.","Code reuse; Cross-project clones; GitHub",,"IEEE Computer Society","14th IEEE/ACM International Conference on Mining Software Repositories, MSR 2017","20 May 2017 through 21 May 2017",,128853,"Conference Paper","Scopus"
"Using contexts similarity to predict relationships between tasks","Maalej W., Ellmann M., Robbes R.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018458943&doi=10.1016%2fj.jss.2016.11.033&partnerID=40&md5=6ecdc4ef105a18c75b64447849776fa7",2017,"Journal of Systems and Software","Developersâ€™ tasks are often interrelated. A task might succeed, precede, block, or depend on another task. Or, two tasks might simply have a similar aim or require similar expertise. When working on tasks, developers interact with artifacts and tools, which constitute the contexts of the tasks. This work investigates the extent to which the similarity of the contexts predicts whether and how the respective tasks are related. The underlying assumption is simple: if during two tasks the same artifacts are touched or similar interactions are observed, the tasks might be interrelated. We define a task context as the set of all developer's interactions with the artifacts during the task. We then apply Jaccard index, a popular similarity measure to compare two contexts. Instead of only counting the artifacts in the intersection and union of the contexts as Jaccard does, we scale the artifacts with their relevance to the task. For this, we suggest a simple heuristic based on the Frequency, Duration, and Age of the interactions with the artifacts (FDA). Alternatively, artifact relevance can be estimated by the Degree-of-Interest (DOI) used in task-focused programming. To compare the accuracy of the context similarity models for predicting task relationships, we conducted a field study with professionals, analyzed data from the open source task repository Bugzilla, and ran an experiment with students. We studied two types of relationships useful for work coordination (dependsOn and blocks) and two types useful for personal work management (isNextTo and isSimilarTo). We found that context similarity models clearly outperform a random prediction for all studied task relationships. We also found evidence that, the more interrelated the tasks are, the more accurate the context similarity predictions are. Our results show that context similarity is roughly as accurate to predict task relationships as comparing the textual content of the task descriptions. Context and content similarity models might thus be complementary in practice, depending on the availability of text descriptions or context data. We discuss several use cases for this research, e.g. to assist developers choose the next task or to recommend other tasks they should be aware of. Â© 2017 The Authors","Developer productivity; Empirical study; Recommender systems; Task context; Task dependencies; Task management",,"Elsevier Inc.",,,,,"Article","Scopus"
"On the use of developersâ€™ context for automatic refactoring of software anti-patterns","Morales R., Soh Z., Khomh F., Antoniol G., Chicano F.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028248184&doi=10.1016%2fj.jss.2016.05.042&partnerID=40&md5=c7768526a3d9ed2be2f9d5eae855e209",2017,"Journal of Systems and Software","Anti-patterns are poor solutions to design problems that make software systems hard to understand and extend. Entities involved in anti-patterns are reported to be consistently related to high change and fault rates. Refactorings, which are behavior preserving changes are often performed to remove anti-patterns from software systems. Developers are advised to interleave refactoring activities with their regular coding tasks to remove anti-patterns, and consequently improve software design quality. However, because the number of anti-patterns in a software system can be very large, and their interactions can require a solution in a set of conflicting objectives, the process of manual refactoring can be overwhelming. To automate this process, previous works have modeled anti-patterns refactoring as a batch process where a program provides a solution for the total number of classes in a system, and the developer has to examine a long list of refactorings, which is not feasible in most situations. Moreover, these proposed solutions often require that developers modify classes on which they never worked before (i.e., classes on which they have little or no knowledge). To improve on these limitations, this paper proposes an automated refactoring approach, ReCon (Refactoring approach based on task Context), that leverages information about a developer's task (i.e., the list of code entities relevant to the developer's task) and metaheuristics techniques to compute the best sequence of refactorings that affects only entities in the developer's context. We mine 1705 task contexts (collected using the Eclipse plug-in Mylyn) and 1013 code snapshots from three open-source software projects (Mylyn, PDE, Eclipse Platform) to assess the performance of our proposed approach. Results show that ReCon can remove more than 50% of anti-patterns in a software system, using fewer resources than the traditional approaches from the literature. Â© 2016 Elsevier Inc.","Anti-patterns; Automatic refactoring; Interaction traces; Metaheuristics; Software maintenance; Task context",,"Elsevier Inc.",,,,,"Article","Scopus"
"Continuous Analysis of Collaborative Design","Bang J.Y., Brun Y., Medvidovic N.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021393633&doi=10.1109%2fICSA.2017.45&partnerID=40&md5=a06b1b68c9d04cdd0e30d7576afb1c8e",2017,"Proceedings - 2017 IEEE International Conference on Software Architecture, ICSA 2017","In collaborative design, architects' individual design decisions may conflict and, when joined, may violate system consistency rules or non-functional requirements. These design conflicts can hinder collaboration and result in wasted effort. Proactive detection of code-level conflicts has been shown to improve collaborative productivity, however, the computational resource requirements for proactively computing design conflicts have hindered its applicability in practice. Our survey and interviews of 50 architects from six large software companies find that 60% of their projects involve collaborative design, that architects consider integration costly, and that design conflicts are frequent and lead to lost work. To aid collaborative design, we re-engineer FLAME, our prior design conflict detection technique, to use cloud resources and a novel prioritization algorithm that, together, achieve efficient and nonintrusive conflict detection, and guarantee a bound on the time before a conflict is discovered. Two controlled experiments with 90 students trained in software architecture in a professional graduate program, demonstrate that architects using FLAME design more efficiently, produce higher-quality designs, repair conflicts faster, and prefer using FLAME. An empirical performance evaluation demonstrates FLAME's scalability and verifies its time-bound guarantees. Â© 2017 IEEE.","collaborative design; conflict detection; proactive conflict detection; speculative analysis",,"Institute of Electrical and Electronics Engineers Inc.","2017 IEEE International Conference on Software Architecture, ICSA 2017","3 April 2017 through 7 April 2017",,127837,"Conference Paper","Scopus"
"Continuous code reviews: A social coding tool for code reviews inside the IDE","DÃ¼rschmid T.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028743606&doi=10.1145%2f3079368.3079374&partnerID=40&md5=58b359b27bd0f336d47fe16e36cb857b",2017,"ACM International Conference Proceeding Series","Code reviews play an important and successful role in modern so.- ware development. But usually they happen only once before new code is merged into the main branch. We present a concept which helps developers to continuously give feedback on their source code directly in the integrated development environment (IDE) by using the metaphor of social networks. .is reduces context switches for developers, improves the software development process and allows to give feedback to developers of external libraries and frameworks. Â© 2017 ACM.","Code quality; Code review; Feedback; Social coding","Aspect-Oriented Software Association (AOSA)","Association for Computing Machinery","1st International Conference on the Art, Science and Engineering of Programming, Programming 2017","3 April 2017 through 6 April 2017",,129681,"Conference Paper","Scopus"
"A survey of the use of crowdsourcing in software engineering","Mao K., Capra L., Harman M., Jia Y.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995569607&doi=10.1016%2fj.jss.2016.09.015&partnerID=40&md5=6afcf90b7efddb88a3b8359f254d2a3f",2017,"Journal of Systems and Software","The term â€˜crowdsourcingâ€™ was initially introduced in 2006 to describe an emerging distributed problem-solving model by online workers. Since then it has been widely studied and practiced to support software engineering. In this paper we provide a comprehensive survey of the use of crowdsourcing in software engineering, seeking to cover all literature on this topic. We first review the definitions of crowdsourcing and derive our definition of Crowdsourcing Software Engineering together with its taxonomy. Then we summarise industrial crowdsourcing practice in software engineering and corresponding case studies. We further analyse the software engineering domains, tasks and applications for crowdsourcing and the platforms and stakeholders involved in realising Crowdsourced Software Engineering solutions. We conclude by exposing trends, open issues and opportunities for future research on Crowdsourced Software Engineering. Â© 2016","Crowdsourced software engineering; Crowdsourcing; Literature survey; Software crowdsourcing",,"Elsevier Inc.",,,,,"Article","Scopus"
"Exploiting diversity in type checkers for better error messages","Chen S., Erwig M., Smeltzer K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979713800&doi=10.1016%2fj.jvlc.2016.07.001&partnerID=40&md5=29d8890b66de2a6e78cc341b199e32f8",2017,"Journal of Visual Languages and Computing","Producing precise and helpful type error messages has been a challenge for the implementations of functional programming languages for over 3 decades now. Many different approaches and methods have been tried to solve this thorny problem, but current type-error reporting tools still suffer from a lack of precision in many cases. Based on the observation that different approaches work well in different situations, we have studied the question of whether a combination of tools that exploits their diversity can lead to improved accuracy. Specifically, we have studied Helium, a Haskell implementation particularly aimed at producing good type error messages, and Lazy Typing, an approach developed previously by us to address the premature-error-commitment problem in type checkers. By analyzing the respective strengths and weaknesses of the two approaches we were able to identify a strategy to combine both tools that could markedly improve the accuracy of reported errors. Specifically, we report an evaluation of 1069 unique ill-typed programs out of a total of 11,256 Haskell programs that reveals that this combination strategy enjoys a correctness rate of 82%, which is an improvement of 25%/20% compared to using Lazy Typing/Helium alone. In addition to describing this particular case study, we will also report insights we gained into the combination of error-reporting tools in general. Â© 2016 Elsevier Ltd","Tool combining; Type-error diagnosing",,"Academic Press",,,,,"Article","Scopus"
"City-Wide Traffic Flow Estimation from a Limited Number of Low-Quality Cameras","Ide T., Katsuki T., Morimura T., Morris R.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983077466&doi=10.1109%2fTITS.2016.2597160&partnerID=40&md5=cbf2b06bcc7c2b0387eaa39673ab5606",2017,"IEEE Transactions on Intelligent Transportation Systems","We present a new approach to lightweight intelligent transportation systems. Our approach does not rely on traditional expensive infrastructures, but rather on advanced machine learning algorithms. It takes images from traffic cameras at a limited number of locations and estimates the traffic over the entire road network. Our approach features two main algorithms. The first is a probabilistic vehicle counting algorithm from low-quality images that falls into the category of unsupervised learning. The other is a network inference algorithm based on an inverse Markov chain formulation that infers the traffic at arbitrary links from a limited number of observations. We evaluated our approach on two different traffic data sets, one acquired in Nairobi, Kenya, and the other in Kyoto, Japan. Â© 2000-2011 IEEE.","Gaussian mixtures; image analysis; inverse Markov problem; object counting; variational Bayes",,"Institute of Electrical and Electronics Engineers Inc.",,,,,"Article","Scopus"
"Visual simple transformations: Empowering end-users to wire internet of things objects","Akiki P.A., Bandara A.K., Yu Y.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019623162&doi=10.1145%2f3057857&partnerID=40&md5=080ce61f39fc69f1f363c49dafad80f2",2017,"ACM Transactions on Computer-Human Interaction","Empowering end-users to wire Internet of Things (IoT) objects (things and services) together would allow them to more easily conceive and realize interesting IoT solutions. A challenge lies in devising a simple end-user development approach to support the specification of transformations, which can bridge the mismatch in the data being exchanged among IoT objects. To tackle this challenge, we present Visual Simple Transformations (ViSiT) as an approach that allows end-users to use a jigsaw puzzle metaphor for specifying transformations that are automatically converted into underlying executable workflows. ViSiT is explained by presenting meta-models and an architecture for implementing a system of connected IoT objects. A tool is provided for supporting end-users in visually developing and testing transformations. Another tool is also provided for allowing software developers to modify, if they wish, a transformation's underlying implementation. This work was evaluated from a technical perspective by developing transformations and measuring ViSiT's efficiency and scalability and by constructing an example application to show ViSiT's practicality. A study was conducted to evaluate this work from an end-user perspective, and its results showed positive indications of perceived usability, learnability, and the ability to conceive real-life scenarios for ViSiT. Â© 2017 ACM.","End-user development; Internet of things; Transformations",,"Association for Computing Machinery",,,,,"Article","Scopus"
"Enriching in-IDE process information with fine-grained source code history","Proksch S., Nadi S., Amann S., Mezini M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018449691&doi=10.1109%2fSANER.2017.7884626&partnerID=40&md5=77e4484a202233c82e38e3b6d1c2c16c",2017,"SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering","Current studies on software development either focus on the change history of source code from version-control systems or on an analysis of simplistic in-IDE events without context information. Each of these approaches contains valuable information that is unavailable in the other case. Our work proposes enriched event streams, a solution that combines the best of both worlds and provides a holistic view on the software development process. Enriched event streams not only capture developer activities in the IDE, but also specialized context information, such as source-code snapshots for change events. To enable the storage of such code snapshots in an analyzable format, we introduce a new intermediate representation called Simplified Syntax Trees (SSTs) and build CAâ–¡RET, a platform that offers reusable components to conveniently work with enriched event streams. We implement FEEDBAG++, an instrumentation for Visual Studio that collects enriched event streams with code snapshots in the form of SSTs. We share a dataset of enriched event streams captured from 58 users and representing 915 days of work. Additionally, to demonstrate usefulness, we present three research applications that have already made use of CAâ–¡RET and FEEDBAG++. Â© 2017 IEEE.",,"Alpen-Adria Universitat;IEEE;IEEE Computer Society;Technical Council on Software Engineering (TCSE)","Institute of Electrical and Electronics Engineers Inc.","24th IEEE International Conference on Software Analysis, Evolution, and Reengineering, SANER 2017","21 February 2017 through 24 February 2017",,127060,"Conference Paper","Scopus"
"STRICT: Information retrieval based search term identification for concept location","Rahman M.M., Roy C.K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018397079&doi=10.1109%2fSANER.2017.7884611&partnerID=40&md5=e3f3634604ff28b7e1249cd82db8b31e",2017,"SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering","During maintenance, software developers deal with numerous change requests that are written in an unstructured fashion using natural language. Such natural language texts illustrate the change requirement involving various domain related concepts. Software developers need to find appropriate search terms from those concepts so that they could locate the possible locations in the source code using a search technique. Once such locations are identified, they can implement the requested changes there. Studies suggest that developers often perform poorly in coming up with good search terms for a change task. In this paper, we propose a novel technique-STRICT-that automatically identifies suitable search terms for a software change task by analyzing its task description using two information retrieval (IR) techniques-TextRank and POSRank. These IR techniques determine a term's importance based on not only its co-occurrences with other important terms but also its syntactic relationships with them. Experiments using 1,939 change requests from eight subject systems report that STRICT can identify better quality search terms than baseline terms from 52%-62% of the requests with 30%-57% Top-10 retrieval accuracy which are promising. Comparison with two state-of-the-art techniques not only validates our empirical findings and but also demonstrates the superiority of our technique. Â© 2017 IEEE.","Concept location; information retrieval; POSRank; search term identification; TextRank","Alpen-Adria Universitat;IEEE;IEEE Computer Society;Technical Council on Software Engineering (TCSE)","Institute of Electrical and Electronics Engineers Inc.","24th IEEE International Conference on Software Analysis, Evolution, and Reengineering, SANER 2017","21 February 2017 through 24 February 2017",,127060,"Conference Paper","Scopus"
"Automated generation of consistency-achieving model editors","Neubauer P., Bill R., Mayerhofer T., Wimmer M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018390328&doi=10.1109%2fSANER.2017.7884615&partnerID=40&md5=13bdcc57256146a732d8bca40da4bee3",2017,"SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering","The advances of domain-specific modeling languages (DSMLs) and their editors created with modern language work-benches, have convinced domain experts of applying them as important and powerful means in their daily endeavors. Despite the fact that such editors are proficient in retaining syntactical model correctness, they present major shortages in mastering the preservation of consistency in models with elaborated language-specific constraints which require language engineers to manually implement sophisticated editing capabilities. Consequently, there is a demand for automating procedures to support editor users in both comprehending as well as resolving consistency violations. In this paper, we present an approach to automate the generation of advanced editing support for DSMLs offering automated validation, content-assist, and quick fix capabilities beyond those created by state-of-the-art language workbenches that help domain experts in retaining and achieving the consistency of models. For validation, we show potential error causes for violated constraints, instead of only the context in which constraints are violated. The state-space explosion problem is mitigated by our approach resolving constraint violations by increasing the neighborhood scope in a three-stage process, seeking constraint repair solutions presented as quick fixes to the editor user. We illustrate and provide an initial evaluation of our approach based on an Xtext-based DSML for modeling service clusters. Â© 2017 IEEE.","Advanced Editor Support; Domain Specific Modeling Languages; Model Driven Engineering; Search-based Software Engineering","Alpen-Adria Universitat;IEEE;IEEE Computer Society;Technical Council on Software Engineering (TCSE)","Institute of Electrical and Electronics Engineers Inc.","24th IEEE International Conference on Software Analysis, Evolution, and Reengineering, SANER 2017","21 February 2017 through 24 February 2017",,127060,"Conference Paper","Scopus"
"Change impact analysis for hardware designs from natural language to system level","Ring M., Stoppe J., Luth C., Drechsler R.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017168571&doi=10.1109%2fFDL.2016.7880369&partnerID=40&md5=df094abcbd01359ef2d60ef6a483926d",2017,"Forum on Specification and Design Languages","Design processes are increasingly moving to more abstract description levels; no single formalism can handle the complexities of modern designs. However, keeping designs consistent across different abstraction levels, in particular in the presence of changes, has up to now been an arduous manual task. This paper presents a framework which provides a uniform, interconnected representation of the descriptions across the abstraction levels, starting from natural language requirement specifications over SysML design specifications down to executable SystemC models, allowing to track changes on all levels of abstraction, and ensuring consistency throughout the development process. The framework has been implemented in a tool, CHIMPANC, to show its viability. It assists the developer by highlighting inconsistencies and proof obligations across various descriptions levels in order to simplify the development process. Â© 2016 ECSI.",,,"IEEE Computer Society","2016 Forum on Specification and Design Languages, FDL 2016","14 September 2016 through 16 September 2016",,126894,"Conference Paper","Scopus"
"Work fragmentation in developer interaction data","Cruz L.C., Sanchez H., GonzÃ¡lez V.M., Robbes R.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011664439&doi=10.1002%2fsmr.1839&partnerID=40&md5=f83437b388ee3dcd73c84b5d55fbd7f1",2017,"Journal of Software: Evolution and Process","Information workers and software developers are exposed to work fragmentation, an interleaving of activities and interruptions during their normal work day. Small-scale observational studies have shown that this can be detrimental to their work. In this paper, we perform a large-scale study of this phenomenon for the particular case of software developers performing software evolution tasks. Our study is based on several thousands interaction traces collected by Mylyn and the Eclipse Usage Data Collector. We observe that work fragmentation is correlated to lower observed productivity at both the macro level (for entire sessions) and at the micro level (around markers of work fragmentation); further, longer activity switches seem to strengthen the effect, and different activities seem to be affected differently. These observations give ground for subsequent studies investigating the phenomenon of work fragmentation. Copyright Â© 2017 John Wiley & Sons, Ltd.","interaction data; interruptions; work fragmentation",,"John Wiley and Sons Ltd",,,,,"Conference Paper","Scopus"
"Beyond just text: Semantic emoji similarity modeling to support expressive communication","Pohl H., Domin C., Rohs M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016475608&doi=10.1145%2f3039685&partnerID=40&md5=d01737639623d9841d8a8e8901f2365b",2017,"ACM Transactions on Computer-Human Interaction","Emoji, a set of pictographic Unicode characters, have seen strong uptake over the last couple of years. All common mobile platforms and many desktop systems now support emoji entry, and users have embraced their use. Yet, we currently know very little about what makes for good emoji entry. While soft keyboards for text entry are well optimized, based on language and touch models, no such information exists to guide the design of emoji keyboards. In this article, we investigate of the problem of emoji entry, starting with a study of the current state of the emoji keyboard implementation in Android. To enable moving forward to novel emoji keyboard designs, we then explore a model for emoji similarity that is able to inform such designs. This semantic model is based on data from 21 million collected tweets containing emoji. We compare this model against a solely description-based model of emoji in a crowdsourced study. Our model shows good perfor mance in capturing detailed relationships between emoji. 2017 Copyright is held by the owner/author(s). Publication rights licensed to ACM.","Emoji; Emoticons; Mobile text entry",,"Association for Computing Machinery",,,,,"Article","Scopus"
"Eliph: Effective visualization of code history for peer assessment in programming education","Park J., Park Y.H., Kim S., Oh A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014784750&doi=10.1145%2f2998181.2998285&partnerID=40&md5=c503edba512d73eae8618de438cebd58",2017,"Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW","In this paper, we investigate the effectiveness of visualization of code history on peer assessment in computer science education. Peer assessment is found to be an effective learning tool for programming education. While many systems are proposed to support peer assessment in programming education, little effort has been devoted to finding ways to improve the peer assessment by assisting the students to understand the programs they are assessing. We introduce Eliph, a web-based peer assessment system for programming education with code history visualization. Eliph incorporates the visualization of character-level code history, selection-based history tracking and the integration of execution events to assist students in understanding programs written by peers, thereby leading to more effective peer assessment. We evaluate Eliph with an experiment in an undergraduate CS course. We show that visualization of code history has positive effects on promoting higher quality of peer feedback by understanding the intention and thought process. Â© 2017 ACM.","Code history; Data visualization; Peer assessment; Peer review; Time series visualization","ACM SIGCHI","Association for Computing Machinery","2017 ACM Conference on Computer Supported Cooperative Work and Social Computing, CSCW 2017","25 February 2017 through 1 March 2017",,126334,"Conference Paper","Scopus"
"How Social and Communication Channels Shape and Challenge a Participatory Culture in Software Development","Storey M.-A., Zagalsky A., Filho F.F., Singer L., German D.M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013074755&doi=10.1109%2fTSE.2016.2584053&partnerID=40&md5=b2ea51107719e6ac88f1e79b0414079b",2017,"IEEE Transactions on Software Engineering","Software developers use many different communication tools and channels in their work. The diversity of these tools has dramatically increased over the past decade and developers now have access to a wide range of socially enabled communication channels and social media to support their activities. The availability of such social tools is leading to a participatory culture of software development, where developers want to engage with, learn from, and co-create software with other developers. However, the interplay of these social channels, as well as the opportunities and challenges they may create when used together within this participatory development culture are not yet well understood. In this paper, we report on a large-scale survey conducted with 1,449 GitHub users. We discuss the channels these developers find essential to their work and gain an understanding of the challenges they face using them. Our findings lay the empirical foundation for providing recommendations to developers and tool designers on how to use and improve tools for software developers. Â© 2016 IEEE.","communication; CSCW; Participatory culture; social media; software engineering",,"Institute of Electrical and Electronics Engineers Inc.",,,,,"Article","Scopus"
"Extracting and analyzing time-series HCI data from screen-captured task videos","Bao L., Li J., Xing Z., Wang X., Xia X., Zhou B.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954492015&doi=10.1007%2fs10664-015-9417-1&partnerID=40&md5=a4666ecaf49f22d48e07c8d38fff1271",2017,"Empirical Software Engineering","Recent years have witnessed the increasing emphasis on human aspects in software engineering research and practices. Our survey of existing studies on human aspects in software engineering shows that screen-captured videos have been widely used to record developersâ€™ behavior and study software engineering practices. The screen-captured videos provide direct information about which software tools the developers interact with and which content they access or generate during the task. Such Human-Computer Interaction (HCI) data can help researchers and practitioners understand and improve software engineering practices from human perspective. However, extracting time-series HCI data from screen-captured task videos requires manual transcribing and coding of videos, which is tedious and error-prone. In this paper we report a formative study to understand the challenges in manually transcribing screen-captured videos into time-series HCI data. We then present a computer-vision based video scraping technique to automatically extract time-series HCI data from screen-captured videos. We also present a case study of our scvRipper tool that implements the video scraping technique using 29-hours of task videos of 20 developers in two development tasks. The case study not only evaluates the runtime performance and robustness of the tool, but also performs a detailed quantitative analysis of the toolâ€™s ability to extract time-series HCI data from screen-captured task videos. We also study the developerâ€™s micro-level behavior patterns in software development from the quantitative analysis. Â© 2016, Springer Science+Business Media New York.","HCI data; Online search behavior; Screen-captured video; Video scraping",,"Springer New York LLC",,,,,"Article","Scopus"
"jsCoq: Towards hybrid theorem proving interfaces","Gallego Arias E.J., Pin B., Jouvelot P.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018935172&doi=10.4204%2fEPTCS.239.2&partnerID=40&md5=e04e04bff5a89886dc34d62692dc2902",2017,"Electronic Proceedings in Theoretical Computer Science, EPTCS","We describe jsCoq, a new platform and user environment for the Coq interactive proof assistant. The jsCoq system targets the HTML5-ECMAScript 2015 specification, and it is typically run inside a standards-compliant browser, without the need of external servers or services. Targeting educational use, jsCoq allows the user to start interaction with proof scripts right away, thanks to its self-contained nature. Indeed, a full Coq environment is packed along the proof scripts, easing distribution and installation. Starting to use jsCoq is as easy as clicking on a link. The current release ships more than 10 popular Coq libraries, and supports popular books such as Software Foundations or Certified Programming with Dependent Types. The new target platform has opened up new interaction and display possibilities. It has also fostered the development of some new Coq-related technology. In particular, we have implemented a new serialization-based protocol for interaction with the proof assistant, as well as a new package format for library distribution. Â© 2017, Open Publishing Association. All rights reserved.",,,"Open Publishing Association","12th Workshop on User Interfaces for Theorem Provers, UITP 2016","2 July 2016",,127022,"Conference Paper","Scopus"
"An Empirical Study on the Impact of an IDE Tool Support in the Pair and Solo Programming","Gomez O.S., Aguileta A.A., Aguilar R.A., Ucan J.P., Rosero R.H., Cortes-Verdin K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028916077&doi=10.1109%2fACCESS.2017.2701339&partnerID=40&md5=f3199184b3f39062b6dc932ad6310871",2017,"IEEE Access","The adoption of Agile software development approaches has been widespread. One well-known Agile approach is extreme programming, which encompasses twelve practices of which pair programming is one of them. Although various aspects of pair programming have been studied, we have not found, under a traditional setting of pair programming, studies that examine the impact of using a tool support, such as an integrated development environment (IDE) or a simple text editor. In an attempt to obtain a better understanding of the impact of using an IDE in this field, we present the results of a controlled experiment that expose the influence on quality, measured as the number of defects injected per hour, and cost, measured as the time necessary to complete a programming assignment, of pair and solo programming with and without the use of an IDE. For quality, our findings suggest that the use of an IDE results in significantly higher defect injection rates (for both pairs and solos) when the programming assignment is not very complicated. Nevertheless, defect injection rates seem to decrease when pairs work on more complicated programming assignments irrespective of the tool support that they use. For cost, the programming assignment significantly affects the time needed to complete the assignment. In relation to the programming type, pairs and solos performed in a similar way with regards to quality and cost. Â© 2017 IEEE.","controlled experiment; integrated development environment; Pair programming; software engineering; software quality and cost",,"Institute of Electrical and Electronics Engineers Inc.",,,,,"Article","Scopus"
"A dynamical quality model to continuously monitor software maintenance","Lenarduzzi V., Stan A.C., Taibi D., Tosi D., Venters G.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029853227&partnerID=40&md5=66acee2d50a4878bd911a5e65a9702ca",2017,"Proceedings of the 11th European Conference on Information Systems Management, ECISM 2017","Context: several companies, particularly Small and Medium Sized Enterprises (SMEs), often face software maintenance issues due to the lack of Software Quality Assurance (SQA). SQA is a complex task that requires a lot of effort and expertise, often not available in SMEs. Several SQA models, including maintenance prediction models, have been defined in research papers. However, these models are commonly defined as ""one-size-fits-All"" and are mainly targeted at the big industry, which can afford software quality experts who undertake the data interpretation tasks. Objective: in this work, we propose an approach to continuously monitor the software operated by end users, automatically collecting issues and recommending possible fixes to developers. The continuous exception monitoring system will also serve as knowledge base to suggest a set of quality practices to avoid (re)introducing bugs into the code. Method: first, we identify a set of SQA practices applicable to SMEs, based on the main constraints of these. Then, we identify a set of prediction techniques, including regressions and machine learning, keeping track of bugs and exceptions raised by the released software. Finally, we provide each company with a tailored SQA model, automatically obtained from companies' bug/issue history. Developers are then provided with the quality models through a set of plug-ins for integrated development environments. These suggest a set of SQA actions that should be undertaken, in order to maintain a certain quality level and allowing to remove the most severe issues with the lowest possible effort. Conclusion: The collected measures will be made available as public dataset, so that researchers can also benefit of the project's results. This work is developed in collaboration with local SMEs and existing Open Source projects and communities.","Dynamic Software Measurement; Software Maintenance; Software Quality",,"Academic Conferences and Publishing International Limited","11th European Conference on Information Systems Management, ECISM 2017","14 September 2017 through 15 September 2017",,131830,"Conference Paper","Scopus"
"Arousal level classification of the aging adult from electro-dermal activity: From hardware development to software architecture","MartÃ­nez-Rodrigo A., ZangrÃ³niz R., Pastor J.M., Sokolova M.V.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964691204&doi=10.1016%2fj.pmcj.2016.04.006&partnerID=40&md5=312ec746d64d8a6fd5fd2430f30dc295",2017,"Pervasive and Mobile Computing","The fast aging of the population around the world makes ambient intelligence-based assistive technologies essential for sustainable and efficient health-care systems. Aging adults who decide to live alone at home need constant monitoring to control their health status and quality of life. This work introduces a new wearable device that continuously monitors the emotional state of the elderly. Electro-dermal activity (EDA) is used to classify the aging adults into two groups: sleepiness and stressed. The results report a performance of more than 83% of accuracy by using exclusively time and magnitude features from EDA events. Â© 2016 Elsevier B.V.","Ambient intelligence; Electro-dermal activity; Stress quantification; Wearable",,"Elsevier B.V.",,,,,"Article","Scopus"
"UX assessment of mobile recommender app for household electrical energy savings","Hussain A., Isam M., Mkpojiogu E.O.C.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031098384&partnerID=40&md5=92279dc317b4a188d1d2d9d56ddef6d3",2017,"Journal of Telecommunication, Electronic and Computer Engineering","In recent times, the use of mobile devices has drastically increased as a result of how feasible it is in utilizing them in solving everyday problems using several applications. One of the recurrent issues in everyday life is electricity consumption and the need for energy savings. This is an enormous issue and challenge faced by the Iraqi's Ministry of Power. Most households in Iraq lack access to information regarding electric energy consumption and the need to save and conserve energy. In addition, there is also a dearth of suitable applications that educate households about energy saving. In the light of this, this study assessed the user experience of a developed mobile recommender application for Iraqis that helps them understand the need for reduction in electricity consumption based meter readings/data supplied by users. This proposed mobile recommender app was assessed by 50 households from the point of view of ease of use, usefulness, ease of learning, and satisfaction. The study's outcome reveals that majority of the participants found that the app was easy to use, useful, easy to learn, and was satisfied with its features and functionalities. The results of the study offer some useful insights and import about the use of mobile recommender app for promoting household's energy consumption management in Iraq and also prompts a good energy savings culture and behaviour among the people.","Electricity usage; Energy savings; Recommender app; User experience",,"Universiti Teknikal Malaysia Melaka",,,,,"Article","Scopus"
"Robust multi-objective optimization for sustainable design of distributed energy supply systems","Majewski D.E., Wirtz M., Lampe M., Bardow A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009444374&doi=10.1016%2fj.compchemeng.2016.11.038&partnerID=40&md5=055dee8e33136fe8c0668210701fe228",2017,"Computers and Chemical Engineering","Sustainable design of distributed energy supply systems involves multiple aims. Therefore, multi-objective optimization is the appropriate concept for sustainable design. However, input parameters are in general uncertain. If uncertainties are disregarded in the optimization, solutions usually become infeasible in practice. To incorporate uncertain parameters, we apply the concept of minmax robust multi-objective optimization for designing sustainable energy supply systems. We propose a mixed-integer linear problem formulation. The proposed formulation allows to identify robust sustainable designs easily guaranteeing security of energy supply. Energy systems are shown to typically exhibit objective-wise uncertainties. Thus, a Pareto front can still be derived. In a real-world case study, robust designs are identified with a good trade-off between economic and ecologic criteria. The robust designs perform remarkably well in the nominal scenario. The presented problem formulation transfers the important theoretical concept of minmax robust multi-objective optimization into engineering practice for the design of sustainable energy systems. Â© 2016 Elsevier Ltd","Energy system design; Mixed-integer linear programming (MILP); Multi-objective optimization; Robust multi-objective optimization; Robust optimization; Sustainable energy supply systems",,"Elsevier Ltd",,,,,"Article","Scopus"
"Evaluations of Wireless V2X Communication Systems of for Winter Road Surveillance Systems","Uchida N., Ito K., Hirakawa G., Shibata Y.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011076091&doi=10.1109%2fNBiS.2016.83&partnerID=40&md5=a6e42444c14c5f517b81bd2caa3288e7",2016,"NBiS 2016 - 19th International Conference on Network-Based Information Systems","With the recent rapid growth of V2X (Vehicle-to-X) developments, there are many expectations for the purpose of safety driving, road guidance, and the road surveillance system during the winter. However, in case of the road surveillance system during the winter, there are some difficulties for the recognition of the frozen road conditions by the current sensor system such as the locations or elapsed time of the observed data. Therefore, this paper proposes Wireless V2X Communication Systems of for Winter Road Surveillance Systems by the Delay Tolerant Networks (DTN). In the proposed systems, the vehicles equip the wireless V2X networks, and the observed data by the sensor of Quasi Electrostatic Field (QEF) are transferred to the target node with the DTN routing. Then, this papers are evaluated the various wireless networks such as IEEE802.11a/b/g/p, Wi-SUN, and the radio directional controls, and the results are discussed for the effectiveness of the proposed systems and the future studies. Â© 2016 IEEE.","Delay Tolerant Networks; Road Surveillance Sytem; Vehicular Networks",,"Institute of Electrical and Electronics Engineers Inc.","19th International Conference on Network-Based Information Systems, NBiS 2016","7 September 2016 through 9 September 2016",,125623,"Conference Paper","Scopus"
"How is code recommendation applied in android development: A qualitative review","Wu J., Shen L., Guo W., Zhao W.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010619434&doi=10.1109%2fSATE.2016.12&partnerID=40&md5=0dcad7f393bca242afb9671f62d6e28d",2016,"Proceedings - 2016 International Conference on Software Analysis, Testing and Evolution, SATE 2016","Code recommendation has been an indispensable feature of IDE (Integrated Development Environment) for modern software developers. Android developers especially require code recommendation support to ensure high development efficiency and quality, as Android applications are developed based on Android application framework and thus are more pattern-based. Many techniques and tools have been developed to support code recommendation in different ways. However, it is not clear how these techniques and tools can be used in Android development and whether Android development can be well supported by them. In this paper, we present a qualitative review on code recommendation techniques and tools with the focus of support for Android development. We first categorize code recommendation techniques and tools reported in literatures and then evaluate them towards the requirements of Android development. The evaluation results enable us to draw a comprehensive image of how code recommendation benefits Android development and also the deficiencies of state-of-the-art code recommendation techniques in meeting specific requirements of Android development. Based on the evaluation, a comprehensive discussion is presented to suggest possible improvement and future research directions of code recommendation techniques. Â© 2016 IEEE.",,,"Institute of Electrical and Electronics Engineers Inc.","2016 International Conference on Software Analysis, Testing and Evolution, SATE 2016","3 November 2016 through 4 November 2016",,125365,"Conference Paper","Scopus"
"A search based context- Aware approach for understanding and localizing the fault via weighted call graph","Tu J., Xie X., Zhou Y., Xu B., Chen L.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010403025&doi=10.1109%2fTSA.2016.20&partnerID=40&md5=05bfe27e42da4ab4a7cc44b9e0954fa2",2016,"Proceedings - 2016 3rd International Conference on Trustworthy Systems and Their Applications, TSA 2016","Strictly speaking, fault localization includes assessing the code risk of being faulty and identifying the real fault. In practice, only highlighting some possible faulty statements is not helpful enough to reason the roots of the observed failures in a system. Programmers need to manually inspect the highlighted risky statements one by one, reading and understanding their contexts, in order to identify the real faulty ones. However, most related works have been focusing on risk assessment by simply ignoring the fault identification, which makes such techniques much less practical in real world. Therefore, in this paper, we propose a context- Aware approach to assist fault comprehension and identification. Built on risk assessment results, our approach searches for the faults on Weighted Call Graph. In our approach the risky statements are re-ordered by function call chains, which can provide much richer information to understand the context and hence reduce the efforts in manual code inspection. Case studies with three open-source systems show that the proposed approach could help to improve the effectiveness of the whole fault localization process. Â© 2016 IEEE.","Fault identification; Fault localization; Program comprehension; Weighted call graph","Hsiuping University of Science and Technology;Tunghai University","Institute of Electrical and Electronics Engineers Inc.","3rd International Conference on Trustworthy Systems and Their Applications, TSA 2016","18 September 2016 through 20 September 2016",,125371,"Conference Paper","Scopus"
"Engineering Adaptive Model-Driven User Interfaces","Akiki P.A., Bandara A.K., Yu Y.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006710450&doi=10.1109%2fTSE.2016.2553035&partnerID=40&md5=f56f6372b900ca991623f1aefbd140ad",2016,"IEEE Transactions on Software Engineering","Software applications that are very large-scale, can encompass hundreds of complex user interfaces (UIs). Such applications are commonly sold as feature-bloated off-the-shelf products to be used by people with variable needs in the required features and layout preferences. Although many UI adaptation approaches were proposed, several gaps and limitations including: extensibility and integration in legacy systems, still need to be addressed in the state-of-the-art adaptive UI development systems. This paper presents Role-Based UI Simplification (RBUIS) as a mechanism for increasing usability through adaptive behavior by providing end-users with a minimal feature-set and an optimal layout, based on the context-of-use. RBUIS uses an interpreted runtime model-driven approach based on the Cedar Architecture, and is supported by the integrated development environment (IDE), Cedar Studio. RBUIS was evaluated by integrating it into OFBiz, an open-source ERP system. The integration method was assessed and measured by establishing and applying technical metrics. Afterwards, a usability study was carried out to evaluate whether UIs simplified with RBUIS show an improvement over their initial counterparts. This study leveraged questionnaires, checking task completion times and output quality, and eye-tracking. The results showed that UIs simplified with RBUIS significantly improve end-user efficiency, effectiveness, and perceived usability. Â© 1976-2012 IEEE.","Design tools and techniques; software architectures; support for adaptation; user interfaces",,"Institute of Electrical and Electronics Engineers Inc.",,,,,"Article","Scopus"
"3D visual discomfort prediction using low complexity disparity algorithms","Chen J., Zhou J., Sun J., Bovik A.C.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985995649&doi=10.1186%2fs13640-016-0127-4&partnerID=40&md5=a6db345966e611f6ff6c7f5d9698797b",2016,"Eurasip Journal on Image and Video Processing","Algorithms that predict the degree of visual discomfort experienced when viewing stereoscopic 3D (S3D) images usually first execute some form of disparity calculation. Following that, features are extracted on these disparity maps to build discomfort prediction models. These features may include, for example, the maximum disparity, disparity range, disparity energy, and other measures of the disparity distribution. Hence, the accuracy of prediction largely depends on the accuracy of disparity calculation. Unfortunately, computing disparity maps is expensive and difficult and most leading assessment models are based on features drawn from the outputs of high complexity disparity calculation algorithms that deliver high quality disparity maps. There is no consensus on the type of stereo matching algorithm that should be used for this type of model. Towards filling this gap, we study the relative performances of discomfort prediction models that use disparity algorithms having different levels of complexity. We also propose a set of new discomfort predictive features with good performance even when using low complexity disparity algorithms. Â© 2016, The Author(s).","3D NSS; Low complexity disparity calculation algorithms; Uncertainty map; Visual discomfort",,"Springer International Publishing",,,,,"Article","Scopus"
"An empirical analysis of the relationship between the initialization method performance and the convergence speed of a meta-heuristic for Fuzzy Job-Shop scheduling problems","Shaheed I.M., Shukor S.A., Nababan E.B.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002199029&partnerID=40&md5=9aee521fc43382881b76decc744021d6",2016,"Journal of Theoretical and Applied Information Technology","Numerous studies mentioned that the quality initial population can affect meta-heuristics convergence speed, these studies are often theoretical. However, the functionality of the initial population is extensively ignored. This overlooking may due to the literature lack of statistical evidence on the relationship between the initialization method performance and a meta-heuristic convergence speed. Therefore, this study statistically investigated aforementioned relationship by conducting an experiment and used population quality and best error rate (BRE) to gauge the performance of the state of the art initialization methods for Fuzzy Job-Shop Scheduling Problems (Fuzzy JSSPs), namely, random-based and priority rules-based methods. Thereafter, this initialization approach utilised to initiate a memetic algorithm (MA). CPU time was used to compute the MA time to reach the lower bound of 50 different sized Fuzzy JSSP instances. A Spearman's test was operated to measure the intended correlation. As a result, there was effective negative association between the initial population quality and the MA convergence speed. While, there was a dominant positive relationship between the BRE and MA convergence speed. Consequently, it is highly recommended to develop advanced initialization approaches that can generate high-quality initial population, which consists of most favourable or near to best possible solutions. Â© 2005-2016 JATIT & LLS. All rights reserved.","Best relative error; Convergence speed; Fuzzy job-shop scheduling problems; Initialization methods; Memetic algorithm; Population quality",,"Asian Research Publishing Network",,,,,"Article","Scopus"
"Addressing scalability in API method call analytics","Cergani E., Proksch S., Nadi S., Mezini M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006973492&doi=10.1145%2f2989238.2989240&partnerID=40&md5=77a6c0917ef88f5e83b2715326f85cae",2016,"SWAN 2016 - Proceedings of the 2nd International Workshop on Software Analytics, co-located with FSE 2016","Intelligent code completion recommends relevant code to de- velopers by comparing the editor content to code patterns extracted by analyzing large repositories. However, with the vast amount of data available in such repositories, scalability of the recommender system becomes an issue. We propose using Boolean Matrix Factorization (BMF) as a clustering technique for analyzing code in order to improve scalability of the underlying models. We compare model size, inference speed, and prediction quality of an intelligent method call completion engine built on top of canopy clustering versus one built on top of BMF. Our results show that BMF reduces model size up to 80% and increases inference speed up to 78%, without signifficant change in prediction quality. Â© 2016 ACM.","Analytics of code repositories; Boolean Matrix Factorization; Intelligent method call completion; Scalability","ACM SIGSOFT","Association for Computing Machinery, Inc","2nd International Workshop on Software Analytics, SWAN 2016","13 November 2016",,124934,"Conference Paper","Scopus"
"A cross-Tool communication study on program analysis tool notifications","Johnson B., Pandita R., Smith J., Ford D., Elder S., Murphy-Hill E., Heckman S., Sadowski C.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997402803&doi=10.1145%2f2950290.2950304&partnerID=40&md5=1e9f7b8a7bc5b1b32c8aad2a00b8b3d9",2016,"Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering","Program analysis tools use notifications to communicate with developers, but previous research suggests that developers encounter challenges that impede this communication. This paper describes a qualitative study that identifies 10 kinds of challenges that cause notifications to miscommunicate with developers. Our resulting notification communication theory reveals that many challenges span multiple tools and multiple levels of developer experience. Our results suggest that, for example, future tools that model developer experience could improve communication and help developers build more accurate mental models. Â© 2016 ACM.","Communication; Human Factors; Program Analysis Tools","Special Interest Group on Software Engineering (ACM SIGSOFT)","Association for Computing Machinery","24th ACM SIGSOFT International Symposium on Foundations of Software Engineering, FSE 2016","13 November 2016 through 18 November 2016",,124602,"Conference Paper","Scopus"
"Why we Refactor? Confessions of Github contributors","Silva D., Tsantalis N., Valente M.T.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997428839&doi=10.1145%2f2950290.2950305&partnerID=40&md5=29c96c43c7fbcc798c112eee9b56c1ac",2016,"Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering","Refactoring is a widespread practice that helps developers to improve the maintainability and readability of their code. However, there is a limited number of studies empirically investigating the actual motivations behind specific refac-toring operations applied by developers. To fill this gap, we monitored Java projects hosted on GitHub to detect re-cently applied refactorings, and asked the developers to ex-plain the reasons behind their decision to refactor the code. By applying thematic analysis on the collected responses, we compiled a catalogue of 44 distinct motivations for 12 well-known refactoring types. We found that refactoring ac-tivity is mainly driven by changes in the requirements and much less by code smells. Extract Method is the most versatile refactoring operation serving 11 different purposes. Finally, we found evidence that the IDE used by the devel-opers affects the adoption of automated refactoring tools.","Code smells; GitHub; Refactoring; Software evolution","Special Interest Group on Software Engineering (ACM SIGSOFT)","Association for Computing Machinery","24th ACM SIGSOFT International Symposium on Foundations of Software Engineering, FSE 2016","13 November 2016 through 18 November 2016",,124602,"Conference Paper","Scopus"
"Interactive and guided architectural refactoring with search-based recommendation","Lin Y., Peng X., Cai Y., Dig D., Zheng D., Zhao W.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997124483&doi=10.1145%2f2950290.2950317&partnerID=40&md5=a398b600201d12b8e78ac14606f33ccb",2016,"Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering","Architectural refactorings can contain hundreds of steps and experienced developers could carry them out over several weeks. Moreover, developers need to explore a correct sequence of refactorings steps among many more incorrect alternatives. Thus, carrying out architectural refactorings is costly, risky, and challenging. In this paper, we present Refactoring Navigator: A tool-supported and interactive recommendation approach for aiding architectural refactoring. Our approach takes a given implementation as the starting point, a desired high-level design as the target, and iteratively recommends a series of refactoring steps. Moreover, our approach allows the user to accept, reject, or ignore a recommended refactoring step, and uses the user's feedback in further refactoring recommendations. We evaluated the effectiveness of our approach and tool using a controlled experiment and an industrial case study. The controlled experiment shows that the participants who used Refactoring Navigator accomplished their tasks in 77.4% less time and manually edited 98.3% fewer lines than the control group. The industrial case study suggests that Refactoring Navigator has the potential to help with architectural refactorings in practice. Â© 2016 ACM.","Automatic Refactoring; High-Level Design; Interactive; Reflexion Model; User Feedback","Special Interest Group on Software Engineering (ACM SIGSOFT)","Association for Computing Machinery","24th ACM SIGSOFT International Symposium on Foundations of Software Engineering, FSE 2016","13 November 2016 through 18 November 2016",,124602,"Conference Paper","Scopus"
"Beyond the attack surface assessing security risk with random walks on call graphs","Munaiah N., Meneely A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002083583&doi=10.1145%2f2995306.2995311&partnerID=40&md5=c0e9667f89935b47c8b3cb85c25f3a4c",2016,"SPRO 2016 - Proceedings of the 2016 ACM Workshop on Software PROtection, co-located with CCS 2016","When reasoning about software security, researchers and practitioners use the phrase \attack surface"" as a metaphor for risk. Enumerate and minimize the ways attackers can break in then risk is reduced and the system is better protected, the metaphor says. But software systems are much more complicated than their surfaces. We propose functionand file-level attack surface metrics|proximity and risky walk|that enable fine-grained risk assessment. Our risky walk metric is highly configurable: we use PageRank on a probability-weighted call graph to simulate attacker behavior of finding or exploiting a vulnerability. We provide evidence-based guidance for deploying these metrics, including an extensive parameter tuning study. We conducted an empirical study on two large open source projects, FFmpeg and Wireshark, to investigate the potential correlation between our metrics and historical post-release vulnerabilities. We found our metrics to be statistically significantly associated with vulnerable functions/files with a small-to-large Cohen's d effect size. Our prediction model achieved an increase of 36% (in FFmpeg) and 27% (in Wireshark) in the average value of F2-measure over a base model built with SLOC and coupling metrics. Our prediction model outperformed comparable models from prior literature with notable improvements: 58% reduction in false negative rate, 81% reduction in false positive rate, and 548% increase in F2-measure. These metrics advance vulnerability prevention by (a) being exible in terms of granularity, (b) performing better than vulnerability prediction literature, and (c) being tunable so that practitioners can tailor the metrics to their products and better assess security risk. Â© 2016 ACM.","Attack surface; Metric; Page rank; Risk; Vulnerability","ACM SIGSAC","Association for Computing Machinery, Inc","2nd International Workshop on Software PROtection, SPRO 2016","28 October 2016",,124612,"Conference Paper","Scopus"
"Searching crowd knowledge to recommend solutions for API usage tasks","Campos E.C., de Souza L.B.L., Maia M.D.A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979781067&doi=10.1002%2fsmr.1800&partnerID=40&md5=237ba07637d1e26580b45f52af70c1f5",2016,"Journal of Software: Evolution and Process","Stack Overflow (SO) is a question and answer service directed to issues related to software development. In SO, developers post questions related to a programming topic and other members of the site can provide answers to help them. The information available on this type of service is also known as â€˜crowd knowledgeâ€™ and currently is one important trend in supporting activities related to software development. We present an approach that makes use of â€˜crowd knowledgeâ€™ in SO to recommend information that can assist developer activities. This strategy recommends a ranked list of question-answer pairs from SO based on a query. The criteria for ranking are based on three main aspects: the textual similarity of the pairs with respect to the query related to the developer's problem, the quality of the pairs, and a filtering mechanism that considers only â€˜how-toâ€™ posts. We conducted an experiment considering programming problems on three different topics (Swing, Boost and LINQ) widely used by the software development community to evaluate the proposed recommendation strategy. The results have shown that for Lucene + Score + How-to approach, 77.14% of the assessed activities have at least one recommended pair proved to be useful concerning the target programming problem. Copyright Â© 2016 John Wiley & Sons, Ltd. Copyright Â© 2016 John Wiley & Sons, Ltd.","crowd knowledge; Q&A services; recommendation systems",,"John Wiley and Sons Ltd",,,,,"Conference Paper","Scopus"
"A hub-and-spoke model for tool integration in distributed development","Calefato F., Lanubile F.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994642865&doi=10.1109%2fICGSE.2016.12&partnerID=40&md5=ab1d6fb39c00c45dfd17e0c02fba824d",2016,"Proceedings - 11th IEEE International Conference on Global Software Engineering, ICGSE 2016","Today distributed development depend on an ever-growing plethora of tools that provide a continual stream of updates and place developers into a situation of channel overload and information fragmentation. In this paper, we present our initial work on the definition of a model, named hub-and-spoke, for a loosely-coupled integration of development tools that can help developers cope with these issues, while also increasing their overall situational awareness. Â© 2016 IEEE.","awareness; channel overload; devops; distributed development; information fragmentation; tool integration","Collaboration Research in Action, Design, and Learning (CRALD);IEEE Computer Society;Microsoft;Siemens;UCI Donald Bren School of Information and Computer Sciences;University of California, Institute for Software Research","Institute of Electrical and Electronics Engineers Inc.","11th IEEE International Conference on Global Software Engineering, ICGSE 2016","2 August 2016 through 5 August 2016",,124102,"Conference Paper","Scopus"
"Staged model evolution and proactive quality guidance for model libraries","Ganser A., Lichter H., Roth A., Rumpe B.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948147312&doi=10.1007%2fs11219-015-9298-y&partnerID=40&md5=d5eba656895ab668cb1c28edff6ae519",2016,"Software Quality Journal","A variety of modeling approaches, including model-driven development, consider model reuse as one of their cornerstones, but lack support for model reuse. This may be due to the available model repositories that barely exceed support for enhanced versioning or collaborative work and disregard model evolution. We believe that current model evolution approaches do not consider reuse sufficiently and that model repositories for reuse purposes should act as model libraries. This requires new functionality, because models for reuse need to achieve and maintain high quality. Moreover, quality assessment and assurance, which are tasks often considered tedious, need to be as simple as putting away or maintaining artifacts for reuse. In this study, we propose an approach for model evolution in UML model libraries that differs from general model evolution, since it is aimless and triggered by new external requirements. Our approach is founded on graphs that are partitioned into three stages with respect to the level of reusability. Each stage is defined by quality characteristics that are manifestations of a quality model consisting of four essential quality dimensions: syntactic, semantic, pragmatic, and emotional. In order to achieve the next level of reusability, i.e., change the stage of a model, a quality gate needs to be passed. This can be supported by a proactive approach that guides the modeler through the enhancement process and offers additional recommendations based on the level of reusability. Since guidance cannot be fully automated, we implement a review mechanism founded on the idea of the six thinking hats to help maintain focus on the main aspects of a review. Finally, our approach is enhanced to support the evolution of generations, i.e., a group of several model snapshots, to ease reusability. Â© 2015, Springer Science+Business Media New York.","Evolution; Libraries; Model; Quality; UML",,"Springer New York LLC",,,,,"Article","Scopus"
"Query Expansion Based on Crowd Knowledge for Code Search","Nie L., Jiang H., Ren Z., Sun Z., Li X.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994508124&doi=10.1109%2fTSC.2016.2560165&partnerID=40&md5=7a96c12b0ca2caf68958e829a55697be",2016,"IEEE Transactions on Services Computing","As code search is a frequent developer activity in software development practices, improving the performance of code search is a critical task. In the text retrieval based search techniques employed in the code search, the term mismatch problem is a critical language issue for retrieval effectiveness. By reformulating the queries, query expansion provides effective ways to solve the term mismatch problem. In this paper, we propose Query Expansion based on Crowd Knowledge (QECK), a novel technique to improve the performance of code search algorithms. QECK identifies software-specific expansion words from the high quality pseudo relevance feedback question and answer pairs on Stack Overflow to automatically generate the expansion queries. Furthermore, we incorporate QECK in the classic Rocchio's model, and propose QECK based code search method QECKRocchio. We conduct three experiments to evaluate our QECK technique and investigate QECKRocchio in a large-scale corpus containing real-world code snippets and a question and answer pair collection. The results show that QECK improves the performance of three code search algorithms by up to 64 percent in Precision, and 35 percent in NDCG. Meanwhile, compared with the state-of-the-art query expansion method, the improvement of QECKRocchio is 22 percent in Precision, and 16 percent in NDCG. Â© 2016 IEEE.","Code search; crowd knowledge; information retrieval; query expansion; question & answer pair",,"Institute of Electrical and Electronics Engineers",,,,,"Article","Scopus"
"Ambiguity and tacit knowledge in requirements elicitation interviews","Ferrari A., Spoletini P., Gnesi S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961575358&doi=10.1007%2fs00766-016-0249-3&partnerID=40&md5=0669e880ad9b9f18e7ef1bdad28b2c6b",2016,"Requirements Engineering","Interviews are the most common and effective means to perform requirements elicitation and support knowledge transfer between a customer and a requirements analyst. Ambiguity in communication is often perceived as a major obstacle for knowledge transfer, which could lead to unclear and incomplete requirements documents. In this paper, we analyze the role of ambiguity in requirements elicitation interviews, when requirements are still tacit ideas to be surfaced. To study the phenomenon, we performed a set of 34 customerâ€“analyst interviews. This experience was used as a baseline to define a framework to categorize ambiguity. The framework presents the notion of ambiguity as a class of four main sub-phenomena, namely unclarity, multiple understanding, incorrect disambiguation and correct disambiguation. We present examples of ambiguities from our interviews to illustrate the different categories, and we highlight the pragmatic components that determine the occurrence of ambiguity. Along the study, we discovered a peculiar relation between ambiguity and tacit knowledge in interviews. Tacit knowledge is the knowledge that a customer has but does not pass to the analyst for any reason. From our experience, we have discovered that, rather than an obstacle, the occurrence of an ambiguity is often a resource for discovering tacit knowledge. Again, examples are presented from our interviews to support this vision. Â© 2016, Springer-Verlag London.","Ambiguity; Interviews; Natural language; Requirements elicitation; Requirements engineering",,"Springer London",,,,,"Article","Scopus"
"Demonstrating correspondence between decision-support models and dynamics of real-world environmental systems","Huffaker R., MuÃ±oz-Carpena R., Campo-BescÃ³s M.A., Southworth J.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969759528&doi=10.1016%2fj.envsoft.2016.04.024&partnerID=40&md5=f8392d6898767d070273ae897ad1be75",2016,"Environmental Modelling and Software","There are increasing calls to audit decision-support models used for environmental policy to ensure that they correspond with the reality facing policy makers. Modelers can establish correspondence by providing empirical evidence of real-world behavior that their models skillfully simulate. Since real-world behavior-especially in environmental systems-is often complex, credibly modeling underlying dynamics is essential. We present a pre-modeling diagnostic framework based on Nonlinear Time Series (NLTS) methods for reconstructing real-world environmental dynamics from observed data. The framework is illustrated with a case study of saltwater intrusion into coastal wetlands in Everglades National Park, Florida, USA. We propose that environmental modelers test for systematic dynamic behavior in observed data before resorting to conventional stochastic exploratory approaches unable to detect this valuable information. Reconstructed data dynamics can be used, along with other expert information, as a rigorous benchmark to guide specification and testing of environmental decision-support models corresponding with real-world behavior. Â© 2016 Elsevier Ltd.","Extreme value statistics; Model evaluation; Nonlinear dynamics; Phase space reconstruction",,"Elsevier Ltd",,,,,"Article","Scopus"
"QUICKAR: Automatic query reformulation for concept location using crowdsourced knowledge","Rahman M.M., Roy C.K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989195962&doi=10.1145%2f2970276.2970362&partnerID=40&md5=1dd46e369b8469dfb5860e532c71f525",2016,"ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering","During maintenance, software developers deal with numerous change requests made by the users of a software system. Studies show that the developers find it challenging to select appropriate search terms from a change request during concept location. In this paper, we propose a novel technique-QUICKAR-that automatically suggests helpful reformulations for a given query by leveraging the crowdsourced knowledge from Stack Overow. It determines semantic similarity or relevance between any two terms by analyzing their adjacent word lists from the programming questions of Stack Overow, and then suggests semantically relevant queries for concept location. Experiments using 510 queries from two software systems suggest that our technique can improve or preserve the quality of 76% of the initial queries on average which is promising. Comparison with one baseline technique validates our preliminary findings, and also demonstrates the potential of our technique. Â© 2016 ACM.","Adjacency list; Crowdsourced knowledge; Query reformulation; Semantic rel-evance; Stack Overow; Word co-occurrence","et al.;Lee Foundation;Living Analytics Research Centre (LARC);Singapore Management University (SMU);Singapore Management University (SMU), School of Information Systems;Software Analytics Research Group (SOAR)","Association for Computing Machinery, Inc","31st IEEE/ACM International Conference on Automated Software Engineering, ASE 2016","3 September 2016 through 7 September 2016",,123481,"Conference Paper","Scopus"
"Evaluating the evaluations of code recommender systems: A reality check","Proksch S., Amann S., Nadi S., Mezini M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989170526&doi=10.1145%2f2970276.2970330&partnerID=40&md5=89470c873e1e038c1cd70392d1471069",2016,"ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering","While researchers develop many new exciting code recommender systems, such as method-call completion, code-snippet completion, or code search, an accurate evaluation of such systems is always a challenge. We analyzed the current literature and found that most of the current evaluations rely on artificial queries extracted from released code, which begs the question: Do such evaluations reect real-life usages? To answer this question, we capture 6,189 fine-grained development histories from real IDE interactions. We use them as a ground truth and extract 7,157 real queries for a specific method-call recommender system. We compare the results of such real queries with different artificial evaluation strategies and check several assumptions that are repeatedly used in research, but never empirically evaluated. We find that an evolving context that is often observed in practice has a major effect on the prediction quality of recommender systems, but is not commonly reected in artificial evaluations. Â© 2016 ACM.","Artificial Evaluation; Empirical Study; IDE Interaction Data","et al.;Lee Foundation;Living Analytics Research Centre (LARC);Singapore Management University (SMU);Singapore Management University (SMU), School of Information Systems;Software Analytics Research Group (SOAR)","Association for Computing Machinery, Inc","31st IEEE/ACM International Conference on Automated Software Engineering, ASE 2016","3 September 2016 through 7 September 2016",,123481,"Conference Paper","Scopus"
"It Takes Two to Tango: Deleted Stack Overflow Question Prediction with Text and Meta Features","Xia X., Lo D., Correa D., Sureka A., Shihab E.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987984525&doi=10.1109%2fCOMPSAC.2016.145&partnerID=40&md5=c5b37ce5ba6622d5162e5ee3194238f0",2016,"Proceedings - International Computer Software and Applications Conference","Stack Overflow is a popular community-based Q&A website that caters to technical needs of software developers. As of February 2015 - Stack Overflow has more than 3.9M registered users, 8.8M questions, and 41M comments. Stack Overflow provides explicit and detailed guidelines on how to post questions but, some questions are very poor in quality. Such questions are deleted by the experienced community members and moderators. Deleted questions increase maintenance cost and have an adverse impact on the user experience. Therefore, predicting deleted questions is an important task. In this study, we propose a two stage hybrid approach - DelPredictor - which combines text processing and classification techniques to predict deleted questions. In the first stage, DelPredictor converts text in the title, body, and tag fields of questions into numerical textual features via text processing and classification techniques. In the second stage, it extracts meta features that can be categorized into: profile, community, content, and syntactic features. Next, it learns and combines two independent classifiers built on the textual and meta features. We evaluate DelPredictor on 5 years (2008 - 2013) of deleted questions from Stack Overflow. Our experimental results show that DelPredictor improves the F1-scores over baseline prediction, a prior approach [12] and a text-based approach by 29.50%, 9.34%, and 28.11%, respectively. Â© 2016 IEEE.","Classification; Deleted Question; Stack Overflow; Text Processing",,"IEEE Computer Society","2016 IEEE 40th Annual Computer Software and Applications Conference, COMPSAC 2016","10 June 2016 through 14 June 2016",,123590,"Conference Paper","Scopus"
"A study on estimating the attractiveness of food photography","Takahashi K., Doman K., Kawanishi Y., Hirayama T., Ide I., Deguchi D., Murase H.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987657341&doi=10.1109%2fBigMM.2016.41&partnerID=40&md5=a4f310a262520feeb0c7992263180c0e",2016,"Proceedings - 2016 IEEE 2nd International Conference on Multimedia Big Data, BigMM 2016","This paper proposes a method for estimating the attractiveness of food photos in order to assist a user to shoot them attractively. The proposed method extracts both color and shape features from input food images, and then integrates them according to a regression scheme. By this way, the proposed method estimates the attractiveness of an unknown food photo. We also created a food image dataset taken from various 3D-angles for each food category, and set target values of their attractiveness through subjective experiments. Then, we evaluated the performance of the proposed method in two different ways of constructing the attractiveness estimator: One that constructs it for each food category, and the other that constructs a common attractiveness estimator for all food categories. Experimental results showed the effectiveness of the proposed method in addition to the necessity for adaptively selecting the estimator depending on the appearance of foods for further performance improvement. Â© 2016 IEEE.","Attractiveness; Food photo; Framing",,"Institute of Electrical and Electronics Engineers Inc.","2nd IEEE International Conference on Multimedia Big Data, BigMM 2016","20 April 2016 through 22 April 2016",,123381,"Conference Paper","Scopus"
"A Semantic Approach to Intelligent and Personal Tutoring System","Sette M., Tao L., Gai K., Jiang N.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987642029&doi=10.1109%2fCSCloud.2016.50&partnerID=40&md5=96262af0df4cd25e46320941b21011e8",2016,"Proceedings - 3rd IEEE International Conference on Cyber Security and Cloud Computing, CSCloud 2016 and 2nd IEEE International Conference of Scalable and Smart Cloud, SSC 2016","Cyberlearning is challenged by the lack of personal and assessment-driven learning, and students are often puzzled by the lack of instructor guidance and feedback, the huge volume and diversity of the learning materials, and thelack of the ability to zoom in from the general concepts to the more specific ones, or the opposite. Intelligent tutoring systems are needed to improve the cyberlearning quality. One of the major difficulties is knowledge representation. The current industry standard is to use Web Ontology Language (OWL) forrepresenting knowledge structure. But OWL only supports one ""first-class"" relation, ""is-a"", between the concepts, and different knowledge areas usually need different custom relations to describe the relations among the concepts. For example ""part-of"" and time dependency are important relations torepresent most engineering knowledge bodies. OWL has to use object properties to emulate such custom relations, leading to awkward knowledge representation hard for domain experts to code, validate and use such knowledge bases. This research uses Pace University's extension to OWL, named Knowledge Graph (KG), to support knowledge representation with custom relations. The instructors can use Pace University extended Protege IDE to declare and apply custom relations in a single document. The instructor teaching experience is also coded in the KG to better support custom learning order by students with different backgrounds. The prototype of a knowledge-driven tutoring system was designed and implemented to illustrate how the KG supports integrated assessments, using assessment results to custom student learning order or material, and let the students freely navigate in the knowledge space from general to specific or the opposite, and following various custom relations. A web technology tutorial is used to validate the design and effectiveness of this approach. Â© 2016 IEEE.","adaptive learning; Custom relations; intelligent tutoring systems; knowledge representation; learning objects","Beihang University;et al.;IEEE;IEEE Computer Society;IEEE TCSC;Pace University","Institute of Electrical and Electronics Engineers Inc.","3rd IEEE International Conference on Cyber Security and Cloud Computing, CSCloud 2016 and 2nd IEEE International Conference of Scalable and Smart Cloud, SSC 2016","25 June 2016 through 27 June 2016",,123376,"Conference Paper","Scopus"
"Major motivations for extract method refactorings: analysis based on interviews and change histories","Liu W., Liu H.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978175718&doi=10.1007%2fs11704-016-5131-4&partnerID=40&md5=d1de2566e06bb786e7d197140c33ca52",2016,"Frontiers of Computer Science","Extract method is one of the most popular software refactorings. However, little work has been done to investigate or validate the major motivations for such refactorings. Digging into this issue might help researchers to improve tool support for extract method refactorings, e.g., proposing better tools to recommend refactoring opportunities, and to select fragments to be extracted. To this end, we conducted an interview with 25 developers, and our results suggest that current reuse, decomposition of long methods, clone resolution, and future reuse are the major motivations for extract method refactorings.We also validated the results by analyzing the refactoring history of seven open-source applications. Analysis results suggest that current reuse was the primary motivation for 56% of extract method refactorings, decomposition of methods was the primary motivation for 28% of extract method refactorings, and clone resolution was the primary motivation for 16% of extract method refactorings. These findings might suggest that recommending extract method opportunities by analyzing only the inner structure (e.g., complexity and length) of methods alone would miss many extract method opportunities. These findings also suggest that extract method refactorings are often driven by current and immediate reuse. Consequently, how to recognize or predict reuse requirements timely during software evolution may play a key role in the recommendation and automation of extract method refactorings. We also investigated the likelihood for the extracted methods to be reused in future, and our results suggest that such methods have a small chance Received April 2, 2015; accepted November 10, 2015 E-mail: Liuhui08@bit.edu.cn (12%) to be reused in future unless the extracted fragment could be reused immediately in software evolution and extracting such a fragment can resolve existing clones at the same time. Â© 2016, Higher Education Press and Springer-Verlag Berlin Heidelberg.","data mining; extract method; motivation; software quality; software refactoring",,"Higher Education Press",,,,,"Article","Scopus"
"A cooperative approach for combining client-based and library-based API usage pattern mining","Saied M.A., Sahraoui H.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979759219&doi=10.1109%2fICPC.2016.7503717&partnerID=40&md5=dd03d6baf8c8bdad5b8b1015460734e9",2016,"IEEE International Conference on Program Comprehension","Software developers need to cope with the complexity of Application Programming Interfaces (APIs) of external libraries or frameworks. Typical APIs provide thousands of methods to their client programs, and these methods are not used independently of each other. Much existing work has provided different techniques to mine API usage patterns based on client programs in order to help developers understanding and using existing libraries. Other techniques propose to overcome the strong constraint of clients' dependency and infer API usage patterns only using the library source code. In this paper, we propose a cooperative usage pattern mining technique (COUPminer) that combines client-based and library-based usage pattern mining. We evaluated our technique through four APIs and the obtained results show that the cooperative approach allows taking advantage at the same time from the precision of client-based technique and from the generalizability of library-based techniques. Â© 2016 IEEE.","API Documentation; API Usage; Software Clustering; Usage Pattern","Association for Computing Machinery (ACM);IEEE Computer Society;IEEE Technical Council on Software Engineering (TCSE);Special Interest Group on Software Engineering (SIGSOFT)","IEEE Computer Society","24th IEEE International Conference on Program Comprehension, ICPC 2016","16 May 2016 through 17 May 2016",,122593,"Conference Paper","Scopus"
"TrendQuery: A system for interactive exploration of trends","Kamat N., Wu E., Nandi A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979752521&doi=10.1145%2f2939502.2939514&partnerID=40&md5=7deedb49e20e891134d0b8ee80870e32",2016,"HILDA 2016 - Proceedings of the Workshop on Human-In-the-Loop Data Analytics","The surfacing of trends from data collections such as user-generated content streams and news articles is a popular and important data analysis activity, used in applications such as business intelligence, quantitative stock trading and, social media exploration. Unlike traditional content analysis, trend analysis includes an additional vital time dimension: a trend can be defined as a temporal pattern over a group of semantically related items. The unsupervised discovery of trends is often not sufficient, either due to inadequacies in the trend analysis algorithm, or because the data collection itself does not possess all of the information to identify the trend. Thus, it is necessary for an expert human-in-the-loop to be involved in the process of trend analysis. To this end, we introduce TrendQuery, a system designed towards iterative and interactive surfacing of trends. Our system provides a set of trends to the expert, and enumerates iterative operations to curate the result. This process continues until the expert is satisfied with the surfaced trends. Since the space of possible tweaks to the result can be extremely large, the system continually provides feedback and guidance to the expert to prioritize possible operations. Our system allows interactive curation of trends providing better insights than a purely unsupervised approach. Â© 2016 Copyright held by the owner/author(s).",,"IBM;Paxata;Tableau;Trifacta","Association for Computing Machinery, Inc","1st Workshop on Human-in-the-Loop Data Analytics, HILDA 2016","26 June 2016",,122413,"Conference Paper","Scopus"
"Interactive exploration of developer interaction traces using a Hidden Markov Model","Damevski K., Chen H., Shepherd D., Pollock L.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974534250&doi=10.1145%2f2901739.2901741&partnerID=40&md5=f66cc7a27d811eec03bab028b42e429d",2016,"Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016","Using IDE usage data to analyze the behavior of software developers in the field, during the course of their daily work, can lend support to (or dispute) laboratory studies of developers. This paper describes a technique that leverages Hidden Markov Models (HMMs) as a means of mining high-level developer behavior from low-level IDE interaction traces of many developers in the field. HMMs use dual stochastic processes to model higher-level hidden behavior using observable input sequences of events. We propose an interactive approach of mining interpretable HMMs, based on guiding a human expert in building a high quality HMM in an iterative, one state at a time, manner. The final result is a model that is both representative of the field data and captures the field phenomena of interest. We apply our HMM construction approach to study debugging behavior, using a large IDE interaction dataset collected from nearly 200 developers at ABB, Inc. Our results highlight the different modes and constituent actions in debugging, exhibited by the developers in our dataset. Â© 2016 ACM.",,"Association for Computing Machinery, Special Interest Group on Software Engineering (ACM SIGSOFT);IEEE Computer Society;IEEE Technical Council on Software Engineering (TCSE)","Association for Computing Machinery, Inc","13th Working Conference on Mining Software Repositories, MSR 2016","14 May 2016 through 15 May 2016",,121803,"Conference Paper","Scopus"
"Nomenest omen: Exploring and exploiting similarities between argument and parameter names","Liu H., Liu Q., Staicu C.-A., Pradel M., Luo Y.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971482960&doi=10.1145%2f2884781.2884841&partnerID=40&md5=54af6b38397b8bcfb88f5b7303b7a43c",2016,"Proceedings - International Conference on Software Engineering","Programmer-provided identifier names convey information about the semantics of a program. This information can complement traditional program analyses in various software engineering tasks, such as bug finding, code completion, and documentation. Even though identifier names appear to be a rich source of information, little is known about their properties and their potential usefulness. This paper presents an empirical study of the lexical similarity between arguments and parameters of methods, which is one prominent situation where names can provide otherwise missing information. The study involves 60 real-world Java programs. We find that, for most arguments, the similarity is either very high or very low, and that short and generic names often cause low similarities. Furthermore, we show that inferring a set of low-similarity parameter names from one set of programs allows for pruning such names in another set of programs. Finally, the study shows that many arguments are more similar to the corresponding parameter than any alternative argument available in the call site's scope. As applications of our findings, we present an anomaly detection technique that identifies 144 renaming opportunities and incorrect arguments in 14 programs, and a code recommendation system that suggests correct arguments with a precision of 83%. Â© 2016 ACM.","Empirical study; Identifier names; Method arguments; Name-based program analysis; Static analysis","Association for Computing Machinery, Special Interest Group on Software Engineering (ACM SIGSOFT);IEEE Computer Society;IEEE Technical Council on Software Engineering (TCSE)","IEEE Computer Society","2016 IEEE/ACM 38th IEEE International Conference on Software Engineering, ICSE 2016","14 May 2016 through 22 May 2016",,121623,"Conference Paper","Scopus"
"Mining duplicate questions in Stack Overflow","Ahasanuzzaman M., Asaduzzaman M., Roy C.K., Schneider K.A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974627765&doi=10.1145%2f2901739.2901770&partnerID=40&md5=7a9fddb75718a7d90dd42eca9587370c",2016,"Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016","Stack Overflow is a popular question answering site that is focused on programming problems. Despite efforts to prevent asking questions that have already been answered, the site contains duplicate questions. This may cause developers to unnecessarily wait for a question to be answered when it has already been asked and answered. The site currently depends on its moderators and users with high reputation to manually mark those questions as duplicates, which not only results in delayed responses but also requires additional efforts. In this paper, we first perform a manual investigation to understand why users submit duplicate questions in Stack Overflow. Based on our manual investigation we propose a classification technique that uses a number of carefully chosen features to identify duplicate questions. Evaluation using a large number of questions shows that our technique can detect duplicate questions with reasonable accuracy. We also compare our technique with DupPredictor, a state-ofthe-art technique for detecting duplicate questions, and we found that our proposed technique has a better recall-rate than that technique. Â© 2016 ACM.","Discriminative classifier; Duplicate questions; Stack Overflow","Association for Computing Machinery, Special Interest Group on Software Engineering (ACM SIGSOFT);IEEE Computer Society;IEEE Technical Council on Software Engineering (TCSE)","Association for Computing Machinery, Inc","13th Working Conference on Mining Software Repositories, MSR 2016","14 May 2016 through 15 May 2016",,121803,"Conference Paper","Scopus"
"Technology at the table: Attitudes about mobile phone use at mealtimes","Moser C., Schoenebeck S.Y., Reinecke K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991457093&doi=10.1145%2f2858036.2858357&partnerID=40&md5=b376010b927dafea482f50bc17cb7611",2016,"Conference on Human Factors in Computing Systems - Proceedings","Mealtimes are a cherished part of everyday life around the world. Often centered on family, friends, or special occasions, sharing meals is a practice embedded with traditions and values. However, as mobile phone adoption becomes increasingly pervasive, tensions emerge about how appropriate it is to use personal devices while sharing a meal with others. Furthermore, while personal devices have been designed to support awareness for the individual user (e.g., notifications), little is known about how to support shared awareness in acceptability in social settings such as meals. In order to understand attitudes about mobile phone use during shared mealtimes, we conducted an online survey with 1,163 English-speaking participants. We find that attitudes about mobile phone use at meals differ depending on the particular phone activity and on who at the meal is engaged in that activity, children versus adults. We also show that three major factors impact participants' attitudes: 1) their own mobile phone use; 2) their age; and 3) whether a child is present at the meal. We discuss the potential for incorporating social awareness features into mobile phone systems to ease tensions around conflicting mealtime behaviors and attitudes.","Attitudes; Mealtimes; Mobile phones; Norms; Social media","ACM Special Interest Group on Computer-Human Interaction (SIGCHI)","Association for Computing Machinery","34th Annual Conference on Human Factors in Computing Systems, CHI 2016","7 May 2016 through 12 May 2016",,121621,"Conference Paper","Scopus"
"CTViz: A tool for the visualization of transport in nanocomposites","Beach B., Brown J., Tarlton T., Derosa P.A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964391079&doi=10.1016%2fj.jmgm.2016.03.012&partnerID=40&md5=b4bee605972e5941026041c1cb1af7af",2016,"Journal of Molecular Graphics and Modelling","A visualization tool (CTViz) for charge transport processes in 3-D hybrid materials (nanocomposites) was developed, inspired by the need for a graphical application to assist in code debugging and data presentation of an existing in-house code. As the simulation code grew, troubleshooting problems grew increasingly difficult without an effective way to visualize 3-D samples and charge transport in those samples. CTViz is able to produce publication and presentation quality visuals of the simulation box, as well as static and animated visuals of the paths of individual carriers through the sample. CTViz was designed to provide a high degree of flexibility in the visualization of the data. A feature that characterizes this tool is the use of shade and transparency levels to highlight important details in the morphology or in the transport paths by hiding or dimming elements of little relevance to the current view. This is fundamental for the visualization of 3-D systems with complex structures. The code presented here provides these required capabilities, but has gone beyond the original design and could be used as is or easily adapted for the visualization of other particulate transport where transport occurs on discrete paths. Â© 2016 Elsevier Inc. All rights reserved.","Nanocomposites; Transport visualization; Visualization",,"Elsevier Inc.",,,,,"Article","Scopus"
"PESTO: Data integration for visualization and device control in the SmartCare project","Burns N.B., Sassaman P., Daniel K., Huber M., ZÃ¡ruba G.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966565222&doi=10.1109%2fPERCOMW.2016.7457137&partnerID=40&md5=7bfff4e38578162af183c26bd7c22949",2016,"2016 IEEE International Conference on Pervasive Computing and Communication Workshops, PerCom Workshops 2016","The SmartCare project is to design, develop, and evaluate an intelligent sensor-driven living environment for the elderly. The core objectives are to provide emergency detection, improve quality of life, extend independence for the elderly, and detect patterns of behavior that could suggest early signs of a physical or cognitive issue, all in an unobtrusiveness manner. This paper specifically focuses on the development of the infrastructure integration component: PESTO and two of its sub components: a 3D visualization of the resident's smart apartment (VISMA) and a system to provide everyday task assistance through the Z-Wave home automation technology (ZAPS). Â© 2016 IEEE.","3D modeling; eldercare; home automation; home healthcare; pervasive computing; sensor networks; smart technology; ubiquitous computing; virtual environment; z-wave",,"Institute of Electrical and Electronics Engineers Inc.","13th IEEE International Conference on Pervasive Computing and Communication Workshops, PerCom Workshops 2016","14 March 2016 through 18 March 2016",,121284,"Conference Paper","Scopus"
"A systematic mapping study on mining software repositories","De Farias M.A.F., ColaÃ§o M., Jr., MendonÃ§a M., Novais R., Da Silva Carvalho L.P., SpÃ­nola R.O.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975865303&doi=10.1145%2f2851613.2851786&partnerID=40&md5=d1b1fc4a5e42c4d56609c78a0b546e52",2016,"Proceedings of the ACM Symposium on Applied Computing","Background: Software repositories provide large amount of data encompassing software changes throughout its evolution. Those repositories can be effectively used to extract and analyze pertinent information and derive conclusions related to the software history or its current snapshot. Objective: This work aims to investigate recent studies on Mining Software Repositories (MSR) approaches collecting evidences about software analysis goals (purpose, focus, and object of analysis), data sources, evaluation methods, tools, and how the area is evolving. Method: A systematic mapping study was performed to identify and analyze research on mining software repositories by analyzing five editions of Working Conference on Mining Software Repositories - the main conference on this area. Results: MSR approaches have been used for many different goals, mainly for comprehension of defects, analysis of the contribution and behavior of developers, and software evolution comprehension. Besides, some gaps were identified with respect to their goals, focus, and data source type (e.g. lack of usage of comments to identify smells, refactoring, and issues of software quality). Regarding the evaluation method, our analysis pointed out to an extensive usage of some types of empirical evaluation. Conclusion: Studies of the MSR have focused on different goals, however there are still many research opportunities to be explored and issues associated with MSR that should be considered. Â© 2016 ACM.","Empirical software engineering; Mining software repository; Secondary study; Systematic mapping study","ACM Special Interest Group on Applied Computing (SIGAPP)","Association for Computing Machinery","31st Annual ACM Symposium on Applied Computing, SAC 2016","4 April 2016 through 8 April 2016",,121991,"Conference Paper","Scopus"
"Physical rehabilitation assessment based on smart training equipment and mobile APPs","Postolache O.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978035445&doi=10.1109%2fEHB.2015.7391530&partnerID=40&md5=5f6381183f4be851a427d712e53fc908",2016,"2015 E-Health and Bioengineering Conference, EHB 2015","The access to patient-related information during decision-making and the information provided by training equipment, in the present, are still limited that made the objective evaluation of physical rehabilitation effectiveness a difficult task. The paper focuses on smart training equipment, expressed by smart walkers and crutches, for physical therapy monitoring. The used interfaces for visualization of the signals delivered by the measurement channels embedded on the level of training equipment are expressed by mobile devices and appropriate software applications (APPs). The implemented software applications provide appropriate information for physiotherapist about the personal health record and metrics associated with the interaction between patient and the instrumented equipment during rehabilitation sessions. Novel sensing solutions expressed by piezo resistive force sensors, MEMS inertial sensors and Doppler radar sensors perform the user assessment during training are also described. The appropriate processing of the data provided by smart objects as part of smart physiotherapy ecosystem permits to extract information about the rehabilitation outcome. Â© 2015 IEEE.","force sensing; mobile applications; motion sensing; physiotherapy; wireless communication","Zepter International","Institute of Electrical and Electronics Engineers Inc.","5th IEEE International Conference on E-Health and Bioengineering, EHB 2015","19 November 2015 through 21 November 2015",,119150,"Conference Paper","Scopus"
"Open source as an innovative approach in computer science education A systematic review of advantages and challenges","Alasbali N., Benatallah B.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963826978&doi=10.1109%2fMITE.2015.7375330&partnerID=40&md5=c70343aa14ffe197163e33a992f06f36",2016,"Proceedings of the 2015 IEEE 3rd International Conference on MOOCs, Innovation and Technology in Education, MITE 2015","This paper presents the results of a systematic literature review (SLR) of the advantages and challenges of using open source (OS) in computer science education. The review highlights an innovative approach of requesting students of computer science to become active members of existing OS communities as part of their curriculum. The acquisition of a wide range of skills, increasing student motivation, support for contextual learning and student-centered courses as well as the availability of a wealth of data to inform and support decision-making by educators are the main advantages as identified by the SLR. Similarly, high barriers to entry in OS projects, difficulties related to student support, assessments, grouping of students and choice of an adequate OS project are the potential challenges. In this paper, the advantages and challenges of using OS in an existing framework of a course design and delivery have been discussed in detail. In particular, the potential impact of OS in computer science education curriculum has been explored from the perspective of education providers. Â© 2015 IEEE.","computer science; computer science education; ICT; ICT skills; open academy; Open source",,"Institute of Electrical and Electronics Engineers Inc.","3rd IEEE International Conference on MOOCs, Innovation and Technology in Education, MITE 2015","1 October 2015 through 2 October 2015",,118997,"Conference Paper","Scopus"
"Intelligent code completion with Bayesian networks","Proksch S., Lerch J., Mezini M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016266706&partnerID=40&md5=5cd8c9c0adc2f953ffaa2848765ae656",2016,"Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)","Code completion is an integral part of modern Integrated Development Environments (IDEs). Intelligent code completion systems can reduce long lists of type-correct proposals to relevant items. In this work, we replace an existing code completion engine named Best-Matching Neighbor (BMN) by an approach using Bayesian Networks named Pattern-based Bayesian Network (PBN).We use additional context information for more precise recommendations and apply clustering techniques to improve model sizes and to increase speed. We compare the new approach with the existing algorithm and, in addition to prediction quality, we also evaluate model size and inference speed. Our results show that the additional context information we collect improves prediction quality, and that PBN can obtain comparable prediction quality to BMN, while model size and inference speed scale better with large input sizes.","Evaluation; Machine learning; Recommender system; Static analysis",,"Gesellschaft fur Informatik (GI)","Software Engineering-Konferenz, SE 2016  - Software Engineering Conference, SE 2016","23 February 2016 through 26 February 2016",,126694,"Conference Paper","Scopus"
"Flow, intrinsic motivation, and developer experience in software engineering","Kuusinen K., Petrie H., Fagerholm F., Mikkonen T.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971497793&doi=10.1007%2f978-3-319-33515-5_9&partnerID=40&md5=9fdfba07f6340f8dd9adb263e328de07",2016,"Lecture Notes in Business Information Processing","Software developers are both users of development tools but also designers of new software systems. This dual role makes developers special users of work-related software. To increase the understanding of developers as users and to evaluate the ability of common measurement scales to address developer experience, we conducted a survey measuring developersâ€™ flow state, intrinsic motivation and user experience. Scales used were the Short Dispositional Flow Scale, items from the Intrinsic Motivation Inventory, the Short AttrakDiff-2, and our own DEXI scale. 57 developers from 25 countries responded and results indicate that intrinsic motivation and autotelic experience are significant predictors of developersâ€™ UX whereas hedonic, pragmatic, and general quality are not. In addition, developersâ€™ needs are characterized by efficiency, informativeness, intuitiveness, and flexibility of the tool. Â© The Author(s) 2016.","Developer experience; Development tools; Human factors; Integrated development environments; Software development; User experience",,"Springer Verlag","17th International Conference on Agile Processes in Software Engineering and Extreme Programming, XP 2016","24 May 2016 through 27 May 2016",,175529,"Conference Paper","Scopus"
"Automatic change recommendation of models and meta models based on change histories","KÃ¶gel S., Groner R., Tichy M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996799553&partnerID=40&md5=5adf5f9259609fe9fd4bedd87fc420f2",2016,"CEUR Workshop Proceedings","Model-driven software engineering uses models and meta models as key artefacts in the software development process. Typically, changes in the models (or meta models) do not come in isolation but are part of more complex change sets where a single change depends on other changes, e.g., a component is added to an architectural model and thereafter ports and connectors connect this component to other components. Furthermore, these sets of related and depending changes are often recurring, e.g., always when a component is added to an architecture, it is highly likely that ports are added to that component, too. This is similar for changes in meta models. Our goal is to help engineers by (1) automatically identifying clusters of related changes on model histories and (2) recommending corresponding changes after the engineer performs a single change. In this position paper, we present an initial technique to achieve our goal. We evaluate our technique with models from the Eclipse GMF project and present our recommendations as well as the recommendation quality. Our evaluation found an average precision between 0:43 and 0:82 for our recommendations.","Change recommendation; Model-driven development; Revision history mining",,"CEUR-WS","10th Workshop on Models and Evolution, ME 2016","2 October 2016",,124592,"Conference Paper","Scopus"
"A change guide method based on developers' interaction and past recommendation","Yamamori A., Kobayashi T.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015873843&doi=10.2316%2fP.2016.835-012&partnerID=40&md5=5cfa9770f232aa6adbb0b7562c15af66",2016,"Proceedings of the 13th IASTED International Conference on Parallel and Distributed Computing and Networks, PDCN 2016","In this paper, we propose a change guide method based on the past developers' activity that consists of read and write access records of artifacts. In our proposed method, we calculate candidates of next change recommendation considering the history of its recommendations. We define ""cumulative likelihood"" to enable the method to recommend the appropriate candidates when a change propagates more than one code elements. A case study using interaction history logs from 15 participants showed the improvement of the accuracy of the method-level change recommendation.","Change guide; Interaction history; Software maintenance; Software repository mining",,"Acta Press","13th IASTED International Conference on Parallel and Distributed Computing and Networks, PDCN 2016","15 February 2016 through 16 February 2016",,126242,"Conference Paper","Scopus"
"Design of CQA systems for flexible and scalable deployment and evaluation","Srba I., Bielikova M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977495635&doi=10.1007%2f978-3-319-38791-8_30&partnerID=40&md5=23744a1c6cda80af652f3594e0a644d4",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","Successfulness of Community Question Answering (CQA) systems on the open web (e.g. Yahoo! Answers) motivated for their utilization in new contexts (e.g. education or enterprise) and environments (e.g. inside organizations). In spite of initial research how their specifics influence design of CQA systems, many additional problems have not been addressed so far. Especially a poor flexibility and scalability which hamper: (1) CQA essential features to be employed in various settings (e.g. in different educational organizations); and (2) collaboration support methods to be effectively evaluated (e.g. in offline as well as in live experiments). In this paper, we provide design recommendations how to achieve flexible and scalable deployment and evaluation by means of a case study on educational and organizational CQA system Askalot. Its universal and configurable features allow us to deploy it at two universities as well as in MOOC system edX. In addition, by means of its experimental infrastructure, we can integrate various collaboration support methods which are loosely coupled and can be easily evaluated online as well as offline with datasets from Askalot itself or even from all CQA systems built on the top of the Stack Exchange platform. Â© Springer International Publishing Switzerland 2016.","Askalot; CQA; Flexibility; Scalability; System design","Etal;Google;innoQ;lastminute.comgroup;Nokia;Springer","Springer Verlag","16th International Conference on Web Engineering, ICWE 2016","6 June 2016 through 9 June 2016",,176609,"Conference Paper","Scopus"
"Service analytics for IT service management","Diao Y., Jan E., Li Y., Rosu D., Sailer A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978959434&doi=10.1147%2fJRD.2016.2520620&partnerID=40&md5=77c3ba810563cea6826f3dafcaf99776",2016,"IBM Journal of Research and Development","Outsourcing enterprise IT service management is an increasingly challenging business. On one hand, service providers must deliver with respect to customer expectations of service quality and innovation. On the other hand, they must continuously seek competitive reductions in the costs of service delivery and management. These targets can be achieved with integration of innovative service management tools, automation, and advanced analytics. In this paper, we focus on service analytics, the subset of analytics problems and solutions concerning specific service delivery and management performance and cost optimization. The paper reviews various service analytics methods and technologies that have been developed and applied to enhance IT service management. We use our industrial experience to highlight the challenges faced in the development and adoption of service analytics, and we discuss open problems. Â© 1957-2012 IBM.",,,"IBM Corporation",,,,,"Article","Scopus"
"Augmented live communication workspace platform to assist and utilize cognitive abilities of senior workers","Kosugi A., Nishiguchi S., Izumi M., Kobayashi M., Hiyama A., Hirose M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978818539&doi=10.1007%2f978-3-319-40238-3_37&partnerID=40&md5=36c62c4f67cd7f38dfeea16ff6ba774d",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","Live communication over long distances is indispensable for seniors, and tools have evolved to improve the sense of presence: sight is added by video and robots represent bodies at remote locations. Emerging technologies that assist cognitive abilities may improve the communication quality beyond reality. However, they have been independently developed and cannot be integrated easily. This results in not only raising the development cost but also hampering new technology being installed in this area. Therefore, we propose a platform for remote and live communication that supports portable and fast data transfer connections as fundamental functions and possesses a plug-in framework that enables features to be extended dynamically on the basis of a common interface. In this paper, we explain the design of this platform and describe some plug-in based applications and scenarios built on it as examples validating the concept. Â© Springer International Publishing Switzerland 2016.","Accessibility; Agent conversations; Augmented reality; Cognitive assistance; Physical devices; Plug-ins; Remote communication; Senior workforce; Tele-presence robot; WebRTC",,"Springer Verlag","10th International Conference on Universal Access in Human-Computer Interaction, UAHCI 2016 and Held as Part of 18th International Conference on Human-Computer Interaction, HCI International 2016","17 July 2016 through 22 July 2016",,177929,"Conference Paper","Scopus"
"Rain streak removal using layer priors","Li Y., Tan R.T., Guo X., Lu J., Brown M.S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986247490&partnerID=40&md5=6d69345604a7586585e9f35b9579da86",2016,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","This paper addresses the problem of rain streak removal from a single image. Rain streaks impair visibility of an image and introduce undesirable interference that can severely affect the performance of computer vision algorithms. Rain streak removal can be formulated as a layer decomposition problem, with a rain streak layer superimposed on a background layer containing the true scene content. Existing decomposition methods that address this problem employ either dictionary learning methods or impose a low rank structure on the appearance of the rain streaks. While these methods can improve the overall visibility, they tend to leave too many rain streaks in the background image or over-smooth the background image. In this paper, we propose an effective method that uses simple patch-based priors for both the background and rain layers. These priors are based on Gaussian mixture models and can accommodate multiple orientations and scales of the rain streaks. This simple approach removes rain streaks better than the existing methods qualitatively and quantitatively. We overview our method and demonstrate its effectiveness over prior work on a number of examples.",,,"IEEE Computer Society","2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016","26 June 2016 through 1 July 2016",,123331,"Conference Paper","Scopus"
"Exploring the usability and effectiveness of interactive annotation and code review for the detection of security vulnerabilities","Thomas T.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959896602&doi=10.1109%2fVLHCC.2015.7357234&partnerID=40&md5=de6d8d68f4a79296e6ab456a3922eb18",2015,"Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC","According to a recent IBM study, the average cost for a stolen record raised 9% to $145 in 2014 [1]. Since millions of credit card records are stolen every year, the cost can easily run into billions of dollars. Consequently, application security is a very important concern during the development of applications today. Resolving security problems later in the development process is very time consuming and expensive. Therefore, it is favorable to detect and resolve security vulnerabilities as soon as possible during the development process. Â© 2015 IEEE.",,"Clemson University;IEEE Computer Society;NSF","IEEE Computer Society","IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC 2015","18 October 2015 through 22 October 2015",,118707,"Conference Paper","Scopus"
"A study of interactive code annotation for access control vulnerabilities","Thomas T., Chu B., Lipford H., Smith J., Murphy-Hill E.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959891339&doi=10.1109%2fVLHCC.2015.7357200&partnerID=40&md5=4b417b02769feaf5476d53f913df07e3",2015,"Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC","While there are a variety of existing tools to help detect security vulnerabilities in code, they are seldom used by developers due to the time or security expertise required. We are investigating techniques integrated within the IDE to help developers detect and mitigate security vulnerabilities. In this paper, we examine using interactive annotation for access control vulnerabilities. We evaluated whether developers could indicate access control logic using interactive annotation and understand the vulnerabilities reported as a result. Our study indicates that developers can easily find and annotate access control logic but can struggle to use our tool to trace the cause of the vulnerability. Our results provide design guidance for improving the interaction and communication of such security tools with developers. Â© 2015 IEEE.",,"Clemson University;IEEE Computer Society;NSF","IEEE Computer Society","IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC 2015","18 October 2015 through 22 October 2015",,118707,"Conference Paper","Scopus"
"Multi-class weather classification on single images","Zhang Z., Ma H.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956597509&doi=10.1109%2fICIP.2015.7351637&partnerID=40&md5=e8e3f579e30f0c9d9d390b12695d16b9",2015,"Proceedings - International Conference on Image Processing, ICIP","Multi-class weather classification from single images is a fundamental operation in many outdoor computer vision applications. However, it remains difficult and the limited work is carried out for addressing the difficulty. Moreover, existing method is based on the fixed scene. In this paper we present a method for any scenario multi-class weather classification based on multiple weather features and multiple kernel learning. Our approach extracts multiple weather features and takes properly processing. By combining these features into high dimensional vectors, we utilize multiple kernel learning to learn an adaptive classifier. We collect an outdoor image set that contains 20K images called MWI (Multi-class Weather Image) set. Experimental results show that the proposed method can efficiently recognize weather on MWI dataset. Â© 2015 IEEE.","Multi-class weather classification; multiple kernel learning; multiple weather features","The Institute of Electrical and Electronics Engineers on Signal Processing Society","IEEE Computer Society","IEEE International Conference on Image Processing, ICIP 2015","27 September 2015 through 30 September 2015",,117806,"Conference Paper","Scopus"
"Automatically augmenting learning material with practical questions to increase its relevance","Singh G.K., Kumar V., Bhat S., Pedanekar N.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960435526&doi=10.1109%2fFIE.2015.7344369&partnerID=40&md5=f9bf24808115eac98398a741dc70d97e",2015,"Proceedings - Frontiers in Education Conference, FIE","Relevance of a concept being taught to the real world is believed to contribute to an increase in the intrinsic motivation and engagement of a learner. Such relevance is often found lacking in learning material such as textbooks. Practical issues and problems one could face while learning or implementing new concepts are a means of establishing such relevance. In this paper, we propose a method to automatically augment learning material with practical questions about the concept being learnt. We use questions and answers from StackOverflow, a leading social Questions and Answers (Q&A) website to augment an electronic textbook interface, thus connecting the concepts being taught to the real world. For achieving this automatically, we first mine the textbook content to locate words and phrases which are likely to be the most important concepts on each page of the textbook. We then select only those words and phrases which appear as 'tags' in StackOverflow, typically defined by users while asking and answering questions. Using permutations of these tags as queries, we query the StackOverflow database to obtain relevant questions and answers to augment any given page. We present an interface to augment textbooks with such questions using the aforementioned method. We also present the results of a student survey examining the effectiveness of the augmentation in establishing relevance to their learning. Â© 2015 IEEE.","Augmentation; Computer Science Education; e-learning; Questions; Relevance; StackOverflow","ASEE Educational Research and Methods Division;IEEE Computer Society;IEEE Education Society;New Mexico State University;University of Texas","Institute of Electrical and Electronics Engineers Inc.","2015 IEEE Frontiers in Education Conference, FIE 2015","21 October 2015 through 24 October 2015",,118740,"Conference Paper","Scopus"
"Automated formation of the interactive tasks for the computer-aided training","Dukkardt A.N., Kuliev E.V., Kureichik V.V., Novikov A.A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960869224&doi=10.1109%2fICAICT.2015.7338632&partnerID=40&md5=8ac4dee2585316fdec457c2f9f9f140d",2015,"9th International Conference on Application of Information and Communication Technologies, AICT 2015 - Proceedings","The automated formation of the interactive task for the computer aided training means is studied in the article. This problem is of current importance since there is a necessity to create an instrumental tool that would give an opportunity to realize different types of the testing and training tasks on basis of the interactive technologies in the modern stage of the computer training system development. The instrumental tool aimed at formation of the dynamic tasks and assisting in the development of the interactive work of a trainee with the system is suggested in this work. Â© 2015 IEEE.","an automated system; an information system; interactive tasks; interactivity; training tasks",,"Institute of Electrical and Electronics Engineers Inc.","9th International Conference on Application of Information and Communication Technologies, AICT 2015","14 October 2015 through 16 October 2015",,118382,"Conference Paper","Scopus"
"Recommending insightful comments for source code using crowdsourced knowledge","Rahman M.M., Roy C.K., Keivanloo I.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963620738&doi=10.1109%2fSCAM.2015.7335404&partnerID=40&md5=d65c55e1df5a9a3ba5962a677e1e708b",2015,"2015 IEEE 15th International Working Conference on Source Code Analysis and Manipulation, SCAM 2015 - Proceedings","Recently, automatic code comment generation is proposed to facilitate program comprehension. Existing code comment generation techniques focus on describing the functionality of the source code. However, there are other aspects such as insights about quality or issues of the code, which are overlooked by earlier approaches. In this paper, we describe a mining approach that recommends insightful comments about the quality, deficiencies or scopes for further improvement of the source code. First, we conduct an exploratory study that motivates crowdsourced knowledge from Stack Overflow discussions as a potential resource for source code comment recommendation. Second, based on the findings from the exploratory study, we propose a heuristic-based technique for mining insightful comments from Stack Overflow Q & A site for source code comment recommendation. Experiments with 292 Stack Overflow code segments and 5,039 discussion comments show that our approach has a promising recall of 85.42%. We also conducted a complementary user study which confirms the accuracy and usefulness of the recommended comments. Â© 2015 IEEE.","code examples; code insight; comment recommendation; program analysis; Stack Overflow","Grammatech;IEEE Computer Society TCSE;Microsoft Research","Institute of Electrical and Electronics Engineers Inc.","IEEE 15th International Working Conference on Source Code Analysis and Manipulation, SCAM 2015","27 September 2015 through 28 September 2015",,118260,"Conference Paper","Scopus"
"Identifying wasted effort in the field via developer interaction data","Balogh G., Antal G., Beszedes A., Vidacs L., Gyimothy T., Vegh A.Z.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961675304&doi=10.1109%2fICSM.2015.7332490&partnerID=40&md5=666bbdafd32164061de69ce1de8a7b9e",2015,"2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings","During software projects, several parts of the source code are usually re-written due to imperfect solutions before the code is released. This wasted effort is of central interest to the project management to assure on-time delivery. Although the amount of thrown-away code can be measured from version control systems, stakeholders are more interested in productivity dynamics that reflect the constant change in a software project. In this paper we present a field study of measuring the productivity of a medium-sized J2EE project. We propose a productivity analysis method where productivity is expressed through dynamic profiles- the so-called Micro-Productivity Profiles (MPPs). They can be used to characterize various constituents of software projects such as components, phases and teams. We collected detailed traces of developers' actions using an Eclipse IDE plug-in for seven months of software development throughout two milestones. We present and evaluate profiles of two important axes of the development process: by milestone and by application layers. MPPs can be an aid to take project control actions and help in planning future projects. Based on the experiments, project stakeholders identified several points to improve the development process. It is also acknowledged, that profiles show additional information compared to a naive diff-based approach. Â© 2015 IEEE.","Productivity; Project management; Security; Software; Software measurement; Stakeholders; User interfaces","IEEE Computer Society TCSE","Institute of Electrical and Electronics Engineers Inc.","31st IEEE International Conference on Software Maintenance and Evolution, ICSME 2015","29 September 2015 through 1 October 2015",,118301,"Conference Paper","Scopus"
"SODA: The stack overflow dataset almanac","Latorre N., Minelli R., Mocci A., Ponzanelli L., Lanza M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962251947&doi=10.1109%2fMUD.2015.7327961&partnerID=40&md5=65a4cc0bc4b4298f34818281d1555639",2015,"2015 IEEE 5th Workshop on Mining Unstructured Data, MUD 2015 - Proceedings","Stack Overflow has become a fundamental resource for developers, becoming the de facto Question and Answer (Q&A) website, and one of the standard unstructured data sources for software engineering research to mine knowledge about development. We present SODA, the Stack Overflow Dataset Almanac, a tool that helps researchers and developers to better understand the trends of discussion topics in Stack Overflow, based on the available tagging system. SODA provides an effective visualization to support the analysis of topics in different time intervals and frames, leveraging single or co-occurrent tags. We show, through simple usage scenarios, how SODA can be used to find interesting peculiar moments in the evolution of Stack Overflow discussions that closely match specific recent events in the area of software development. SODA is available at http://rio.inf.usi.ch/soda/ Â© 2015 IEEE.",,"IEEE Computer Society TCSE","Institute of Electrical and Electronics Engineers Inc.","5th IEEE Workshop on Mining Unstructured Data, MUD 2015","28 September 2015",,118605,"Conference Paper","Scopus"
"Noises in Interaction Traces Data and Their Impact on Previous Research Studies","Soh Z., Drioul T., Rappe P.-A., Khomh F., GuÃ©hÃ©neuc Y.-G., Habra N.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961612939&doi=10.1109%2fESEM.2015.7321209&partnerID=40&md5=139e764d814be6dab44950388cb91dbc",2015,"International Symposium on Empirical Software Engineering and Measurement","Context: Developers' interaction traces (ITs) are commonly used in software engineering to understand how developers maintain and evolve software systems. Researchers make several assumptions when mining ITs, e.g., edit events are considered to be change activities and the time mined from ITs is considered to be the time spent by the developers performing the maintenance task. Goal: We investigate the extent to which these assumptions are correct. We examine noises in developers'''' ITs data and the impact of these noises on previous results derived from these traces. Approach: We perform an experiment with 15 participants, whom we asked to perform bug-fixing activities and collect Mylyn ITs and VLC video captures. We then investigate noises between the two data sets and propose an approach to correct noises in ITs. Results: We find that Mylyn ITs can miss on average about 6% of the time spent performing a task and contain on average about 28% of false edit-events. We report that these noises may have led researchers to mislabel some participants'''' editing styles in about 34% of the cases and that the numbers of edit-events performed by developers and the times that they spent on tasks are correlated, when they were considered not to be. Conclusion: We show that ITs must be carefully cleaned before being used in research studies. Â© 2015 IEEE.","interaction traces; maintenance effort; noises; Software maintenance; video captures",,"IEEE Computer Society","ACM/IEEE International Symposium on Empirical Software Engineering and Measurement, ESEM 2015","22 October 2015 through 23 October 2015",,118866,"Conference Paper","Scopus"
"A tool for automatic formal modeling of railway interlocking systems","Ali Nur Oz M., Sener I., Turay Kaymakci O., Ustoglu I., Cansever G.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961711163&doi=10.1109%2fEUROCON.2015.7313752&partnerID=40&md5=05caf8610df6d793fc9c1b493b91dbad",2015,"Proceedings - EUROCON 2015","This paper introduces a new software tool, which can be used for automatic generation of Timed Arc Petri Net (TAPN) models from the railway station topology for interlocking systems. The introduced software tool has two components, 'Graphical User Interface' to draw the station topology and 'Application Software' to generate TAPN models from the station topology. TAPN is a highly recommended formal modeling method by the CENELEC EN50128 standard. Generated models, belonging to the station, are stored in an XML file and can be viewed using TAPAAL. Â© 2015 IEEE.","Automatic model generation; Interlocking system; Railway; Software tool; Timed arc Petri net",,"Institute of Electrical and Electronics Engineers Inc.","International Conference on Computer as a Tool, IEEE EUROCON 2015","8 September 2015 through 11 September 2015",,118530,"Conference Paper","Scopus"
"FlierMeet: A Mobile Crowdsensing System for Cross-Space Public Information Reposting, Tagging, and Sharing","Guo B., Chen H., Yu Z., Xie X., Huangfu S., Zhang D.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928574717&doi=10.1109%2fTMC.2014.2385097&partnerID=40&md5=b9e0fd7f6dffc84623144545589762bd",2015,"IEEE Transactions on Mobile Computing","Community bulletin boards serve an important function for public information sharing in modern society. Posted fliers advertise services, events, and other announcements. However, fliers posted offline suffer from problems such as limited spatial-temporal coverage and inefficient search support. In recent years, with the development of sensor-enhanced mobile devices, mobile crowd sensing (MCS) has been used in a variety of application areas. This paper presents FlierMeet, a crowd- powered sensing system for cross-space public information reposting, tagging, and sharing. The tags learned are useful for flier sharing and preferred information retrieval and suggestion. Specifically, we utilize various contexts (e.g., spatio-temporal info, flier publishing/reposting behaviors, etc.) and textual features to group similar reposts and classify them into categories. We further identify a novel set of crowd-object interaction hints to predict the semantic tags of reposts. To evaluate our system, 38 participants were recruited and 2,035 reposts were captured during an eight-week period. Experiments on this dataset showed that our approach to flier grouping is effective and the proposed features are useful for flier category/semantic tagging. Â© 2002-2012 IEEE.","cross-space reposting; data grouping and selection; interaction-based semantic tagging; Participatory sensing; urban sensing",,"Institute of Electrical and Electronics Engineers Inc.",,,,,"Article","Scopus"
"Design Guidelines for Business Intelligence Tools for Novice Users","Smuts M., Scholtz B., Calitz A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959431827&doi=10.1145%2f2815782.2815788&partnerID=40&md5=f2fc28815d0aff703770f63999be4d67",2015,"ACM International Conference Proceeding Series","The use of interactive dashboards has become a popular technique to aid users in Business Intelligence (BI) analysis and data discovery. The increase in the number of BI platforms on the market is driven by the expanding end-user population. A wider range of novice users, such as business users with minimal Information Technology (IT) or data science skills, are demanding BI tools that support rapid and easy dashboard development. Dashboard development is often a tedious process, involving a number of developers and software tools. Self-service BI tools are becoming prominent environments in which novice users can fulfil their BI requirements without the intervention of IT experts. However, the usability of BI tools has not fully matured to a level where novice users can utilise its features efficiently and effectively without the assistance from IT experts. Limited research has been conducted regarding usability criteria specific to BI tools that support novice users. The purpose of this paper is to expand on existing BI usability criteria for supporting novice users with their data analysis activities. Furthermore, the study proposes a set of design guidelines that can be used as a reference for designing, evaluating and selecting BI tools that aid novice users. Evaluations were carried out on current BI tools to investigate its usability and the extent to which these tools follow the proposed guidelines. Additionally, a field study was conducted with novice users to evaluate the difficulties of current BI tools. This study is concerned with the design of front-end features and usability of BI tools and not on the design of dashboards itself. The results indicated that the proposed design guidelines can be effectively used to select a BI tool for novices. Â© 2015 ACM.","Business intelligence; Dashboards; Design guidelines; Information Visualisation; Novice users","Amazon Development Centre Cape Town;IBM South Africa;Information Science Department and the Centre for AI Research at Stellenbosch University;Meraka Institute of the Council for Scientific and Industrial Research (CSIR);Microsoft South Africa;OutSystems South Africa","Association for Computing Machinery","2015 Annual Research Conference of the South African Institute of Computer Scientists and Information Technologists, SAICSIT 2015","28 September 2015 through 30 September 2015",,118802,"Conference Paper","Scopus"
"Identifying Renaming Opportunities by Expanding Conducted Rename Refactorings","Liu H., Liu Q., Liu Y., Wang Z.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942323643&doi=10.1109%2fTSE.2015.2427831&partnerID=40&md5=0f60862fcefb27520a2ca7a172b93895",2015,"IEEE Transactions on Software Engineering","To facilitate software refactoring, a number of approaches and tools have been proposed to suggest where refactorings should be conducted. However, identification of such refactoring opportunities is usually difficult because it often involves difficult semantic analysis and it is often influenced by many factors besides source code. For example, whether a software entity should be renamed depends on the meaning of its original name (natural language understanding), the semantics of the entity (source code semantics), experience and preference of developers, and culture of companies. As a result, it is difficult to identify renaming opportunities. To this end, in this paper we propose an approach to identify renaming opportunities by expanding conducted renamings. Once a rename refactoring is conducted manually or with tool support, the proposed approach recommends to rename closely related software entities whose names are similar to that of the renamed entity. The rationale is that if an engineer makes a mistake in naming a software entity it is likely for her to make the same mistake in naming similar and closely related software entities. The main advantage of the proposed approach is that it does not involve difficult semantic analysis of source code or complex natural language understanding. Another advantage of this approach is that it is less influenced by subjective factors, e.g., experience and preference of software engineers. The proposed approach has been evaluated on four open-source applications. Our evaluation results show that the proposed approach is accurate in recommending entities to be renamed (average precision 82 percent) and in recommending new names for such entities (average precision 93 percent). Evaluation results also suggest that a substantial percentage (varying from 20 to 23 percent) of rename refactorings are expansible. Â© 2015 IEEE.","Code Smells; Identification; Refactoring Opportunity; Rename; Software Refactoring",,"Institute of Electrical and Electronics Engineers Inc.",,,,,"Article","Scopus"
"Implementation of Multimodal Interactive Continuous Scoring for 3D Quality of Experience","Kang J., Kim T., Lee S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957428200&doi=10.1007%2fs11277-015-2680-z&partnerID=40&md5=762ca32c86c39ad5818a5fc2a46d3618",2015,"Wireless Personal Communications","When watching videos in 3D space, viewers perceive dynamic levels of quality of experience accompanied by visual immersion. To measure these dynamics, a reliable methodology is needed to gauge subjective viewer experience. This paper proposes a new methodology called multimodal interactive continuous scoring of quality (MICSQ). MICSQ is comprised of device interaction processes between the 3D display and tablet assessment tool and human interaction processes between the subject and tablet. When MICSQ device interaction takes place over wireless network protocols, such as TCP/IP or Bluetooth, it efficiently handles the diverse viewing environment. Therefore, there is a high degree of freedom to perform subjective assessment in certain viewing environments in terms of multimodal cues (aural and tactile senses), diverse illumination conditions including darkness, handheld portability over wireless networks, and real-time recording. Moreover, it is also possible for multiple subjects to simultaneously perform assessments in a large space, such as a movie theater. For the simulations, the server application in the 3D display was developed in Java, and the tablet device client application was developed with a mobile software development kit and functions optimally in commercial tablets. The experimental results demonstrate that MICSQ shows a higher reliability than the conventional single stimulus continuous quality evaluation method through the proposed implementation on a commercial tablet PC. Â© 2015, Springer Science+Business Media New York.","3D QoE; Interactive continuous quality evaluation; Mobile assessment device; Visual discomfort; Wireless quality assessment",,"Springer New York LLC",,,,,"Article","Scopus"
"Crowd debugging","Chen F., Kim S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960393115&doi=10.1145%2f2786805.2786819&partnerID=40&md5=b989d9a2a1cb36798ea196dae432e413",2015,"2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings","Research shows that, in general, many people turn to QA sites to solicit answers to their problems. We observe in Stack Overflow a huge number of recurring questions, 1,632,590, despite mechanisms having been put into place to prevent these recurring questions. Recurring questions imply developers are facing similar issues in their source code. However, limitations exist in the QA sites. Developers need to visit them frequently and/or should be familiar with all the content to take advantage of the crowd's knowledge. Due to the large and rapid growth of QA data, it is difficult, if not impossible for developers to catch up. To address these limitations, we propose mining the QA site, Stack Overflow, to leverage the huge mass of crowd knowledge to help developers debug their code. Our approach reveals 189 warnings and 171 (90.5%) of them are confirmed by developers from eight high-quality and well-maintained projects. Developers appreciate these findings because the crowd provides solutions and comprehensive explanations to the issues. We compared the confirmed bugs with three popular static analysis tools (FindBugs, JLint and PMD). Of the 171 bugs identified by our approach, only FindBugs detected six of them whereas JLint and PMD detected none. Â© 2015 ACM.","Crowd debugging; Crowd sourcing; Debugging","Association for Computing Machinery Special Interest Group on Software Engineering (ACM SIGSOFT)","Association for Computing Machinery, Inc","10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015","30 August 2015 through 4 September 2015",,118131,"Conference Paper","Scopus"
"Everything as a Service (XaaS) on the Cloud: Origins, Current and Future Trends","Duan Y., Fu G., Zhou N., Sun X., Narendra N.C., Hu B.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960153644&doi=10.1109%2fCLOUD.2015.88&partnerID=40&md5=b38f81c01ad5c94f48abdd3ab67fb335",2015,"Proceedings - 2015 IEEE 8th International Conference on Cloud Computing, CLOUD 2015","For several years now, scientists have been proposing numerous models for defining anything 'as a service (aaS)', including discussions of products, processes, data &amp; information management, and security as a service. In this paper, based on a thorough literature survey, we investigate the vast stream of the state of the art in Everything as a Service (XaaS). We then use this investigation to explore an integrated view of XaaS that will help propose approaches for migrating applications to the cloud and exposing them as services. Â© 2015 IEEE.","Anything as a Service; Cloud computing; Everything as a Service; SOA","IEEE Computer Society Technical Committee on Services Computing (TC-SVC);Services Society (SS)","Institute of Electrical and Electronics Engineers Inc.","8th IEEE International Conference on Cloud Computing, CLOUD 2015","27 June 2015 through 2 July 2015",,116940,"Conference Paper","Scopus"
"CodeAware: Sensor-Based Fine-Grained Monitoring and Management of Software Artifacts","Abreu R., Erdogmus H., Perez A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951805742&doi=10.1109%2fICSE.2015.192&partnerID=40&md5=7d9cf8c0544935b910f13d38347530e5",2015,"Proceedings - International Conference on Software Engineering","Current continuous integration (CI) tools, although extensible, can be limiting in terms of flexibility. In particular, artifact analysis capabilities available through plug in mechanisms are both coarse-grained and centralized. To address this limitation, this paper introduces a new paradigm, Code Aware, for distributed and fine-grained artifact analysis. Code Aware is an ecosystem inspired by sensor networks, consisting of monitors and actuators, aimed at improving code quality and team productivity. Code ware's vision entails (a) the ability to probe software artifacts of any granularity and localization, from variables to classes or files to entire systems, (b) the ability to perform both static and dynamic analyses on these artifacts, and (c) the ability to describe targeted remediation actions, for example to notify interested developers, through automated actuators. We provide motivational examples for the use of Code Aware that leverage current CI solutions, sketch the architecture of its underlying ecosystem, and outline research challenges. Â© 2015 IEEE.",,"Association for Computing Machinery Special Interest Group on Software Engineering (ACM SIGSOFT);IEEE Computer Society Technical Council on Software Engineering (TCSE)","IEEE Computer Society","37th IEEE/ACM International Conference on Software Engineering, ICSE 2015","16 May 2015 through 24 May 2015",,116104,"Conference Paper","Scopus"
"DIETs: Recommender Systems for Mobile API Developers","Beyer S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951770011&doi=10.1109%2fICSE.2015.278&partnerID=40&md5=6bfd3771d3ef5aaf08adda263ceb955a",2015,"Proceedings - International Conference on Software Engineering","The increasing number of posts related to mobile app development indicates unaddressed problems in the usage of mobile APIs. Arguing that these problems result from in- adequate documentation and shortcomings in the design and implementation of the APIs, the goal of this research is to develop and evaluate two developers' issues elimination tools (DIETs) for mobile API developers to diminish the problems of mobile applications (apps) development.After categorizing the problems, we investigate their causes, by exploring the relationships between the topics and trends of posts on Stack Overflow, the app developers' experience, the API and test code, and its changes. The results of these studies will be used to develop two DIETs that support API developers to improve the documentation, design, and implementation of their APIs. Â© 2015 IEEE.","Api; Mobile api developers; Recommender systems","Association for Computing Machinery Special Interest Group on Software Engineering (ACM SIGSOFT);IEEE Computer Society Technical Council on Software Engineering (TCSE)","IEEE Computer Society","37th IEEE/ACM International Conference on Software Engineering, ICSE 2015","16 May 2015 through 24 May 2015",,116104,"Conference Paper","Scopus"
"Borrowing from the crowd: A study of recombination in software design competitions","La Toza T.D., Chen M., Jiang L., Zhao M., Van Der Hoek A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951850707&doi=10.1109%2fICSE.2015.72&partnerID=40&md5=97a08a2de3da5d5d9c06e029dad0eabb",2015,"Proceedings - International Conference on Software Engineering","One form of crowdsourcing is the competition, which poses an open call for competing solutions. Commercial systems such as TopCoder have begun to explore the application of competitions to software development, but have important limitations diminishing the potential benefits drawn from the crowd. In particular, they employ a model of independent work that ignores the opportunity for designs to arise from the ideas of multiple designers. In this paper, we examine the potential for software design competitions to incorporate recombination, in which competing designers are given the designs of others and encouraged to use them to revise their own designs. To explore this, we conducted two software design competitions in which participants were asked to produce both an initial and a revised design, drawing on lessons learned from the crowd. We found that, in both competitions, all participants borrowed ideas and most improved the quality of their designs. Our findings demonstrate the potential benefits of recombination in software design and suggest several ways in which software design competitions can be improved. Â© 2015 IEEE.","Collaborative design; Collective intelligence; Crowdsourcing; Software design","Association for Computing Machinery Special Interest Group on Software Engineering (ACM SIGSOFT);IEEE Computer Society Technical Council on Software Engineering (TCSE)","IEEE Computer Society","37th IEEE/ACM International Conference on Software Engineering, ICSE 2015","16 May 2015 through 24 May 2015",,116104,"Conference Paper","Scopus"
"StORMeD: Stack overflow ready made data","Ponzanelli L., Mocci A., Lanza M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957065977&doi=10.1109%2fMSR.2015.67&partnerID=40&md5=b6f301ff96052026e10aa21e6e4a8316",2015,"IEEE International Working Conference on Mining Software Repositories","Stack Overflow is the de facto Question and Answer (Q&A) website for developers, and it has been used in many approaches by software engineering researchers to mine useful data. However, the contents of a Stack Overflow discussion are inherently heterogeneous, mixing natural language, source code, stack traces and configuration files in XML or JSON format. We constructed a full island grammar capable of modeling the set of 700,000 Stack Overflow discussions talking about Java, building a heterogeneous abstract syntax tree (H-AST) of each post (question, answer or comment) in a discussion. The resulting dataset models every Stack Overflow discussion, providing a full H-AST for each type of structured fragment (i.e., JSON, XML, Java, Stack traces), and complementing this information with a set of basic meta-information like term frequency to enable natural language analyses. Our dataset allows the end-user to perform combined analyses of the Stack Overflow by visiting the H-AST of a discussion. Â© 2015 IEEE.","H-ast; Island parsing; Unstructured data","Association for Computing Machinery Special Interest Group on Software Engineering (ACM SIGSOFT);IEEE Computer Society Technical Council on Software Engineering (TCSE)","IEEE Computer Society","12th Working Conference on Mining Software Repositories, MSR 2015","16 May 2015 through 17 May 2015",,117060,"Conference Paper","Scopus"
"Using developer-interaction trails to triage change requests","Zanjani M.B., Kagdi H., Bird C.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957103031&doi=10.1109%2fMSR.2015.16&partnerID=40&md5=faccde453e431e69aecab560dd970917",2015,"IEEE International Working Conference on Mining Software Repositories","The paper presents an approach, namely iHDev, to recommend developers who are most likely to implement incoming change requests. The basic premise of iHDev is that the developers who interacted with the source code relevant to a given change request are most likely to best assist with its resolution. A machine-learning technique is first used to locate source code entities relevant to the textual description of a given change request. Ihdev then mines interaction trails (i.e., Mylyn sessions) associated with these source code entities to recommend a ranked list of developers. Ihdev integrates the interaction trails in a unique way to perform its task, which was not investigated previously. An empirical study on open source systems Mylyn and Eclipse Project was conducted to assess the effectiveness of iHDev. A number of change requests were used in the evaluated bench-mark. Recall for top one to five recommended developers and Mean Reciprocal Rank (MRR) values are reported. Furthermore, a comparative study with two previous approaches that use commit histories and/or the source code authorship information for developer recommendation was performed. Results show that iHDev could provide a recall gain of up to 127.27% with equivalent or improved MRR values by up to 112.5%. Â© 2015 IEEE.","Computer bugs; Context; Data mining; History; Mathematical model; Software; XML","Association for Computing Machinery Special Interest Group on Software Engineering (ACM SIGSOFT);IEEE Computer Society Technical Council on Software Engineering (TCSE)","IEEE Computer Society","12th Working Conference on Mining Software Repositories, MSR 2015","16 May 2015 through 17 May 2015",,117060,"Conference Paper","Scopus"
"A cooking-step scheduling algorithm with guidance system for homemade cooking","Matsushima Y., Funabiki N.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938909620&doi=10.1587%2ftransinf.2015EDP7048&partnerID=40&md5=fa9e63084ebb2bf65d803f28996ef586",2015,"IEICE Transactions on Information and Systems","Homemade cooking plays a key role for a healthy and cost-efficient life. Unfortunately, preparing multiple dishes is generally time-consuming. In this paper, an algorithm is proposed to minimize the cooking time by scheduling the cooking-step of multiple dishes. The cooking procedure of a dish is divided into a sequence of six types of cooking-steps to consider the constraints in cooks and cooking utensils in a kitchen. A cooking model is presented to optimize the cooking-step schedule and estimate the cooking time for a given starting order of dishes under various constraints of cooks and utensils. Then, a high-quality schedule is sought by repeating the generation of a new order and the model application based on exhaustive search and simulated annealing. Our simulation results and cooking experiments confirm the effectiveness of our proposal. Copyright Â© 2015 The Institute of Electronics, Information and Communication Engineers.","Algorithm; Cooking model; Cooking-step scheduling; Exhaustive search; Homemade cooking; Simulated annealing",,"Maruzen Co., Ltd.",,,,,"Article","Scopus"
"Learning outcome achievement in non-traditional (virtual and remote) versus traditional (hands-on) laboratories: A review of the empirical research","Brinson J.R.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937113661&doi=10.1016%2fj.compedu.2015.07.003&partnerID=40&md5=c1c49acc78528c45abd2e8878c767675",2015,"Computers and Education","Abstract This review presents the first attempt to synthesize recent (post-2005) empirical studies that focus on directly comparing learning outcome achievement using traditional lab (TL; hands-on) and non-traditional lab (NTL; virtual and remote) participants as experimental groups. Findings suggest that most studies reviewed (n = 50, 89%) demonstrate student learning outcome achievement is equal or higher in NTL versus TL across all learning outcome categories (knowledge and understanding, inquiry skills, practical skills, perception, analytical skills, and social and scientific communication), though the majority of studies (n = 53, 95%) focused on outcomes related to content knowledge, with most studies (n = 40, 71%) employing quizzes and tests as the assessment instrument. Scientific inquiry skills was the least assessed learning objective (n = 4, 7%), and lab reports/written assignments (n = 5, 9%) and practical exams (n = 5, 9%) were the least common assessment instrument. The results of this review raise several important concerns and questions to be addressed by future research. Â© 2015 Elsevier Ltd.","Distance education and telelearning; Distributed learning environments; Evaluation of CAL systems; Simulations; Teaching/learning strategies",,"Elsevier Ltd",,,,,"Article","Scopus"
"Preventing data errors with continuous testing","MuÅŸlu K., Brun Y., Meliou A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975704360&doi=10.1145%2f2771783.2771792&partnerID=40&md5=1263d390cbde21ddf60fc4628abd06e7",2015,"2015 International Symposium on Software Testing and Analysis, ISSTA 2015 - Proceedings","Today, software systems that rely on data are ubiquitous, and ensuring the data's quality is an increasingly important challenge as data errors result in annual multi-billion dollar losses. While software debugging and testing have received heavy research attention, less effort has been devoted to data debugging: identifying system errors caused by well-formed but incorrect data. We present continuous data testing (CDT), a low-overhead, delay-free technique that quickly identifies likely data errors. CDT continuously executes domain-specific test queries; when a test fails, CDT unobtrusively warns the user or administrator. We implement CDT in the CONTEST prototype for the PostgreSQL database management system. A feasibility user study with 96 humans shows that CONTEST was extremely effective in a setting with a data entry application at guarding against data errors: With CONTEST, users corrected 98.4% of their errors, as opposed to 40.2% without, even when we injected 40% false positives into CONTEST's output. Further, when using CONTEST, users corrected data entry errors 3.2 times faster than when using state-of-the-art methods. Copyright is held by the owner/author(s).","Continuous testing; Data debugging; Data testing","ACM SIGSOFT;et al.;Fujitsu;Huawei;IBM;Samsung","Association for Computing Machinery, Inc","24th International Symposium on Software Testing and Analysis, ISSTA 2015","13 July 2015 through 17 July 2015",,117924,"Conference Paper","Scopus"
"Educational data mining and learning analytics in programming: Literature review and case studies","Ihantola P., Vihavainen A., Ahadi A., Butler M., BÃ¶rstler J., Edwards S.H., Isohanni E., Korhonen A., Petersen A., Rivers K., Rubio M.A., Sheard J., Skupas B., Spacco J., Szabo C., Toll D.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964789171&doi=10.1145%2f2858796.2858798&partnerID=40&md5=cb19b5b750219ebf41358f3a7b486273",2015,"ITiCSE-WGP 2015 - Proceedings of the 2015 ITiCSE Conference on Working Group Reports","Educational data mining and learning analytics promise better understanding of student behavior and knowledge, as well as new information on the tacit factors that contribute to student actions. This knowledge can be used to inform decisions related to course and tool design and pedagogy, and to further engage students and guide those at risk of failure. This working group report provides an overview of the body of knowledge regarding the use of educational data mining and learning analytics focused on the teaching and learning of programming. In a literature survey on mining students' programming processes for 2005-2015, we observe a significant increase in work related to the field. However, the majority of the studies focus on simplistic metric analysis and are conducted within a single institution and a single course. This indicates the existence of further avenues of research and a critical need for validation and replication to better understand the various contributing factors and the reasons why certain results occur. We introduce a novel taxonomy to analyse replicating studies and discuss the importance of replicating and reproducing previous work. We describe what is the state of the art in collecting and sharing programming data. To better understand the challenges involved in replicating or reproducing existing studies, we report our experiences from three case studies using programming data. Finally, we present a discussion of future directions for the education and research community. Â© 2015 ACM.","Educational data mining; Learning analytics; Literature review; Programming; Replication","ACM SIGCSE","Association for Computing Machinery, Inc","2015 Innovation and Technology in Computer Science Education Conference, ITiCSE-WGP 2015","4 July 2015 through 8 July 2015",,119356,"Conference Paper","Scopus"
"Automatic detection and resolution of lexical ambiguity in process models","Pittke F., Leopold H., Mendling J.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933047249&doi=10.1109%2fTSE.2015.2396895&partnerID=40&md5=e9f10f68105788701e14f76fc4f4b092",2015,"IEEE Transactions on Software Engineering","System-related engineering tasks are often conducted using process models. In this context, it is essential that these models do not contain structural or terminological inconsistencies. To this end, several automatic analysis techniques have been proposed to support quality assurance. While formal properties of control flow can be checked in an automated fashion, there is a lack of techniques addressing textual quality. More specifically, there is currently no technique available for handling the issue of lexical ambiguity caused by homonyms and synonyms. In this paper, we address this research gap and propose a technique that detects and resolves lexical ambiguities in process models. We evaluate the technique using three process model collections from practice varying in size, domain, and degree of standardization. The evaluation demonstrates that the technique significantly reduces the level of lexical ambiguity and that meaningful candidates are proposed for resolving ambiguity. Â© 2015 IEEE.","Business Process Models; Identification of Lexical Ambiguity; Resolution of Lexical Ambiguity",,"Institute of Electrical and Electronics Engineers Inc.",,,,,"Article","Scopus"
"Use of politeness strategies in signed open peer review","Nobarany S., Booth K.S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948140545&doi=10.1002%2fasi.23229&partnerID=40&md5=56ec5c1109341bf9040483d54e975e2f",2015,"Journal of the Association for Information Science and Technology","Scholarly peer review is a complex collaborative activity that is increasingly supported by web-based systems, yet little is known about how reviewers and authors interact in such environments, how criticisms are conveyed, or how the systems may affect the interactions and use of language of reviewers and authors. We looked at one aspect of the interactions between reviewers and authors, the use of politeness in reviewers' comments. Drawing on Brown and Levinson's (1987) politeness theory, we analyzed how politeness strategies were employed by reviewers to mitigate their criticisms in an open peer-review process of a special track of a human-computer interaction conference. We found evidence of frequent use of politeness strategies and that open peer-review processes hold unique challenges and opportunities for using politeness strategies. Our findings revealed that less experienced researchers tended to express unmitigated criticism more often than did experienced researchers, and that reviewers tended to use more positive politeness strategies (e.g., compliments) toward less experienced authors. Based on our findings, we discuss implications for research communities and the design of peer-reviewing processes and the information systems that support them. Â© 2014 ASIS&T.","feedback; human computer interaction; scholars",,"John Wiley and Sons Inc.",,,,,"Article","Scopus"
"The impact of API change- and fault-proneness on the user ratings of android apps","Bavota G., Linares-VÃ¡squez M., Bernal-CÃ¡rdenas C.E., Di Penta M., Oliveto R., Poshyvanyk D.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928107075&doi=10.1109%2fTSE.2014.2367027&partnerID=40&md5=09996e17c2d1dbc4512b2a48a8068872",2015,"IEEE Transactions on Software Engineering","The mobile apps market is one of the fastest growing areas in the information technology. In digging their market share, developers must pay attention to building robust and reliable apps. In fact, users easily get frustrated by repeated failures, crashes, and other bugs; hence, they abandon some apps in favor of their competition. In this paper we investigate how the fault- and change-proneness of APIs used by Android apps relates to their success estimated as the average rating provided by the users to those apps. First, in a study conducted on 5,848 (free) apps, we analyzed how the ratings that an app had received correlated with the fault- and change-proneness of the APIs such app relied upon. After that, we surveyed 45 professional Android developers to assess (i) to what extent developers experienced problems when using APIs, and (ii) how much they felt these problems could be the cause for unfavorable user ratings. The results of our studies indicate that apps having high user ratings use APIs that are less fault- and change-prone than the APIs used by low rated apps. Also, most of the interviewed Android developers observed, in their development experience, a direct relationship between problems experienced with the adopted APIs and the users' ratings that their apps received. Â© 1976-2012 IEEE.","Android; API changes; Empirical Studies; Mining Software Repositories",,"Institute of Electrical and Electronics Engineers Inc.",,,,,"Article","Scopus"
"A recommendation system for repairing violations detected by static architecture conformance checking","Terra R., Valente M.T., Czarnecki K., Bigonha R.S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920884159&doi=10.1002%2fspe.2228&partnerID=40&md5=c23a29e6ba9c4b20f26abc3d65e973b7",2015,"Software - Practice and Experience","This paper describes a recommendation system that provides refactoring guidelines for maintainers when tackling architectural erosion. The paper formalizes 32 refactoring recommendations to repair violations raised by static architecture conformance checking approaches; it describes a tool - called ArchFix - that triggers the proposed recommendations; and it evaluates the application of this tool in two industrial-strength systems. For the first system - a 21 KLOC open-source strategic management system - our approach has indicated correct refactoring recommendations for 31 out of 41 violations detected as the result of an architecture conformance process. For the second system - a 728 KLOC customer care system used by a major telecommunication company - our approach has triggered correct recommendations for 624 out of 787 violations, as asserted by the system's architect. Moreover, the architects have scored 82% of these recommendations as having moderate or major complexity. Copyright Â© 2013 John Wiley & Sons, Ltd.","Recommendation system; Refactoring; Software architecture",,"John Wiley and Sons Ltd",,,,,"Article","Scopus"
"Software mining studies: Goals, approaches, artifacts, and replicability","Amann S., Beyer S., Kevic K., Gall H.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957030143&doi=10.1007%2f978-3-319-28406-4_5&partnerID=40&md5=7d7486a64aa5349348c2c00c6f013e33",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","The mining of software archives has enabled new ways for increasing the productivity in software development: Analyzing software quality, mining project evolution, investigating change patterns and evolution trends, mining models for development processes, developing methods of integrating mined data from various historical sources, or analyzing natural language artifacts in software repositories, are examples of research topics. Software repositories include various data, ranging from source control systems, issue tracking systems, artifact repositories such as requirements, design and architectural documentation, to archived communication between project members. Practitioners and researchers have recognized the potential of mining these sources to support the maintenance of software, to improve their design or architecture, and to empirically validate development techniques or processes. We revisited software mining studies that were published in recent years in the top venues of software engineering, such as ICSE, ESEC/FSE, and MSR. In analyzing these software mining studies, we highlight different viewpoints: pursued goals, state-of-the-art approaches, mined artifacts, and study replicability. To analyze the mining artifacts, we (lexically) analyzed research papers of more than a decade. In terms of replicability we looked at existing work in the field in mining approaches, tools, and platforms. We address issues of replicability and reproducibility to shed light onto challenges for large-scale mining studies that would enable a stronger conclusion stability. Â© Springer International Publishing Switzerland 2015.",,,"Springer Verlag","10th LASER Summer School on Software Engineering: Software for the Cloud and Big Data, LASER 2013â€“2014","8 September 2013 through 14 September 2013",,161369,"Conference Paper","Scopus"
"Software developer activity as a source for identifying hidden source code dependencies","KonÃ´pka M., BielikovÃ¡ M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922022283&partnerID=40&md5=f52c08102fde688c60629dd4bee8ae89",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","Connections between source code components are important to know in the whole software life. Traditionally, we use syntactic analysis to identify source code dependencies which may not be sufficient in cases of dynamically typed programming languages, loosely coupled components or when multiple programming languages are combined. We aim at using developer activity as a source for identifying implicit source code dependencies, to enrich or supplement explicitly stated dependencies in the source code. We propose a method for identification of implicit dependencies from activity logs in IDE, mainly of switching between source code files in addition to usually used logs of copy-pasting code fragments and commits.We experimentally evaluated our method using data of studentsâ€™ activity working on five projects. We compared implicit dependencies with explicit ones including manual evaluation of their significance. Our results show that implicit dependencies based on developer activity partially reflect explicit dependencies and so may supplement them in cases of their unavailability. In addition, implicit dependencies extend existing dependency graph with new significant connections applicable in software development and maintenance. Â© Springer-Verlag Berlin Heidelberg 2015.","Dependency; Dependency graph; Developer activity; Implicit dependency; Implicit feedback; Software component; Source code","CeskÃ¡ spolecnost pro kybernetiku a informatiku","Springer Verlag","41st International Conference on Current Trends in Theory and Practice of Computer Science, SOFSEM 2015","24 January 2015 through 29 January 2015",,113019,"Conference Paper","Scopus"
"An empirical study of work fragmentation in software evolution tasks","Sanchez H., Robbes R., Gonzalez V.M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928653803&doi=10.1109%2fSANER.2015.7081835&partnerID=40&md5=8a19e05e3bd7ccb9eec3ff41d3f6134f",2015,"2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2015 - Proceedings","Information workers and software developers are exposed to work fragmentation, an interleaving of activities and interruptions during their normal work day. Small-scale observational studies have shown that this can be detrimental to their work. In this paper, we perform a large-scale study of this phenomenon for the particular case of software developers performing software evolution tasks. Our study is based on several thousands interaction traces collected by Mylyn, for dozens of developers. We observe that work fragmentation is correlated to lower observed productivity at both the macro level (for entire sessions), and at the micro level (around markers of work fragmentation); further, longer activity switches seem to strengthen the effect. These observations are basis for subsequent studies investigating the phenomenon of work fragmentation. Â© 2015 IEEE.","interaction data; interruptions; Work fragmentation","Departement de Genie Informatique et Genie Logiciel (GIGL) of Polytechnique Montreal;et al.;Google Montreal;SAP Montreal Labs;The IEEE Computer Society;The Reengineering Forum (REF)","Institute of Electrical and Electronics Engineers Inc.","22nd IEEE International Conference on Software Analysis, Evolution, and Reengineering, SANER 2015","2 March 2015 through 6 March 2015",,111861,"Conference Paper","Scopus"
"Automated test oracles: State of the art, taxonomies, and trends","Oliveira R.A.P., Kanewala U., Nardi P.A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906739372&doi=10.1016%2fB978-0-12-800160-8.00003-6&partnerID=40&md5=79ee0a7e997481e919e6b6c5ceac217f",2015,"Advances in Computers","Test oracle methods have changed significantly over time, which has resulted in clear shifts in the research literature. Over the years, the testing techniques, strategies, and criteria utilized by researchers went through technical developments due to the improvement of technologies and programming languages. Software testing designers, known as testers, currently have several resources to increase their confidence in the software under test correctness. All of these software testing resources are supposed to include a mechanism to decide whether a particular execution is considered a failure or not. In software testing environments, this decision is the responsibility of the test oracle. Despite the evolution and adaptation of testing techniques over more than 30 years, test oracles remain a particular and relevant issue. In this chapter, using literary evidence from a pool of about 300 studies directly related to test oracles, we present a classification of test oracles based on a taxonomy that considers their source of information and notations. Based on this classification, we perform a quantitative analysis to highlight the shifts in (evolution of) research on test oracles. Exploring geographical and quantitative information, we analyzed the maturity of this field using coauthorship networks among studies published between 1978 and 2013. Further, we determine the most prolific authors and their countries, main conferences and journals, supporting tools, and academic efforts and use a comparative analysis between academia and industry. Finally, from these analyses, we draw an analytic reflection about contemporary test oracle approaches and a criticism about oracle trends. Â© 2014 Elsevier Inc.","Automated test oracle; Oracle problem; Scoping study; Software testing; State of the art; Survey; System quality; Test oracles",,"Academic Press Inc.",,,,,"Article","Scopus"
"Usage contracts: Offering immediate feedback on violations of structural source-code regularities","Lozano A., Mens K., Kellens A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929704961&doi=10.1016%2fj.scico.2015.01.004&partnerID=40&md5=315deb6a24165b4fca111d3831945366",2015,"Science of Computer Programming","Developers often encode design knowledge through structural regularities such as API usage protocols, coding idioms and naming conventions. As these regularities express how the source code should be structured, they provide vital information for developers using or extending that code. Adherence to such regularities tends to deteriorate over time because they are not documented and checked explicitly. This paper introduces uContracts, an internal DSL to codify and verify such regularities as 'usage contracts'. Our DSL aims at covering most common usage regularities, while still providing a means to express less common ones. Common regularities are identified based on regularities supported by existing approaches to detect bugs or suggest missing code fragments, techniques that mine for structural regularities, as well as on the analysis of an open-source project. We validate our DSL by documenting the structural regularities of an industrial case study, and analyse how useful the information provided by checking these regularities is for the developers of that case study. Â© 2015 Elsevier B.V. All rights reserved.","IDE integration; Internal domain-specific language; Software development tool support; Source code analysis; Structural regularities",,"Elsevier",,,,,"Conference Paper","Scopus"
"Vehicle-to-Vehicle Delay Tolerant Networks with Area of Interest for Road Surveillance System","Uchida N., Ito K., Hirakawa G., Arai Y., Shibata Y.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964933289&doi=10.1109%2fBWCCA.2015.98&partnerID=40&md5=05414e59ca06821e68a8de85a9784b8b",2015,"Proceedings - 2015 10th International Conference on Broadband and Wireless Computing, Communication and Applications, BWCCA 2015","The road conditions during the winter especially in mountain areas are one of the significant safety subjects for the residences and the tourists. However, the network services such as LTE or 3G are not available as the urban areas. Therefore, this paper proposed the Area of Interest (AOI) for Vehicle-to-vehicle (V2V) based the Delay Tolerant Networks (DTN) of the road surveillance system. In the proposed network system, IEEE802.11a/b/g/p is supposed for introducing the wireless V2V system, and the observed road condition's data is transmitted with the proposed the DTN routing of AOI. For the efficiency of the proposed network system, the GIS road maps of Takizawa in Japan are introduced for the computational simulations. Then, the results are discussed for the future studies of the V2V based DTN routings of the road surveillance system in local areas during the winter. Â© 2015 IEEE.","Delay Tolerant Network; Road Surveillance System; Vehicle-to-Vehicle Network",,"Institute of Electrical and Electronics Engineers Inc.","10th International Conference on Broadband and Wireless Computing, Communication and Applications, BWCCA 2015","4 November 2015 through 6 November 2015",,119875,"Conference Paper","Scopus"
"Practical domain-specific debuggers using the Moldable Debugger framework","ChiÅŸ A., Denker M., GÃ®rba T., Nierstrasz O.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943661802&doi=10.1016%2fj.cl.2015.08.005&partnerID=40&md5=2f0a60826b37bfb1d2925bf22fe46b69",2015,"Computer Languages, Systems and Structures","Understanding the run-time behavior of software systems can be a challenging activity. Debuggers are an essential category of tools used for this purpose as they give developers direct access to the running systems. Nevertheless, traditional debuggers rely on generic mechanisms to introspect and interact with the running systems, while developers reason about and formulate domain-specific questions using concepts and abstractions from their application domains. This mismatch creates an abstraction gap between the debugging needs and the debugging support leading to an inefficient and error-prone debugging effort, as developers need to recover concrete domain concepts using generic mechanisms. To reduce this gap, and increase the efficiency of the debugging process, we propose a framework for developing domain-specific debuggers, called the Moldable Debugger, that enables debugging at the level of the application domain. The Moldable Debugger is adapted to a domain by creating and combining domain-specific debugging operations with domain-specific debugging views, and adapts itself to a domain by selecting, at run time, appropriate debugging operations and views. To ensure the proposed model has practical applicability (i.e., can be used in practice to build real debuggers), we discuss, from both a performance and usability point of view, three implementation strategies. We further motivate the need for domain-specific debugging, identify a set of key requirements and show how our approach improves debugging by adapting the debugger to several domains. Â© 2015 Elsevier Ltd. All rights reserved.","Customization; Debugging; Domain-specific tools; Programming environments; Smalltalk; User interfaces",,"Elsevier Ltd",,,,,"Conference Paper","Scopus"
"Expert Cloud: A Cloud-based framework to share the knowledge and skills of human resources","Jafari Navimipour N., Rahmani A.M., Navin A.H., Hosseinzadeh M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921671043&doi=10.1016%2fj.chb.2015.01.001&partnerID=40&md5=752069125e79d3d1a768f681dfc4e362",2015,"Computers in Human Behavior","Human resources (HRs) as the most important asset of any organizations play a significant role in their success; but, HR virtualization and sharing her/his knowledge and skills was not studied in detail. In this paper, we present a new framework named Expert Cloud to enable the Cloud users for requesting the human expertise and skills without any knowledge of their location and to share the skill, knowledge and experiences of HRs. Also, we analyze, design and implement the Expert Cloud employing Internet infrastructures and Cloud computing concepts. The results show that the Expert Cloud improves the HR utilization, decreases customer response time and task completion time in order to achieve high level of customers' satisfaction and better performance of HR in organization. We conclude the paper with some suggestions for future researches and practices. Â© 2015 Elsevier Ltd.","Cloud; Cloud computing; Expert; Human knowledge; Human resource; Knowledge sharing",,"Elsevier Ltd",,,,,"Article","Scopus"
"Automatic mining of specifications from invocation traces and method invariants","Krka I., Brun Y., Medvidovic N.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986904834&doi=10.1145%2f2635868.2635890&partnerID=40&md5=583c14ce044a7bd11a68bfa69c2ef562",2014,"Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering","Software library documentation often describes individual methods' APIs, but not the intended protocols and method interactions. This can lead to library misuse, and restrict runtime detection of protocol violations and automated verification of software that uses the library. Specification mining, if accurate, can help mitigate these issues, which has led to significant research into new modelinference techniques that produce FSM-based models from program invariants and execution traces. However, there is currently a lack of empirical studies that, in a principled way, measure the impact of the inference strategies on model quality. To this end, we identify four such strategies and systematically study the quality of the models they produce for nine off-the-shelf libraries. We find that (1) using invariants to infer an initial model significantly improves model quality, increasing precision by 4% and recall by 41%, on average; (2) effective invariant filtering is crucial for quality and scalability of strategies that use invariants; and (3) using traces in combination with invariants greatly improves robustness to input noise. We present our empirical evaluation, implement new and extend existing model-inference techniques, and make public our implementations, ground-truth models, and experimental data. Our work can lead to higher-quality model inference, and directly improve the techniques and tools that rely on model inference. Copyright 2014 ACM.","Execution traces; Log analysis; Model inference","ACM Special Interest Group on Software Engineering (SIGSOFT)","Association for Computing Machinery","22nd ACM SIGSOFT International Symposium on the Foundations of Software Engineering, FSE 2014","16 November 2014 through 21 November 2014",,109032,"Conference Paper","Scopus"
"On the comprehension of program comprehension","Maalej W., Tiarks R., Roehm T., Koschke R.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907033652&doi=10.1145%2f2622669&partnerID=40&md5=0405f62141022f546c194282bb18db5a",2014,"ACM Transactions on Software Engineering and Methodology","Research in program comprehension has evolved considerably over the past decades. However, only little is known about how developers practice program comprehension in their daily work. This article reports on qualitative and quantitative research to comprehend the strategies, tools, and knowledge used for program comprehension.We observed 28 professional developers, focusing on their comprehension behavior, strategies followed, and tools used. In an online survey with 1,477 respondents, we analyzed the importance of certain types of knowledge for comprehension and where developers typically access and share this knowledge.We found that developers follow pragmatic comprehension strategies depending on context. They try to avoid comprehension whenever possible and often put themselves in the role of users by inspecting graphical interfaces. Participants confirmed that standards, experience, and personal communication facilitate comprehension. The team size, its distribution, and open-source experience influence their knowledge sharing and access behavior. While face-to-face communication is preferred for accessing knowledge, knowledge is frequently shared in informal comments.Our results reveal a gap between research and practice, as we did not observe any use of comprehension tools and developers seem to be unaware of them. Overall, our findings call for reconsidering the research agendas towards context-aware tool support. Â© 2014 ACM.","Context-aware software engineering; Empirical software engineering; Information needs; Knowledge sharing; Program comprehension",,"Association for Computing Machinery",,,,,"Conference Paper","Scopus"
"SeaDoc: A self-adaptive document link provision system for framework extension tasks","Yu D.-F., Jiau H.C.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894555636&doi=10.1007%2fs00607-013-0344-y&partnerID=40&md5=acf8fb1326dcbd22eb6d9ac274dcf6d4",2014,"Computing","It has been demonstrated that object-oriented frameworks can bring all kinds of advantages to application developers. To gain the advantages, application developers have to follow the framework-based development process. One step of the process is to integrate new components for framework extension. This is defined as a framework extension task in this work. In this task, application developers have to (1) retrieve examples, (2) acquire necessary documents, which are defined as the documents containing example adaptation information, and (3) adapt examples. Currently, acquiring necessary documents requires a lot of time because it is achieved through manually searching the Internet. Although there are many approaches to correctly acquiring those documents, the focus is never on time reduction. To satisfy the new criterion, we find the following challenging issues: (1) the dynamics of the valid document version, and (2) the uncertainty of the relevant necessary documents. The first issue is that the valid document version varies according to the framework version under which the retrieved example is workable. The second one is that the relevant necessary documents cannot be decided until a specific necessary document is specified. To resolve those two issues, a Self-adaptive Document link provision system, named SeaDoc, is provided in this work. SeaDoc resolves the dynamics by dynamically constructing document links with the corresponding valid document version. SeaDoc also resolves the uncertainty by adaptively selecting highly relevant document links. The experimental results show that SeaDoc reduces the time by 73 and 83 % compared with other two approaches. Â© 2013 Springer-Verlag Wien.","Document link; Framework extension task; Self adaptation; Software reuse",,,,,,,"Article","Scopus"
"Towards self-adaptive IDEs","Minelli R.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931095745&doi=10.1109%2fICSME.2014.121&partnerID=40&md5=bf0e1cbac100fe1cf4e76d2df19bdf1c",2014,"Proceedings - 30th International Conference on Software Maintenance and Evolution, ICSME 2014","Developers use Integrated Development Environments (IDEs) to maintain and evolve software systems. IDEs facilitate development activities such as navigating, reading, understanding, and writing source code. Development activities are composed of many basic events, such as browsing the source code of a method or editing the body of a method. We call these actions 'interaction data'. We believe that collecting, processing, and exploiting these interactions at run-time can potentially augment the productivity of developers. Our goal is to create self-adaptive IDEs: IDEs that collect, mine, and leverage the interactions of developers to better support the developers' workflow. We envision a development environment that automatically and seamlessly adapts itself to support developers while maintaining and evolving software systems. To reach our goal, we will develop means to reshape the user interface of the IDE, interaction-based recommenders, and integrate live and adaptive visualizations inside the IDE. As a first step towards our vision, we have developed DFlow, a tool that non-intrusively records all IDE interactions while a developer is programming. At the moment DFlow collects all the interactions between the developer and the IDE, and enables retrospective analysis by means of software visualizations. Â© 2014 IEEE.","developer behavior; ide; interaction data; user interface",,"Institute of Electrical and Electronics Engineers Inc.","30th International Conference on Software Maintenance and Evolution, ICSME 2014","28 September 2014 through 3 October 2014",,111661,"Conference Paper","Scopus"
"Visual storytelling of development sessions","Minelli R., Baracchi L., Mocci A., Lanza M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931050677&doi=10.1109%2fICSME.2014.65&partnerID=40&md5=56947821572853a5708880c9631fb151",2014,"Proceedings - 30th International Conference on Software Maintenance and Evolution, ICSME 2014","Most development activities, like program understanding, source code navigation and editing, are supported by Integrated Development Environments (IDEs). They provide different tools and user interfaces (UI) to interact with the source code, such as browsers, debuggers, and inspectors. It is uncertain how and when programmers use different UI elements of an IDE and to what extent they appropriately support development. Previously we developed DFLOW, a tool that seamlessly records and processes interaction data. Our long-term goal is to assess to what extent the UIs of IDEs support the workflow of developers and whether they can be improved. As a first step we present our approach to analyze development sessions in the form of visual storytelling. We illustrate our initial catalogue of visualizations through two development stories. Â© 2014 IEEE.","development sessions; IDE; interaction data; visual storytelling; visualization",,"Institute of Electrical and Electronics Engineers Inc.","30th International Conference on Software Maintenance and Evolution, ICSME 2014","28 September 2014 through 3 October 2014",,111661,"Conference Paper","Scopus"
"Assessing the quality of meta-models","LÃ³pez-FernÃ¡ndez J.J., Guerra E., De Lara J.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911484484&partnerID=40&md5=0d97177d935d71fc1482f2a56224d488",2014,"CEUR Workshop Proceedings","Meta-models play a pivotal role in Model-Driven Engineering (MDE), as they define the abstract syntax of domain-specific languages, and hence, the structure of models. However, while they play a crucial role for the success of MDE projects, the community still lacks tools to check meta-model quality criteria, like design errors or adherence to naming conventions and best practices. In this paper, we present a language (mmSpec) and a tool (metaBest) to specify and check properties on meta-models and visualise the problematic elements. Then, we use them to evaluate over 295 meta-models of the ATL zoo by provisioning a library of 30 meta-model quality issues. Finally, from this evaluation, we draw recommendations for both MDE practitioners and meta-model tool builders.",,,"CEUR-WS","11th Workshop on Model-Driven Engineering, Verification and Validation, MoDeVVa 2014, Co-located with 17th International Conference on Model Driven Engineering Languages and Systems, MODELS 2014","30 September 2014",,109117,"Conference Paper","Scopus"
"Interactive characterization of a code pattern","Nakayama K., Sakai E., Kobayakawa M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948782638&doi=10.3233%2f978-1-61499-434-3-1016&partnerID=40&md5=97273b3d04665bb7da1a7a451258be29",2014,"Frontiers in Artificial Intelligence and Applications","There are semantic chunks and relations (code patterns) of various granularities in source code. Since a code pattern is closely tied to a unit of understanding by a programmer, searching specific code pattern through unfamiliar souce code would help understanding the code. Conventional string search tools, however, are not suitable for this purpose because, for instance, they do not assume the syntax of a programming language. In this paper, a tool for search-by-example through abstract syntax tree is presented. A programmer gives sets of desired and undesired nodes, then the system presents some candidate nodes resembling desired ones. The method is supervised incremental learning of decision trees. Â© 2014 The authors and IOS Press. All rights reserved.","Abstract syntax tree; Search by example; Source code search",,"IOS Press","13th International Conference on New Trends in Intelligent Software Methodology Tools, and Techniques, SoMeT 2014","22 September 2014 through 24 September 2014",,116901,"Conference Paper","Scopus"
"Automatic segmentation of method code into meaningful blocks: Design and evaluation","Wang X., Pollock L., Vijay-Shanker K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899100471&doi=10.1002%2fsmr.1581&partnerID=40&md5=e86fea1c2c39a3c591692985f73dec3a",2014,"Journal of software: Evolution and Process","Good programming practice and guidelines suggest that programmers use both vertical and horizontal spacing to visibly delineate between code segments that represent different algorithmic steps or high-level actions. Unfortunately, programmers do not always follow these guidelines. Editors and integrated development environments (IDEs) can easily indent codes based on syntax, but they do not currently support automatic blank line insertion, which presents more significant challenges involving the semantics. This paper presents and evaluates a heuristic solution to the automatic blank line insertion problem by leveraging both program structure and naming information to identify 'meaningful blocks', consecutive statements that logically implement a high-level action. Our tool, SEGMENT, takes as input a Java method and outputs a segmented version that separates meaningful blocks by vertical spacing. We report on several studies involving human judgments to evaluate the effectiveness of the automatic blank line insertion algorithm, for different size methods and for different levels of programmer expertise. The results indicate strong positive overall opinion of SEGMENT's effectiveness in comparison with both developer-written blank lines and blank lines inserted by newcomers to the code. The results vary only slightly among short and long methods, and among novice and advanced programmers. SEGMENT assists in making users obtain an overall picture of a method's actions and comprehend it quicker as well as provides hints for internal documentation placement. Copyright Â© 2013 John Wiley & Sons, Ltd.","Automatic formatting; Program understanding; Readability; Software tool",,"John Wiley and Sons Ltd",,,,,"Conference Paper","Scopus"
"Human aspects, gamification, and social media in collaborative software engineering","Vasilescu B.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903577299&doi=10.1145%2f2591062.2591091&partnerID=40&md5=c77139012860beff8437122d20fbdc38",2014,"36th International Conference on Software Engineering, ICSE Companion 2014 - Proceedings","Software engineering is inherently a collaborative venture. In open-source software (OSS) development, such collaborations almost always span geographies and cultures. Because of the decentralised and self-directed nature of OSS as well as the social diversity inherent to OSS communities, the success of an OSS project depends to a large extent on the social aspects of distributed collaboration and achieving coordination over distance. The goal of this dissertation research is to raise our understanding of how human aspects (e.g., gender or cultural diversity), gamification and social media (e.g., participation in social environments such as Stack Overflow or GitHub) impact distributed collaboration in OSS. Copyright Â© 2014 ACM.","Collaborative software engineering; Open source","Accenture;et al.;Google;HCL;IBM;SAP","Association for Computing Machinery","36th International Conference on Software Engineering, ICSE 2014","31 May 2014 through 7 June 2014","Hyderabad",106011,"Conference Paper","Scopus"
"Let's hear both sides: On combining type-error reporting tools","Chen S., Erwig M., Smeltzer K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907405336&doi=10.1109%2fVLHCC.2014.6883038&partnerID=40&md5=64f56ae0fec6f358e2af6e99b277ff72",2014,"Proceedings of IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC","Producing precise and helpful type error messages has been a challenge for the implementations of functional programming languages for over 3 decades now. Many different approaches and methods have been tried to solve this thorny problem, but current type-error reporting tools still suffer from a lack of precision in many cases. Based on the rather obvious observation that different approaches work well in different situations, we have studied the question of whether a combination of tools that exploits their diversity can lead to improved accuracy. Specifically, we have studied Helium, a Haskell implementation particularly aimed at producing good type error messages, and Lazy Typing, an approach developed previously by us to address the premature-error-commitment problem in type checkers. By analyzing the respective strengths and weaknesses of the two approaches we were able to identify a strategy to combine both tools that could markedly improve the accuracy of reported errors. Specifically, we report an evaluation of 1069 unique ill-typed programs out of a total of 11256 Haskell programs that reveals that this combination strategy enjoys a correctness rate of 79%, which is an improvement of 22%/17% compared to using Lazy Typing/Helium alone. In addition to describing this particular case study, we will also report insights we gained into the combination of error-reporting tools in general. Â© 2014 IEEE.",,"Australian National University;IEEE Computer Society;Monash University;NSF;Swinburne University of Technology","IEEE Computer Society","2014 IEEE Symposium on Visual Languages and Human-Centric Computing, VL/HCC 2014","28 July 2014 through 1 August 2014","Melbourne, VIC",107402,"Conference Paper","Scopus"
"An exploratory study of the evolution of communicated information about the execution of large software systems","Shang W., Jiang Z.M., Adams B., Hassan A.E., Godfrey M.W., Nasser M., Flora P.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899085036&doi=10.1002%2fsmr.1579&partnerID=40&md5=6e56dd96ec68648f4bbd8a8f2aae359d",2014,"Journal of software: Evolution and Process","Substantial research in software engineering focuses on understanding the dynamic nature of software systems in order to improve software maintenance and program comprehension. This research typically makes use of automated instrumentation and profiling techniques after the fact, that is, without considering domain knowledge. In this paper, we examine another source of dynamic information that is generated from statements that have been inserted into the code base during development to draw the system administrators' attention to important run-time phenomena. We call this source communicated information (CI). Examples of CI include execution logs and system events. The availability of CI has sparked the development of an ecosystem of Log Processing Apps (LPAs) that surround the software system under analysis to monitor and document various run-time constraints. The dependence of LPAs on the timeliness, accuracy and granularity of the CI means that it is important to understand the nature of CI and how it evolves over time, both qualitatively and quantitatively. Yet, to our knowledge, little empirical analysis has been performed on CI and its evolution. In a case study on two large open source and one industrial software systems, we explore the evolution of CI by mining the execution logs of these systems and the logging statements in the source code. Our study illustrates the need for better traceability between CI and the LPAs that analyze the CI. In particular, we find that the CI changes at a high rate across versions, which could lead to fragile LPAs. We found that up to 70% of these changes could have been avoided and the impact of 15% to 80% of the changes can be controlled through the use of robust analysis techniques by LPAs. We also found that LPAs that track implementation-level CI (e.g. performance analysis) and the LPAs that monitor error messages (system health monitoring) are more fragile than LPAs that track domain-level CI (e.g. workload modelling), because the latter CI tends to be long-lived. Copyright Â© 2013 John Wiley & Sons, Ltd.","Communicated information; Execution log analysis; Reverse engineering; Software evolution",,"John Wiley and Sons Ltd",,,,,"Conference Paper","Scopus"
"On the use of context in recommending exception handling code examples","Rahman M.M., Roy C.K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924859315&doi=10.1109%2fSCAM.2014.15&partnerID=40&md5=281eee0a28cd738853a30d246149855a",2014,"Proceedings - 2014 14th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2014","Studies show that software developers often either misuse exception handling features or use them inefficiently, and such a practice may lead an undergoing software project to a fragile, insecure and non-robust application system. In this paper, we propose a context-aware code recommendation approach that recommends exception handling code examples from a number of popular open source code repositories hosted at GitHub. It collects the code examples exploiting GitHub code search API, and then analyzes, filters and ranks them against the code under development in the IDE by leveraging not only the structural (i.e., graph-based) and lexical features but also the heuristic quality measures of exception handlers in the examples. Experiments with 4,400 code examples and 65 exception handling scenarios as well as comparisons with four existing approaches show that the proposed approach is highly promising. Â© 2014 IEEE.","context-relevance; Exception handler; lexical similarity; structural similarity",,"Institute of Electrical and Electronics Engineers Inc.","14th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2014","28 September 2014 through 29 September 2014",,109634,"Conference Paper","Scopus"
"Investigating intentional clone refactoring","Wang W., Godfrey M.W.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018239054&doi=10.14279%2ftuj.eceasst.63.915.896&partnerID=40&md5=cea8eb040792b05c918ccdfee8ee44d2",2014,"Electronic Communications of the EASST","Software clone refactoring has been studied from many perspectives, including empirical research on clone refactoring history, IDE support for tracking clone change, and recommendation systems for clone management. Most of the work relies on having access to and being able to analyze the history of clone refactoring. However, refactoring cloned code is not equivalent to clone management, as code refactoring can be motivated by goals unrelated to cloning. In this position paper, we introduce a dataset of intentional clone refactoring, which is produced by keywords matching in commit messages within the version control system of Linux kernel. By investigating two important clone evolution scenarios - clone removal and inconsistent changes-in subsystems of Linux kernel, we find that intentional clone refactoring accounts for only a small proportion of all detected clone evolution. Â© Software Clones 2014.","Clone evolution; Clone management; Code refactoring",,"Universitatsbibliothek TU Berlin",,,,,"Conference Paper","Scopus"
"Assessing the capability of code smells to explain maintenance problems: An empirical study combining quantitative and qualitative data","Yamashita A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901842443&doi=10.1007%2fs10664-013-9250-3&partnerID=40&md5=21e261dba154c87d271e3c22988be8b9",2014,"Empirical Software Engineering","Code smells are indicators of deeper design problems that may cause difficulties in the evolution of a software system. This paper investigates the capability of twelve code smells to reflect actual maintenance problems. Four medium-sized systems with equivalent functionality but dissimilar design were examined for code smells. Three change requests were implemented on the systems by six software developers, each of them working for up to four weeks. During that period, we recorded problems faced by developers and the associated Java files on a daily basis. We developed a binary logistic regression model, with ""problematic file"" as the dependent variable. Twelve code smells, file size, and churn constituted the independent variables. We found that violation of the Interface Segregation Principle (a.k.a. ISP violation) displayed the strongest connection with maintenance problems. Analysis of the nature of the problems, as reported by the developers in daily interviews and think-aloud sessions, strengthened our view about the relevance of this code smell. We observed, for example, that severe instances of problems relating to change propagation were associated with ISP violation. Based on our results, we recommend that code with ISP violation should be considered potentially problematic and be prioritized for refactoring. Â© 2013 Springer Science+Business Media New York.","Code smells; Maintenance problems; Refactoring; Software maintenance",,"Kluwer Academic Publishers",,,,,"Article","Scopus"
"Supporting requirements to code traceability through refactoring","Mahmoud A., Niu N.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906310312&doi=10.1007%2fs00766-013-0197-0&partnerID=40&md5=f697420ea2703a23509719e0159153bc",2014,"Requirements Engineering","In this paper, we hypothesize that the distorted traceability tracks of a software system can be systematically re-established through refactoring, a set of behavior-preserving transformations for keeping the system quality under control during evolution. To test our hypothesis, we conduct an experimental analysis using three requirements-to-code datasets from various application domains. Our objective is to assess the impact of various refactoring methods on the performance of automated tracing tools based on information retrieval. Results show that renaming inconsistently named code identifiers, using Rename Identifier refactoring, often leads to improvements in traceability. In contrast, removing code clones, using eXtract Method (XM) refactoring, is found to be detrimental. In addition, results show that moving misplaced code fragments, using Move Method refactoring, has no significant impact on trace link retrieval. We further evaluate Rename Identifier refactoring by comparing its performance with other strategies often used to overcome the vocabulary mismatch problem in software artifacts. In addition, we propose and evaluate various techniques to mitigate the negative impact of XM refactoring. An effective traceability sign analysis is also conducted to quantify the effect of these refactoring methods on the vocabulary structure of software systems. Â© 2013 Springer-Verlag London.","Information retrieval; Refactoring; Traceability",,"Springer-Verlag London Ltd",,,,,"Article","Scopus"
"Impediments for automated testing - An empirical analysis of a user support discussion board","Wiklund K., Sundmark D., Eldh S., Lundvist K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903195625&doi=10.1109%2fICST.2014.24&partnerID=40&md5=a24f513fbf0120e77ebafde331437d1c",2014,"Proceedings - IEEE 7th International Conference on Software Testing, Verification and Validation, ICST 2014","To better understand the challenges encountered by users and developers of automatic software testing, we have performed an empirical investigation of a discussion board used for support of a test automation framework having several hundred users. The messages on the discussion board were stratified into problem reports, help requests, development information, and feature requests. The messages in the problem report and help request strata were then sampled and analyzed using thematic analysis, searching for common patterns. Our analysis indicate that a large part of the impediments discussed on the board are related to issues related to the centralized IT environment, and to erroneous behaviour connected to the use of the framework and related components. We also observed a large amount of impediments related to the use of software development tools. Turning to the help requests, we found that the majority of the help requests were about designing test scripts and not about the areas that appear to be most problematic. From our results and previous publications, we see a clear need to simplify the use, installation, and configuration of test systems of this type. The problems attributable to software development tools suggest that testers implementing test automation need more skills in handling those tools, than historically has been assumed. Finally, we propose that further research into the benefits of centralization of tools and IT environments, as well as structured deployment and efficient use of test automation, is performed. Â© 2014 IEEE.","empirical software engineering; software testing; test automation; test tools","ABB;IEEE;IEEE Computer Society","IEEE Computer Society","7th IEEE International Conference on Software Testing, Verification and Validation, ICST 2014","31 March 2014 through 4 April 2014","Cleveland, OH",105852,"Conference Paper","Scopus"
"CloudWave: Where adaptive cloud management meets DevOps","Bruneo D., Fritz T., Keidar-Barner S., Leitner P., Longo F., Marquezan C., Metzger A., Pohl K., Puliafito A., Raz D., Roth A., Salant E., Segall I., Villari M., Wolfsthal Y., Woods C.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908149098&doi=10.1109%2fISCC.2014.6912638&partnerID=40&md5=ab47a4d28e308f6cb7ff507d118eb7eb",2014,"Proceedings - International Symposium on Computers and Communications","The transition to cloud computing offers a large number of benefits, such as lower capital costs and a highly agile environment. Yet, the development of software engineering practices has not kept pace with this change. Moreover, the design and runtime behavior of cloud based services and the underlying cloud infrastructure are largely decoupled from one another.This paper describes the innovative concepts being developed by CloudWave to utilize the principles of DevOps to create an execution analytics cloud infrastructure where, through the use of programmable monitoring and online data abstraction, much more relevant information for the optimization of the ecosystem is obtained. Required optimizations are subsequently negotiated between the applications and the cloud infrastructure to obtain coordinated adaption of the ecosystem. Additionally, the project is developing the technology for a Feedback Driven Development Standard Development Kit which will utilize the data gathered through execution analytics to supply developers with a powerful mechanism to shorten application development cycles. Â© 2014 IEEE.","Cloud Computing; Coordinated Adaptation; DevOps; Feedback Driven Development",,"Institute of Electrical and Electronics Engineers Inc.","ISCC 2014 Workshop - 5th IEEE International Workshop on Performance Evaluation of Communications in Distributed Systems and Web based Service Architectures, PEDISWESA 2014","23 June 2014 through 26 June 2014",,108095,"Conference Paper","Scopus"
"Equipping IDEs with XML-path reasoning capabilities","GenevÃ¨s P., LayÃ¤ida N.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905235196&doi=10.1145%2f2602573&partnerID=40&md5=7f1325f669477c8a2d10378aeba028a7",2014,"ACM Transactions on Internet Technology","One of the challenges in Web development is to achieve a good level of quality in terms of code size and runtime performance for popular domain-specific languages such as XQuery, XSLT, and XML Schema. We present the first IDE augmented with static detection of inconsistent XPath expressions that assists the programmer with simplifying development and debugging of any application involving XPath expressions. The tool is based on newly developed formal verification techniques based on expressive modal logics, which are now mature enough to be introduced in the process of software development. We further develop this idea in the context of XQuery for which we introduce an analysis for identifying and eliminating dead code automatically. This proof of concept aims at illustrating the benefits of equipping modern IDEs with reasoning capabilities. Â© 2014 ACM.","Analysis; Compile time; Environment; Path; Programming; Query; Reasoning; Schema; XML",,"Association for Computing Machinery",,,,,"Article","Scopus"
"Back-to-back testing of model-based code generators","JÃ¶rges S., Steffen B.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910610656&partnerID=40&md5=61741d1a21e5e37646cbc3fd55148857",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","In this paper, we present the testing approach of the Genesys code generator framework. The employed approach is based on back-to-back- testing, which tests the translation performed by a code generator from a semantic perspective rather than just checking for syntactic correctness of the generation result.We describe the basic testing framework and show that it scales in three dimensions: parameterized tests, testing across multiple target platforms and testing on multiple meta-levels.In particular, the latter is only possible due to the fact that Genesys code generators are constructed as models. Furthermore, in order to facilitate simplicity, Genesys consistently employs one single notation for all artifacts involved in this testing approach: Test data, test cases, the code generators under test, and even the testing framework itself are all modeled using the same graphical modeling language. Â© Springer-Verlag Berlin Heidelberg 2014.",,"Bioinformatics and Human Electrophysiology Lab;EASST;technische universitat dortmund","Springer Verlag","6th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation, ISoLA 2014","8 October 2014 through 11 October 2014",,110129,"Conference Paper","Scopus"
"Explore-by-example: An automatic query steering framework for interactive data exploration","Dimitriadou K., Papaemmanouil O., Diao Y.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904301013&doi=10.1145%2f2588555.2610523&partnerID=40&md5=35968bf0f988677ee779ffb035b7cf0b",2014,"Proceedings of the ACM SIGMOD International Conference on Management of Data","Interactive Data Exploration (IDE) is a key ingredient of a diverse set of discovery-oriented applications, including ones from scientific computing and evidence-based medicine. In these applications, data discovery is a highly ad hoc interactive process where users execute numerous exploration queries using varying predicates aiming to balance the trade-off between collecting all relevant information and reducing the size of returned data. Therefore, there is a strong need to support these human-in-the-loop applications by assisting their navigation in the data to find interesting objects. In this paper, we introduce AIDE, an Automatic Interactive Data Exploration framework, that iteratively steers the user towards interesting data areas and ""predicts"" a query that retrieves his objects of interest. Our approach leverages relevance feedback on database samples to model user interests and strategically collects more samples to refine the model while minimizing the user effort. AIDE integrates machine learning and data management techniques to provide effective data exploration results (matching the user's interests with high accuracy) as well as high interactive performance. It delivers highly accurate query predictions for very common conjunctive queries with very small user effort while, given a reasonable number of samples, it can predict with high accuracy complex conjunctive queries. Furthermore, it provides interactive performance by limiting the user wait time per iteration to less than a few seconds in average. Our user study indicates that AIDE is a practical exploration framework as it significantly reduces the user effort and the total exploration time compared with the current state-of-the-art approach of manual exploration. Â© 2014 ACM.","Data exploration; Database sampling; Query formulation","ACM SIGMOD","Association for Computing Machinery","2014 ACM SIGMOD International Conference on Management of Data, SIGMOD 2014","22 June 2014 through 27 June 2014","Snowbird, UT",106349,"Conference Paper","Scopus"
"Twende-Twende: A mobile application for traffic congestion awareness and routing","Kinai A., Bryant R.E., Walcott-Bryant A., Mibuari E., Weldemariam K., Stewart O.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903649647&doi=10.1145%2f2593902.2593926&partnerID=40&md5=1d6f1579bba09955d1123fd23fec0ee0",2014,"1st International Conference on Mobile Software Engineering and Systems, MOBILESoft 2014 - Proceedings","According to the UN-HABITAT, the city of Nairobi loses half a million USD daily due to congestion on roads designed for a city 10 times smaller. Therefore, there is a great need for traffic management and awareness solutions. Many existing solutions are unsuitable for cities like Nairobi due to economic constraints, dynamic events, uncertainty, and poor infrastructure. Recently, a novel approach called Frugal Innovation has been adopted at IBM Tokyo Research. The approach combines very low quality images (VLQI) captured by existing low-cost cameras with network flow algorithms to accurately estimate traffic flow. We extend their work to develop a mobile app, called Twende-Twende, that provides drivers with real-time traffic information and suggested routes. We incorporate locally relevant context (such as references to landmarks) to predict congestion and create traffic awareness. We deployed the app and evaluated its effectiveness, accuracy and usability. Our initial evaluation indicates that the app enhances the driving experience and can be deployed in other developing countries.","Frugal innovation; Land marking; Mobile application; Traffic congestion","'IEEE Computer Society's Tech. Council on Software Eng. (TCSE)';ACM Special Interest Group on Software Engineering (SIGSOFT)","Association for Computing Machinery","1st International Conference on Mobile Software Engineering and Systems, MOBILESoft 2014","2 June 2014 through 3 June 2014","Hyderabad",106012,"Conference Paper","Scopus"
"Providing ubiquitous communication using handover techniques in VANET systems","Ghosh A., Paranthaman V.V., Mapp G., Gemikonakli O.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904541966&doi=10.1109%2fMedHocNet.2014.6849124&partnerID=40&md5=beb941025f8395d62a87487d14e336a9",2014,"2014 13th Annual Mediterranean Ad Hoc Networking Workshop, MED-HOC-NET 2014","Vehicular Ad hoc Networks are a long-term solution contributing significantly towards Intelligent Transport Systems in providing access to critical life-safety applications and services. Although Vehicular Ad hoc Networks are attracting greater commercial interest, current research has not adequately captured the real-world constraints in Vehicular Ad hoc Network handover techniques. This is necessary in order to provide seamless connectivity for optimal coverage and ideal channel utilization. Our previous work highlighted the challenges in providing ubiquitous communication using Road Side Unit in Vehicular Ad hoc Network. We used some of the concepts of the Y-Comm architecture such as Network Dwell Time, Time before Handover and Exit Time to provide a framework to investigate handover issues concentrating on essential parameters such beaconing and velocity of the vehicle. The results clearly showed that the Network Dwell Time was affected by frequency of the beacon and as well as the velocity of the vehicle. In this paper we conducted simulation studies to further examine the relation between these parameters along with different size of beacons. Simulation of VANET systems depends critically upon the calculation of the probability of a successful reception of a beacon or packet. The current formulas used to calculate the successful beacon reception depends on randomness and do not take into account the frequency of the beacon or velocity of the vehicle. This randomness puts a challenge for estimating the Network Dwell Time. This paper shows that these factors are significant and point to the need for a more complete analytical model for estimating the Network Dwell Time. Â© 2014 IEEE.","IEEE 802.11p beaconing; Network Dwell Time (NDT); Proactive Handover; VANET; Vehicle to Infrastructure Communication (V2I)",,"IEEE Computer Society","2014 13th Annual Mediterranean Ad Hoc Networking Workshop, MED-HOC-NET 2014","2 June 2014 through 4 June 2014","Piran",106458,"Conference Paper","Scopus"
"Autocomplete painting repetitions","Xing J., Chen H.-T., Wei L.-Y.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914703549&doi=10.1145%2f2661229.2661247&partnerID=40&md5=71357321e815608ff7372fc042717d47",2014,"ACM Transactions on Graphics","(Figure Presented) Painting is a major form of content creation, offering unlimited control and freedom of expression. However, it can involve tedious manual repetitions, such as stippling large regions or hatching complex contours. Thus, a central goal in digital painting research is to automate tedious repetitions while allowing user control. Existing methods impose a sequential order, in which a small exemplar is prepared and then cloned through additional gestures. Such sequential mode may break the continuous, spontaneous flow of painting. Moreover, it is more suitable for homogeneous areas than nuanced variations common in real paintings. We present an interactive digital painting system that autocompletes tedious repetitions while preserving nuanced variations and maintaining natural flows. Specifically, users paint as usual, while our system records and analyzes their workflows. When potential repetition is detected, our system predicts what the user might want to draw and offers auto-completes that adjust to the existing shape-color context. Our method eliminates the need for sequential creation-cloning and better adapts to the local painting contexts. Furthermore, users can choose to accept, ignore, or modify those predictions and thus maintain full control. Our method can be considered as the painting analogy of auto-completes in common typing and IDE systems. We demonstrate the quality and usability of our system through painting results and a pilot user study. 2014 Copyright held by the Owner/Author. Publication rights licensed to ACM.","Analysis; Autocomplete; Context; Digital painting; Editing history; Repetition; Synthesis; Texture; User interface; Workflow",,"Association for Computing Machinery",,,,,"Conference Paper","Scopus"
"Robust optimisation of CO<inf>2</inf> sequestration strategies under geological uncertainty using adaptive sparse grid surrogates","Petvipusit K.R., Elsheikh A.H., Laforce T.C., King P.R., Blunt M.J.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939877537&doi=10.1007%2fs10596-014-9425-z&partnerID=40&md5=7e1cae6b0f2dea0f462b42b2bba207cc",2014,"Computational Geosciences","Geologic CO<inf>2</inf> sequestration in deep saline aquifers is a promising technique to mitigate the effect of greenhouse gas emissions. Designing optimal CO<inf>2</inf> injection strategy becomes a challenging problem in the presence of geological uncertainty. We propose a surrogate assisted optimisation technique for robust optimisation of CO<inf>2</inf> injection strategies. The surrogate is built using Adaptive Sparse Grid Interpolation (ASGI) to accelerate the optimisation of CO<inf>2</inf> injection rates. The surrogate model is adaptively built with different numbers of evaluation points (simulation runs) in different dimensions to allow automatic refinement in the dimension where added resolution is needed. This technique is referred to as dimensional adaptivity and provides a good balance between the accuracy of the surrogate model and the number of simulation runs to save computational costs. For a robust design, we propose a utility function which comprises the statistical moment of the objective function. Numerical testing of the proposed approach applied to benchmark functions and reservoir models shows the efficiency of the method for the robust optimisation of CO<inf>2</inf> injection strategies under geological uncertainty. Â© 2014, Springer International Publishing Switzerland.","Adaptive sparse grid interpolation (ASGI); CO<inf>2</inf> sequestration; Geological uncertainty; Robust optimisation; Surrogate-assisted optimisation",,"Kluwer Academic Publishers",,,,,"Article","Scopus"
"Comparing and combining evolutionary couplings from interactions and commits","Bantelay F., Zanjani M.B., Kagdi H.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893388515&doi=10.1109%2fWCRE.2013.6671306&partnerID=40&md5=6221d116edc68ad78c8a02f17159f535",2013,"Proceedings - Working Conference on Reverse Engineering, WCRE","The paper presents an approach to mine evolutionary couplings from a combination of interaction (e.g., Mylyn) and commit (e.g., CVS) histories. These evolutionary couplings are expressed at the file and method levels of granularity, and are applied to support the tasks of commit and interaction predictions. Although the topic of mining evolutionary couplings has been investigated previously, the empirical comparison and combination of the two types from interaction and commit histories have not been attempted. An empirical study on 3272 interactions and 5093 commits from Mylyn, an open source task management tool, was conducted. These interactions and commits were divided into training and testing sets to evaluate the combined, and individual, models. Precision and recall metrics were used to measure the performance of these models. The results show that combined models offer statistically significant increases in recall over the individual models for change predictions. At the file level, the combined models achieved a maximum recall improvement of 13% for commit prediction with a 2% maximum precision drop. Â© 2013 IEEE.","Commit History; Evolutionary Couplings; Interaction History; Mining Software Repositories; Mylyn","The Reengineering Forum;Technical Council on Software Engineering (TCSE)",,"20th Working Conference on Reverse Engineering, WCRE 2013","14 October 2013 through 17 October 2013","Koblenz",102314,"Conference Paper","Scopus"
"Using HTML5 visualizations in software fault localization","Gouveia C., Campos J., Abreu R.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891513202&doi=10.1109%2fVISSOFT.2013.6650539&partnerID=40&md5=909f51a9a2d83330e59b6efb9ec0920e",2013,"2013 1st IEEE Working Conference on Software Visualization - Proceedings of VISSOFT 2013","Testing and debugging is the most expensive, error-prone phase in the software development life cycle. Automated software fault localization can drastically improve the efficiency of this phase, thus improving the overall quality of the software. Amongst the most well-known techniques, due to its efficiency and effectiveness, is spectrum-based fault localization. In this paper, we propose three dynamic graphical forms using HTML5 to display the diagnostic reports yielded by spectrum-based fault localization. The visualizations proposed, namely Sunburst, Vertical Partition, and Bubble Hierarchy, have been implemented within the GZOLTAR toolset, replacing previous and less-intuitive OpenGL-based visualizations. The GZOLTAR toolset is a plug-and-play plugin for the Eclipse IDE to ease world-wide adoption. Finally, we performed an user study with GZOLTAR and confirmed that the visualizations help to drastically reduce the time needed in debugging (e.g., all participants using the visualizations were able to pinpoint the fault, whereas of those using traditional methods only 35% found the fault). The group that used the visualizations took on average 9 minutes and 17 seconds less than the group that did not use them. Â© 2013 IEEE.","Automatic Debugging; GZOLTAR; Reports; Visualizations",,,"2013 1st IEEE Working Conference on Software Visualization, VISSOFT 2013","27 September 2013 through 28 September 2013","Eindhoven",101610,"Conference Paper","Scopus"
"Personalized defect prediction","Jiang T., Tan L., Kim S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893606732&doi=10.1109%2fASE.2013.6693087&partnerID=40&md5=9b3a89ba76b6e95cbd4310d8a686e32a",2013,"2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings","Many defect prediction techniques have been proposed. While they often take the author of the code into consideration, none of these techniques build a separate prediction model for each developer. Different developers have different coding styles, commit frequencies, and experience levels, causing different defect patterns. When the defects of different developers are combined, such differences are obscured, hurting prediction performance. This paper proposes personalized defect prediction - building a separate prediction model for each developer to predict software defects. As a proof of concept, we apply our personalized defect prediction to classify defects at the file change level. We evaluate our personalized change classification technique on six large software projects written in C and Java - the Linux kernel, PostgreSQL, Xorg, Eclipse, Lucene and Jackrabbit. Our personalized approach can discover up to 155 more bugs than the traditional change classification (210 versus 55) if developers inspect the top 20% lines of code that are predicted buggy. In addition, our approach improves the F1-score by 0.01-0.06 compared to the traditional change classification. Â© 2013 IEEE.","Change classification; machine learning; personalized defect prediction; software reliability","IEEE Computer Society;Association for Computing Machinery, Special Interest Group on Software Engineering (ACM SIGSOFT);IEEE Technical Council on Software Engineering (TCSE);ACM SIGART;NASA",,"2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013","11 November 2013 through 15 November 2013","Palo Alto, CA",102467,"Conference Paper","Scopus"
"Natural language-based software analyses and tools for software maintenance","Pollock L., Vijay-Shanker K., Hill E., Sridhara G., Shepherd D.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893806923&doi=10.1007%2f978-3-642-36054-1_4&partnerID=40&md5=0c67eb59f0ff440cef9d3140bbb8b2c6",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","Significant portions of software life cycle resources are devoted to program maintenance, which motivates the development of automated techniques and tools to support the tedious, error-prone tasks. Natural language clues from programmers' naming in literals, identifiers, and comments can be leveraged to improve the effectiveness of many software tools. For example, they can be used to increase the accuracy of software search tools, improve the ability of program navigation tools to recommend related methods, and raise the accuracy of other program analyses by providing access to natural language information. This chapter focuses on how to capture, model, and apply the programmers' conceptual knowledge expressed in both linguistic information as well as programming language structure and semantics. We call this kind of analysis Natural Language Program Analysis (NLPA) since it combines natural language processing techniques with program analysis to extract information for analysis of the source program. Â© Springer-Verlag 2013.","Natural language program analysis; Software engineering tools; Software maintenance",,,"International Summer Schools on Software Engineering, ISSSE 2009-2011",,"Salerno",102510,"Conference Paper","Scopus"
"Recommending auto-completions for software modeling activities","Kuschke T., MÃ¤der P., Rempel P.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886842342&doi=10.1007%2f978-3-642-41533-3_11&partnerID=40&md5=296e6da71eb69d99fbae3882e80d691f",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","Auto-completion of textual inputs benefits software developers using IDEs. However, graphical modeling tools used to design software do not provide this functionality. The challenges of recommending auto-completions for graphical modeling activities are largely unexplored. Recommending such auto-completions requires detecting meaningful partly completed activities, tolerating variance in user actions, and determining most relevant activities that a user wants to perform. This paper proposes an approach that works in the background while a developer is creating or evolving models and handles all these challenges. Editing operations are analyzed and matched to a predefined but extensible catalog of common modeling activities for structural UML models. In this paper we solely focus on determining recommendations rather than automatically completing activities. We demonstrated the quality of recommendations generated by our approach in a controlled experiment with 16 students evolving models.We recommended 88% of a user's activities within a short list of ten recommendations. Â© 2013 Springer-Verlag.",,"Microsoft Research;CEA-List;Intentional Software;Tata Consulting Services;Siemens",,"16th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems, MODELS 2013","29 September 2013 through 4 October 2013","Miami, FL",100444,"Conference Paper","Scopus"
"Transfer defect learning","Nam J., Pan S.J., Kim S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886378360&doi=10.1109%2fICSE.2013.6606584&partnerID=40&md5=4cb028aa369f1909438b53cc8e2523a7",2013,"Proceedings - International Conference on Software Engineering","Many software defect prediction approaches have been proposed and most are effective in within-project prediction settings. However, for new projects or projects with limited training data, it is desirable to learn a prediction model by using sufficient training data from existing source projects and then apply the model to some target projects (cross-project defect prediction). Unfortunately, the performance of cross-project defect prediction is generally poor, largely because of feature distribution differences between the source and target projects. In this paper, we apply a state-of-the-art transfer learning approach, TCA, to make feature distributions in source and target projects similar. In addition, we propose a novel transfer defect learning approach, TCA+, by extending TCA. Our experimental results for eight open-source projects show that TCA+ significantly improves cross-project prediction performance. Â© 2013 IEEE.","cross-project defect prediction; empirical software engineering; transfer learning","Association for Computing Machinery (ACM);SIGSOFT;IEEE Computer Society;Technical Council on Software Engineering (TCSE)",,"2013 35th International Conference on Software Engineering, ICSE 2013","18 May 2013 through 26 May 2013","San Francisco, CA",100317,"Conference Paper","Scopus"
"Assisting developers of big data analytics applications when deploying on Hadoop clouds","Shang W., Jiang Z.M., Hemmati H., Adams B., Hassan A.E., Martin P.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886412175&doi=10.1109%2fICSE.2013.6606586&partnerID=40&md5=700b36ee4046a3ca3ef0dabe0e884edf",2013,"Proceedings - International Conference on Software Engineering","Big data analytics is the process of examining large amounts of data (big data) in an effort to uncover hidden patterns or unknown correlations. Big Data Analytics Applications (BDA Apps) are a new type of software applications, which analyze big data using massive parallel processing frameworks (e.g., Hadoop). Developers of such applications typically develop them using a small sample of data in a pseudo-cloud environment. Afterwards, they deploy the applications in a large-scale cloud environment with considerably more processing power and larger input data (reminiscent of the mainframe days). Working with BDA App developers in industry over the past three years, we noticed that the runtime analysis and debugging of such applications in the deployment phase cannot be easily addressed by traditional monitoring and debugging approaches. In this paper, as a first step in assisting developers of BDA Apps for cloud deployments, we propose a lightweight approach for uncovering differences between pseudo and large-scale cloud deployments. Our approach makes use of the readily-available yet rarely used execution logs from these platforms. Our approach abstracts the execution logs, recovers the execution sequences, and compares the sequences between the pseudo and cloud deployments. Through a case study on three representative Hadoop-based BDA Apps, we show that our approach can rapidly direct the attention of BDA App developers to the major differences between the two deployments. Knowledge of such differences is essential in verifying BDA Apps when analyzing big data in the cloud. Using injected deployment faults, we show that our approach not only significantly reduces the deployment verification effort, but also provides very few false positives when identifying deployment failures. Â© 2013 IEEE.","Big-Data Analytics Application; Cloud Computing; Hadoop; Log Analysis; Monitoring and Debugging","Association for Computing Machinery (ACM);SIGSOFT;IEEE Computer Society;Technical Council on Software Engineering (TCSE)",,"2013 35th International Conference on Software Engineering, ICSE 2013","18 May 2013 through 26 May 2013","San Francisco, CA",100317,"Conference Paper","Scopus"
"Multi-dimensional exploration of API usage","De Roover C., Lammel R., Pek E.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886005716&doi=10.1109%2fICPC.2013.6613843&partnerID=40&md5=4800f7ea8ea330fd756a1f97362fc86c",2013,"IEEE International Conference on Program Comprehension","This paper is concerned with understanding API usage in a systematic, explorative manner for the benefit of both API developers and API users. There exist complementary, less explorative methods, e.g., based on code search, code completion, or API documentation. In contrast, our approach is highly interactive and can be seen as an extension of what IDEs readily provide today. Exploration is based on multiple dimensions: i) the hierarchically organized scopes of projects and APIs; ii) metrics of API usage (e.g., number of project classes extending API classes); iii) metadata for APIs; iv) project- versus API-centric views. We also provide the QUAATLAS corpus of Java projects which enhances the existing QUALITAS corpus to enable API-usage analysis. We implemented the exploration approach in an open-source, IDE-like, Web-enabled tool EXAPUS. Â© 2013 IEEE.","API usage; code exploration; EXAPUS; metadata; program comprehension; QUAATLAS; QUALITAS; reverse engineering",,,"2013 21st International Conference on Program Comprehension, ICPC 2013","20 May 2013 through 21 May 2013","San Francisco, CA",100319,"Conference Paper","Scopus"
"Specification and reasoning in SE projects using a Web IDE","Cook C.T., Drachova-Strang S.V., Sun Y.-S., Sitaraman M., Carver J.C., Hollingsworth J.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884931985&doi=10.1109%2fCSEET.2013.6595254&partnerID=40&md5=cf81114e90749ea9002ed9ee37e32df7",2013,"Software Engineering Education Conference, Proceedings","A key goal of our research is to introduce an approach that involves at the outset using analytical reasoning as a method for developing high quality software. This paper summarizes our experiences in introducing mathematical reasoning and formal specification-based development using a web-integrated environment in an undergraduate software engineering course at two institutions at different levels, with the goal that they will serve as models for other educators. At Alabama, the reasoning topics are introduced over a two-week period and are followed by a project. At Clemson, the topics are covered in more depth over a five-week period and are followed by specification-based software development and reasoning assignments. The courses and project assignments have been offered for multiple semesters. Evaluation of student performance indicates that the learning goals were met. Â© 2013 IEEE.",,"IEEE Computer Society",,"2013 26th International Conference on Software Engineering Education and Training, CSEE and T 2013","19 May 2013 through 21 May 2013","San Francisco, CA",99736,"Conference Paper","Scopus"
"Early detection of collaboration conflicts and risks","Brun Y., Holmes R., Ernst M.D., Notkin D.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883723561&doi=10.1109%2fTSE.2013.28&partnerID=40&md5=e692526fef007634849e5589c24302eb",2013,"IEEE Transactions on Software Engineering","Conflicts among developers' inconsistent copies of a shared project arise in collaborative development and can slow progress and decrease quality. Identifying and resolving such conflicts early can help. Identifying situations which may lead to conflicts can prevent some conflicts altogether. By studying nine open-source systems totaling 3.4 million lines of code, we establish that conflicts are frequent, persistent, and appear not only as overlapping textual edits but also as subsequent build and test failures. Motivated by this finding, we develop a speculative analysis technique that uses previously unexploited information from version control operations to precisely diagnose important classes of conflicts. Then, we design and implement Crystal, a publicly available tool that helps developers identify, manage, and prevent conflicts. Crystal uses speculative analysis to make concrete advice unobtrusively available to developers. Â© 2013 IEEE.","collaboration conflicts; Collaborative development; Crystal; developer awareness; speculative analysisversion control",,,,,,,"Article","Scopus"
"Data debugging with continuous testing","MuÅŸlu K., Brun Y., Meliou A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883722024&doi=10.1145%2f2491411.2494580&partnerID=40&md5=ae8e1598cf444da8f2aa348bbeab3cb1",2013,"2013 9th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2013 - Proceedings","Today, systems rely as heavily on data as on the software that manipulates those data. Errors in these systems are incredibly costly, annually resulting in multi-billion dollar losses, and, on multiple occasions, in death. While software debugging and testing have received heavy research attention, less effort has been devoted to data debugging: discovering system errors caused by well-formed but incorrect data. In this paper, we propose continuous data testing: using otherwise-idle CPU cycles to run test queries, in the background, as a user or database administrator modifies a database. This technique notifies the user or administrator about a data bug as quickly as possible after that bug is introduced, leading to at least three benefits: (1) The bug is discovered quickly and can be fixed before it is likely to cause a problem. (2) The bug is discovered while the relevant change is fresh in the user's or administrator's mind, increasing the chance that the underlying cause of the bug, as opposed to only the discovered side-effect, is fixed. (3) When poor documentation or company policies contribute to bugs, discovering the bug quickly is likely to identify these contributing factors, facilitating updating documentation and policies to prevent similar bugs in the future. We describe the problem space and potential benefits of continuous data testing, our vision for the technique, challenges we encountered, and our prototype implementation for PostgreSQL. The prototype's low overhead shows promise that continuous data testing can address the important problem of data debugging. Copyright 2013 ACM.","Continuous testing; Data debugging; Database testing","Association for Computing Machinery, Special;Interest Group on Software Engineering (ACM SIGSOFT)",,"2013 9th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2013","18 August 2013 through 26 August 2013","Saint Petersburg",99148,"Conference Paper","Scopus"
"Audio-visual integration in stereoscopic 3D","Deas L., Wilcox L.M., Kazimi A., Allison R.S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883694260&doi=10.1145%2f2492494.2492506&partnerID=40&md5=803207e5a3bacc9fa7c0d2673265fabc",2013,"Proceedings - SAP 2013: ACM Symposium on Applied Perception","The perception of synchronous, intelligible, speech is fundamental to a high-quality modern cinema experience. Surprisingly, this issue has remained relatively unexplored in stereoscopic 3D (S3D) media, despite its increasing popularity. Instead, visual parameters have been the primary focus of concern for those who create, and those who study the impact of, S3D content. In the work presented here we ask if ability to integrate audio and visual information is influenced by adding the third dimension to film. We also investigate the effects of known visual parameters (horizontal and vertical parallax), on audio-visual integration. To this end, we use an illusion of speech processing known as the McGurk effect as an objective measure of multi-modal integration. In the classic (2D) version of this phenomenon, discrepant auditory (/ba/) and visual (/ga/) information typically results in the perception of a unique 'fusion' syllable (e.g. /da/). We extended this paradigm to measure the McGurk effect in a small theatre. We varied the horizontal (IA: 0, 6, 12, 18, 24 mm) and vertical (0Â°, 0.5Â°, 0.75Â°, 1Â°) parallax from trial-to-trial and asked observers to report their percept of the phoneme. Our results show a consistently high proportion of the expected fusion responses, with no effect of horizontal or vertical offsets. These data are the first to show that the McGurk effect extends to stereoscopic stimuli and is not a phenomenon isolated to 2D media perception. Furthermore, the results show that audiences can tolerate a high level of both horizontal and vertical disparity and maintain veridical speech perception. We consider these results in terms of current stereoscopic filmmaking recommendations and practices. Â© 2013 ACM.","3D film; audio-visual integration; McGurk effect; stereoscopic 3D; vertical parallax","ACM SIGGRAPH;Disney Research;Acuity;Science Gallery;Failte Ireland",,"2013 ACM Symposium on Applied Perception, SAP 2013","22 August 2013 through 23 August 2013","Dublin",99301,"Conference Paper","Scopus"
"OBEY: Optimal batched refactoring plan execution for class responsibility redistribution","Jiau H.C., Mar L.W., Chen J.C.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883419213&doi=10.1109%2fTSE.2013.19&partnerID=40&md5=523b26d3b76a7361afedb69807690e7c",2013,"IEEE Transactions on Software Engineering","The redistribution of class responsibilities is a common reengineering practice in object-oriented (OO) software evolution. During the redistribution, developers frequently construct batched refactoring plans for moving multiple methods and fields among various classes. With an objective of carefully maintaining the cohesion and coupling degree of the class design, executing a batched refactoring plan without introducing any objective-violating side effect into the refactored code is essential. However, using most refactoring engines for batched refactoring plan execution introduces coupling-increasing Middle Man bad smell in the final refactored code and therefore makes the refactoring execution suboptimal in achieving the redistribution objective. This work proposes Obey, a methodology for optimal batched refactoring plan execution. Obey analyzes a batched refactoring plan, identifies Middle Man symptoms that cause suboptimal execution, and renovates the plan for optimal execution. We have conducted an empirical study on three open-source software projects to confirm the effectiveness of Obey in a practical context. Â© 1976-2012 IEEE.","batched refactoring execution; change impact analysis; class responsibility redistribution; optimization; Reengineering",,,,,,,"Article","Scopus"
"Conflict-aware optimal scheduling of prioritised code clone refactoring","Zibran M.F., Roy C.K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879668288&doi=10.1049%2fiet-sen.2012.0058&partnerID=40&md5=366f88ca90e60c93313a7a69e620bebf",2013,"IET Software","Duplicated or similar source code, also known as code clones, are possible malicious 'code smells' that may need to be removed through refactoring to enhance maintainability. Among many potential refactoring opportunities, the choice and order of a set of refactoring activities may have distinguishable effect on the design/code quality measured in terms of software metrics. Moreover, there may be dependencies and conflicts among those refactorings of different priorities. Addressing all the conflicts, priorities and dependencies, a manual formulation of an optimal refactoring schedule is very expensive, if not impossible. Therefore an automated refactoring scheduler is necessary to 'maximise benefit and minimise refactoring effort'. However, the estimation of the efforts required to perform code clone refactoring is a challenging task. This study makes two contributions. First, the authors propose an effort model for the estimation of code clone refactoring efforts. Second, the authors propose a constraint programming (CP) approach for conflict-aware optimal scheduling of code clone refactoring. A qualitative evaluation of the effort model from the developers' perspective suggests that the model is complete and useful for code clone refactoring effort estimation. The authors also quantitatively compared their refactoring scheduler with other wellknown scheduling techniques such as the genetic algorithm, greedy approaches and linear programming. The authors' empirical study suggests that the proposed CP-based approach outperforms other approaches they considered. Â© 2013 The Institution of Engineering and Technology.",,,,,,,,"Article","Scopus"
"I2SD: Reverse engineering sequence diagrams from enterprise java beans with interceptors","Roubtsov S., Serebrenik A., Mazoyer A., Van Den Brand M.G.J., Roubtsova E.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879651985&doi=10.1049%2fiet-sen.2012.0056&partnerID=40&md5=ec4eb618cb5f0a69b7e53d82343b83f2",2013,"IET Software","An Enterprise JavaBeans (EJB) interceptor is a software mechanism that provides for introducing behaviour implemented as separate code into the execution of a Java application. In this way, EJB interceptors provide a clear separation of the core functionality of the bean and other concerns, such as logging or performance analysis. Despite the beauty of the idea behind the i nterceptors, developing, testing and managing dependencies introduced by the interceptors are considered to be daunting tasks. For example, the developers can specify interceptors at multiple locations and by multiple means. However, different locations and specification means influence the order of the interceptor invocation, which is governed by more than 15 different intertwined rules defined in the EJB standard. To facilitate development of EJB applications, we have designed I2SD, Interceptors to Sequence Diagrams, a tool for reverse engineering EJB applications with interceptors to unified modeling language (UML) sequence diagrams. I2SD provides the developer with a visual feedback and can be used by quality managers to obtain insights in the ways interceptors are used in their project. Â© 2013 The Institution of Engineering and Technology.",,,,,,,,"Article","Scopus"
"Boosting Paired Comparison methodology in measuring visual discomfort of 3DTV: Performances of three different designs","Li J., Barkowsky M., Le Callet P.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878690903&doi=10.1117%2f12.2002075&partnerID=40&md5=c3e270660d0dc99d3ec7611e596b05db",2013,"Proceedings of SPIE - The International Society for Optical Engineering","The pair comparison method is often recommended in subjective experiments because of the reliability of the obtained results. However, a drawback of this method is that the number of comparisons increases exponentially with the number of stimuli, which limits its usability for a large number of stimuli. Several design methods that aim to reduce the number of comparisons were proposed in the literature. However, their performances in the context of 3DTV should be evaluated carefully due to the fact that the results obtained from a paired comparison experiment in 3DTV may be inuenced by two important factors. One is the observation error from observer's attentiveness, in particular inverting the vote. The second factor concerns the dependence on the context in which the evaluation takes place. In this study, three design methods, namely Full Paired Comparison method (FPC), Square Design method (SD) and the Adaptive Square Design method (ASD) were evaluated by subjective visual discomfort experiment in 3DTV. The results from the FPC method were considered as the ground truth. Comparing with the ground truth, the ASD method provided the most accurate results with a given number of trials. It also showed the highest robustness against observation errors and interdependence of comparisons. Due to the efficiency of the ASD method, paired comparison experiments become feasible with a reasonably large number of stimuli for measuring 3DTV visual discomfort. Â© 2013 SPIE-IS&T.","3DTV; Adaptive Square Design; Pair comparison; Subjective experiment; Visual discomfort","The Society for Imaging Science and Technology (IS and T);The Society of Photo-Optical Instrumentation Engineers (SPIE);Qualcomm Inc.;IMAX;DepthQ 3D",,"24th IS and T/SPIE Stereoscopic Displays and Applications Conference, SD and A 2013","4 February 2013 through 6 February 2013","Burlingame, CA",97281,"Conference Paper","Scopus"
"Understanding widespread changes: A taxonomic study","Wang S., Lo D., Jiang X.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877269903&doi=10.1109%2fCSMR.2013.11&partnerID=40&md5=09d0e9e056910fb60a17eaed801dca37",2013,"Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR","Many active research studies in software engineering, such as detection of recurring bug fixes, detection of copy-and-paste bugs, and automated program transformation tools, are motivated by the assumption that many code changes (e.g., changing an identifier name) in software systems are widespread to many locations and are similar to one another. However, there is no study so far that actually analyzes widespread changes in software systems. Understanding the nature of widespread changes could empirically support the assumption, which provides insight to improve the research studies and related tools. Our study in this paper addresses such a need. We propose a semi-automated approach that recovers code changes involving widespread changes in software systems. We further manually analyze more than nine hundred widespread changes recovered from eight software systems and categorize them into 11 families. These widespread changes and their associated families help us understand better why these widespread changes are made. Â© 2013 IEEE.","taxonomic study; wdiespread changes","Reengineering Forum Industry Association;Universita degli Studi di Genova - DIBRIS",,"17th European Conference on Software Maintenance and Reengineering, CSMR 2013","5 March 2013 through 8 March 2013","Genova",96813,"Conference Paper","Scopus"
"Development of scientific software: A systematic mapping, a bibliometrics study, and a paper repository","Farhoodi R., Garousi V., Pfahl D., Sillito J.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880970555&doi=10.1142%2fS0218194013500137&partnerID=40&md5=50466e16f57cf0e054d1ebd2b4eb8aa0",2013,"International Journal of Software Engineering and Knowledge Engineering","Scientific and engineering research is heavily dependent on effective development and use of software artifacts. Many of these artifacts are produced by the scientists themselves, rather than by trained software engineers. To address the challenges in this area, a research community often referred to as ""Development of Scientific Software"" has emerged in the last few decades. As this research area has matured, there has been a sharp increase in the number of papers and results made available, and it has thus become important to summarize and provide an overview about those studies. Through a systematic mapping and bibliometrics study, we have reviewed 130 papers in this area. We present the results of our study in this paper. Also we have made the mapping data available on an online repository which is planned to be updated on a regular basis. The results of our study seem to suggest that many software engineering techniques and activities are being used in the development of scientific software. However, there is still a need for further exploration of the usefulness of specific software engineering techniques (e.g., regarding software maintenance, evolution, refactoring, re(v)-engineering, process and project management) in the scientific context. It is hoped that this article will help (new) researchers get an overview of the research space and help them to understand the trends in the area. Â© 2013 World Scientific Publishing Company.","Bibliometrics study; Development of scientific software; Paper repository; Systematic mapping",,,,,,,"Conference Paper","Scopus"
"On the adoption of MC/DC and control-flow adequacy for a tight integration of program testing and statistical fault localization","Jiang B., Zhai K., Chan W.K., Tse T.H., Zhang Z.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875215650&doi=10.1016%2fj.infsof.2012.10.001&partnerID=40&md5=3585ec99eee33c83c3a270e369f5a8d0",2013,"Information and Software Technology","Context: Testing and debugging consume a significant portion of software development effort. Both processes are usually conducted independently despite their close relationship with each other. Test adequacy is vital for developers to assure that sufficient testing effort has been made, while finding all the faults in a program as soon as possible is equally important. A tight integration between testing and debugging activities is essential. Objective: The paper aims at finding whether three factors, namely, the adequacy criterion to gauge a test suite, the size of a prioritized test suite, and the percentage of such a test suite used in fault localization, have significant impacts on integrating test case prioritization techniques with statistical fault localization techniques. Method: We conduct a controlled experiment to investigate the effectiveness of applying adequate test suites to locate faults in a benchmark suite of seven Siemens programs and four real-life UNIX utility programs using three adequacy criteria, 16 test case prioritization techniques, and four statistical fault localization techniques. We measure the proportion of code needed to be examined in order to locate a fault as the effectiveness of statistical fault localization techniques. We also investigate the integration of test case prioritization and statistical fault localization with postmortem analysis. Result: The main result shows that on average, it is more effective for a statistical fault localization technique to utilize the execution results of a MC/DC-adequate test suite than those of a branch-adequate test suite, and is in turn more effective to utilize the execution results of a branch-adequate test suite than those of a statement-adequate test suite. On the other hand, we find that none of the fault localization techniques studied can be sufficiently effective in suggesting fault-relevant statements that can fit easily into one debug window of a typical IDE. Conclusion: We find that the adequacy criterion and the percentage of a prioritized test suite utilized are major factors affecting the effectiveness of statistical fault localization techniques. In our experiment, the adoption of a stronger adequacy criterion can lead to more effective integration of testing and debugging. Â© 2012 Elsevier B.V. All rights reserved.","Adequacy criterion; Fault localization; MC/DC; Test case prioritization; Testing-debugging integration",,,,,,,"Conference Paper","Scopus"
"Construction and evolution of code generators: A model-driven and service-oriented approach","JÃ¶rges S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874502400&partnerID=40&md5=3a3325ad07091d9b49a12a24c6c4ca6f",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","Automatic code generation is an essential cornerstone of model-driven approaches to software development. It closes the gap that emerges when models are used to abstract from a concrete software system, and thus is to models what compilers are to high-level languages. Consequently, the simple and fast development of code generators is a key requirement of today's approaches to model-driven development, which are increasingly characterized by a strong focus on agility and domain-specificity. Currently, many techniques are available that support the specification and implementation of code generators, such as engines based on templates or rule-based transformations. All these techniques have in common that code generators are either directly programmed or described by means of textual specifications. This monograph presents Genesys, a general approach, which advocates the graphical development of code generators for arbitrary source and target languages, on the basis of models and services. In particular, it is designed to support incremental language development on arbitrary metalevels. The use of models allows building of code generators in a truly platform-independent and domain-specific way. Furthermore, models are amenable to formal verification methods such as model checking, which increase the reliability and robustness of the code generators. Services enable the reuse and integration of existing code generation frameworks and tools regardless of their complexity, and at the same time manifest as easy-to-use building blocks that facilitate agile development through quick interchangeability. Both models and services are reusable and thus form a growing repository for the fast creation and evolution of code generators. This book shows these and further advantages arising from the Genesys approach by means of a full-fledged reference implementation, which has been field-tested in a large number of case studies.. Â© 2013 Springer-Verlag Berlin Heidelberg.",,,,,,,,"Article","Scopus"
"An information foraging theory perspective on tools for debugging, refactoring, and reuse tasks","Fleming S.D., Scaffidi C., Piorkowski D., Burnett M., Bellamy R., Lawrance J., Kwan I.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876106539&doi=10.1145%2f2430545.2430551&partnerID=40&md5=c290c92ef9d3ccaf6060099f1e3b9b3a",2013,"ACM Transactions on Software Engineering and Methodology","Theories of human behavior are an important but largely untapped resource for software engineering research. They facilitate understanding of human developers' needs and activities, and thus can serve as a valuable resource to researchers designing software engineering tools. Furthermore, theories abstract beyond specific methods and tools to fundamental principles that can be applied to new situations. Toward filling this gap, we investigate the applicability and utility of Information Foraging Theory (IFT) for understanding information-intensive software engineering tasks, drawing upon literature in three areas: debugging, refactoring, and reuse. In particular, we focus on software engineering tools that aim to support information-intensive activities, that is, activities in which developers spend time seeking information. Regarding applicability, we consider whether and how the mathematical equations within IFT can be used to explain why certain existing tools have proven empirically successful at helping software engineers. Regarding utility, we applied an IFT perspective to identify recurring design patterns in these successful tools, and consider what opportunities for future research are revealed by our IFT perspective. Â© 2013 ACM.","Information foraging; Software maintenance",,,,,,,"Article","Scopus"
"Dictionary-based query recommendation for local code search","Ge X.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888142856&doi=10.1145%2f2508075.2522609&partnerID=40&md5=ffad300036836b6c43b5166061ccf4dd",2013,"SPLASH 2013 - Proceedings of the 2013 Companion Publication for Conference on Systems, Programming, and Applications: Software for Humanity","Local code search tools help developers efficiently find code snippets in the code base under development. The quality of the retrieved code largely depends on the quality of queries provided by developers. Manually synthesizing high-quality queries is a nontrivial task, partially because it places a cognitive burden on developers by requiring them memorize words used in the code base under search. To help developers synthesize better queries, this paper proposes a recommendation technique called MultiD that uses multiple dictionaries. We also report an ongoing study to evaluate the effectiveness of MultiD. Copyright Â© 2013 by the Association for Computing Machinery, Inc. (ACM).","IDE; Navigation; Recommender; Search; Tool","ACM SIGPLAN","Association for Computing Machinery","4th Annual ACM International Conference on Systems, Programming, Languages, and Applications: Software for Humanity, SPLASH 2013","26 October 2013 through 31 October 2013","Indianapolis, IN",100936,"Conference Paper","Scopus"
"A novel watermarking technique in data transmission between QR codes and database","Mohamed K., Sidi F., Jabar M.A., Ishak I.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897727743&doi=10.1109%2fICOS.2013.6735055&partnerID=40&md5=1e68368cd4cac37d94a913c3ea32ee14",2013,"2013 IEEE Conference on Open Systems, ICOS 2013","Digital data transmission has become prominent in this era of communication. Digital data such as text, still images, movies, or sounds can be wirelessly distributed through internet, intranet or digital broadcasting. A recent data management technology known as QR Code (Quick Response Code) allows data or information to be communicated wirelessly using a common portable device installed with its scanning software. Even though this new data management technology is easy to handle, it is also easy to be duplicated by unauthorized parties. Besides authentication and encryption techniques for security, using watermark will be an advantage to provide better protections for wireless data transmissions. This paper describes how these protections are being applied in ensuring data transmission for QR Codes safe from intruders without affecting the quality of data transmitted. A different technique of applying watermark is proposed in this paper. In this technique, due to its unique information, data of authentication is used for the watermark. The technique is also supported with the use of binary numbers for the encryption. To confirm the effectiveness of this protection technique, percentage of data loss and processing time for data transmissions are measured. From the experiment, results show that there is no data loss but only a slight delay in processing time is observed. This shows that the proposed technique of security protection is effective with only a negligible amount of time delay. Â© 2013 IEEE.","Data management; Data security; Data transmission; Database; QR code",,"IEEE Computer Society","2013 IEEE Conference on Open Systems, ICOS 2013","2 December 2013 through 4 December 2013","Kuching, Sarawak",103067,"Conference Paper","Scopus"
"A conceptual framework for open source software test process","Abdou T., Grogono P., Kamthan P.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870786576&doi=10.1109%2fCOMPSACW.2012.87&partnerID=40&md5=aaf5dfebf024e329475143e889436c4e",2012,"Proceedings - International Computer Software and Applications Conference","The broad acceptance and use of Open Source Software (OSS) has underscored the necessity of investigating the means of assuring their quality. With the aim of identifying an OSS test process, three well-known OSS projects, namely Apache HTTP server, Mozilla Web browser, and NetBeans IDE were studied. In these studies, three activities were found similar to the activities of the ISO/IEC Test Process Standard. However, major differences were observed in tasks related to each of the test process activities. To systematize the OSS test process, an Open Source Software Test Process Framework (OSS-TPF) is proposed. The alignment of OSS-TPF with the ISO/IEC Test Process Standard is illustrated. Â© 2012 IEEE.","Distributed development; Open source software; Software development process; Standards; Test process; Virtual community","IEEE;IEEE Computer Society",,"36th Annual IEEE International Computer Software and Applications Conference Workshops, COMPSACW 2012","16 July 2012 through 20 July 2012","Izmir",94271,"Conference Paper","Scopus"
"Two studies of framework-usage templates extracted from dynamic traces","Heydarnoori A., Czarnecki K., Binder W., Bartolomei T.T.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870511476&doi=10.1109%2fTSE.2011.77&partnerID=40&md5=a5be25d9a2f4b6e9d39bd4cceb60a306",2012,"IEEE Transactions on Software Engineering","Object-oriented frameworks are widely used to develop new applications. They provide reusable concepts that are instantiated in application code through potentially complex implementation steps such as subclassing, implementing interfaces, and calling framework operations. Unfortunately, many modern frameworks are difficult to use because of their large and complex APIs and frequently incomplete user documentation. To cope with these problems, developers often use existing framework applications as a guide. However, locating concept implementations in those sample applications is typically challenging due to code tangling and scattering. To address this challenge, we introduce the notion of concept-implementation templates, which summarize the necessary concept-implementation steps and identify them in the sample application code, and a technique, named FUDA, to automatically extract such templates from dynamic traces of sample applications. This paper further presents the results of two experiments conducted to evaluate the quality and usefulness of FUDA templates. The experimental evaluation of FUDA with 14 concepts in five widely used frameworks suggests that the technique is effective in producing templates with relatively few false positives and false negatives for realistic concepts by using two sample applications. Moreover, we observed in a user study with 28 programmers that the use of templates reduced the concept-implementation time compared to when documentation was used. Â© 2012 IEEE.","application programming interface (API); concept location; concept-implementation templates; dynamic analysis; feature identification; framework comprehension; framework documentation; Object-oriented application frameworks",,,,,,,"Article","Scopus"
"Combining relevancy and methodological quality into a single ranking for evidence-based medicine","Choi S., Ryu B., Yoo S., Choi J.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863983405&doi=10.1016%2fj.ins.2012.05.027&partnerID=40&md5=88131f3e095d6310fbfe219b8f845703",2012,"Information Sciences","Evidence-based medicine has recently received a large amount of attention in medical research. To help clinical practices use evidence-based medicine, it should be easy to find the best current evidence that is relevant to the clinical question and has high methodological quality. However, searching for relevant articles and appraising their validity is demanding work for most clinicians. We hypothesize that, through an effective design that addresses the two major aspects - relevance and quality - together with a ranking algorithm, search engines can automatically retrieve articles that are relevant to clinical questions and are based on valid evidence. The contribution of this study has two parts. First, we approach this problem by combining methodologies. After designing a suitable document query-relevance score and methodological quality score, we combined them using various fusion methods. The result was a twofold increase in the mean average precision. Second, for correct evaluation, we built a test collection using a preexisting reliable database, the Cochrane Reviews, which allowed robust and comprehensive evaluation. Â© 2012 Elsevier Inc. All rights reserved.","Classification; Document quality; Evidence-based medicine; Ranking",,,,,,,"Article","Scopus"
"Increasing clone maintenance support by unifying clone detection and refactoring activities","Tairas R., Gray J.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865722976&doi=10.1016%2fj.infsof.2012.06.011&partnerID=40&md5=e158faafab6e53acd4439e4979bb7e0e",2012,"Information and Software Technology","Context: Clone detection tools provide an automated mechanism to discover clones in source code. On the other side, refactoring capabilities within integrated development environments provide the necessary functionality to assist programmers in refactoring. However, we have observed a gap between the processes of clone detection and refactoring. Objective: In this paper, we describe our work on unifying the code clone maintenance process by bridging the gap between clone detection and refactoring. Method: Through an Eclipse plug-in called CeDAR (Clone Detection, Analysis, and Refactoring), we forward clone detection results to the refactoring engine in Eclipse. In this case, the refactoring engine is supplied with information about the detected clones to which it can then determine those clones that can be refactored. We describe the extensions to Eclipse's refactoring engine to allow clones with additional similarity properties to be refactored. Results: Our evaluation of open source artifacts shows that this process yields considerable increases in the instances of clone groups that may be suggested to the programmer for refactoring within Eclipse. Conclusion: By unifying the processes of clone detection and refactoring, in addition to providing extensions to the refactoring engine of an IDE, the strengths of both processes (i.e., more significant detection capabilities and an established framework for refactoring) can be garnered. Â© 2012 Elsevier B.V. All rights reserved.","Code clones; Maintenance; Refactoring",,,,,,,"Conference Paper","Scopus"
"The state-of-the art tools to visualize software [O estado-da-arte das ferramentas de visualizaÌ§cÌƒao de software]","Petrillo F., Pimenta M., Dal Sasso Freitas C.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886681175&partnerID=40&md5=29d0b66e70e98975421c3dad8b06b4f3",2012,"15th Ibero-American Conference on Software Engineering, CIbSE 2012","Along the recent years, several visualization techniques have been developed to address problems of software engineering, such as comprehension, maintenance and software evolution. Experimentally, these techniques are evaluated through their implementation in visualization tools. However, there are no consistent reports about the current condition of these tools. The goal of this paper is to evaluate the current situation of the implementations of several software visualization techniques described in the literature. We analyzed 52 tools, compiling information about them, and a surprising result is the observation of the low stability of the implementations evaluated: 60% (31 of 52) of the tools do not even have an implementation available, making their use impractical.",,,,"15th Ibero-American Conference on Software Engineering, CIbSE 2012","24 April 2012 through 27 April 2012","Buenos Aires",100510,"Conference Paper","Scopus"
"Programmer information needs after memory failure","Parnin C., Rugaber S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864978899&partnerID=40&md5=965fba05976368f3400a162472bb4e93",2012,"IEEE International Conference on Program Comprehension","Despite its vast capacity and associative powers, the human brain does not deal well with interruptions. Particularly in situations where information density is high, such as during a programming task, recovering from an interruption requires extensive time and effort. Although modern program development environments have begun to recognize this problem, none of these tools take into account the brain's structure and limitations. In this paper, we present a conceptual framework for understanding the strengths and weaknesses of human memory, particularly with respect to it ability to deal with work interruptions. The framework explains empirical results obtained from experiments in which programmers were interrupted while working. Based on the framework, we discuss programmer information needs that development tools must satisfy and suggest several memory aids such tools could provide. We also describe our prototype implementation of these memory aids. Â© 2012 IEEE.",,"IEEE;IEEE Computer Society;Technical Council on Software Engineering (TCSE)",,"2012 20th IEEE International Conference on Program Comprehension, ICPC 2012","11 June 2012 through 13 June 2012","Passau",91930,"Conference Paper","Scopus"
"Clones: What is that smell?","Rahman F., Bird C., Devanbu P.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863723273&doi=10.1007%2fs10664-011-9195-3&partnerID=40&md5=50dc3cb47968914f52e594d42ff7bc35",2012,"Empirical Software Engineering","Clones are generally considered bad programming practice in software engineering folklore. They are identified as a bad smell (Fowler et al. 1999) and a major contributor to project maintenance difficulties. Clones inherently cause code bloat, thus increasing project size and maintenance costs. In this work, we try to validate the conventional wisdom empirically to see whether cloning makes code more defect prone. This paper analyses the relationship between cloning and defect proneness. For the four medium to large open source projects that we studied, we find that, first, the great majority of bugs are not significantly associated with clones. Second, we find that clones may be less defect prone than non-cloned code. Third, we find little evidence that clones with more copies are actually more error prone. Fourth, we find little evidence to support the claim that clone groups that span more than one file or directory are more defect prone than collocated clones. Finally, we find that developers do not need to put a disproportionately higher effort to fix clone dense bugs. Our findings do not support the claim that clones are really a ""bad smell"" (Fowler et al. 1999). Perhaps we can clone, and breathe easily, at the same time. Â© Springer Science+Business Media, LLC 2011.","Empirical software engineering; Software clone; Software evolution; Software maintenance; Software quality",,,,,,,"Conference Paper","Scopus"
"Improving early detection of software merge conflicts","GuimarÃ£es M.L., Silva A.R.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864231591&doi=10.1109%2fICSE.2012.6227180&partnerID=40&md5=9ff2f7b5e249af74278063b114f15b71",2012,"Proceedings - International Conference on Software Engineering","Merge conflicts cause software defects which if detected late may require expensive resolution. This is especially true when developers work too long without integrating concurrent changes, which in practice is common as integration generally occurs at check-in. Awareness of others' activities was proposed to help developers detect conflicts earlier. However, it requires developers to detect conflicts by themselves and may overload them with notifications, thus making detection harder. This paper presents a novel solution that continuously merges uncommitted and committed changes to create a background system that is analyzed, compiled, and tested to precisely and accurately detect conflicts on behalf of developers, before check-in. An empirical study confirms that our solution avoids overloading developers and improves early detection of conflicts over existing approaches. Similarly to what happened with continuous compilation, this introduces the case for continuous merging inside the IDE. Â© 2012 IEEE.","awareness; continuous merging; merge conflicts; version control","IEEE Computer Society;ACM;University of Zurich (UZH), Department of Informatics;Technical Council on Software Engineering (TCSE);Special Interest Group on Software Engineering (SIGSOFT);SI-SE",,"34th International Conference on Software Engineering, ICSE 2012","2 June 2012 through 9 June 2012","Zurich",91326,"Conference Paper","Scopus"
"Test quality feedback improving effectivity and efficiency of unit testing","Perscheid M., Cassou D., Hirschfeld R.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861213188&doi=10.1109%2fC5.2012.7&partnerID=40&md5=3a26fee28580d7a8de22c77ab27d841b",2012,"Proceedings - 10th International Conference on Creating, Connecting and Collaborating through Computing, C5 2012","Writing unit tests for a software system enhances the confidence that a system works as expected. Since time pressure often prevents a complete testing of all application details developers need to know which new tests the system requires. Developers also need to know which existing tests take the most time and slow down the whole development process. Missing feedback about less tested functionality and reasons for long running test cases make it, however, harder to create a test suite that covers all important parts of a software system in a minimum of time. As a result a software system may be inadequately tested and developers may test less frequently. Our approach provides test quality feedback to guide developers in identifying missing tests and correcting low-quality tests. We provide developers with a tool that analyzes test suites with respect to their effectivity (e.g., missing tests) and efficiency (e.g., time and memory consumption). We implement our approach, named Path Map, as an extended test runner within the Squeak Smalltalk IDE and demonstrate its benefits by improving the test quality of representative software systems. Â© 2012 IEEE.","Dynamic Analysis; Test Quality Feedback; Unit Tests","Kyoto University Global COE Program;Viewpoints Research Institute;USC Institute for Creative Technologies",,"10th International Conference on Creating, Connecting and Collaborating through Computing, C5 2012","18 January 2012 through 20 January 2012","Playa Vista, CA",89868,"Conference Paper","Scopus"
"Identifier-based context-dependent API method recommendation","Heinemann L., Bauer V., Herrmannsdoerfer M., Hummel B.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860514537&doi=10.1109%2fCSMR.2012.14&partnerID=40&md5=497b9f412fdfcf599703e21aeb7a8f12",2012,"Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR","Reuse recommendation systems support the developer by suggesting useful API methods, classes or code snippets based on code edited in the IDE. Existing systems based on structural information, such as type and method usage, are not effective in case of general purpose types such as String. To alleviate this, we propose a recommendation system based on identifiers that utilizes the developer's intention embodied in names of variables, types and methods. We investigate the impact of several variation points of our recommendation algorithm and evaluate the approach for recommending methods from the Java and Eclipse APIs in 9 open source systems. Furthermore, we compare our recommendations to those of a structure-based recommendation system and describe a metric for predicting the expected precision of a recommendation. Our findings indicate that our approach performs significantly better than the structure-based approach. Â© 2012 IEEE.","Data mining; Identifier; Recommendation system; Software reuse","Reengineering Forum (REF);University of Szeged;FrontEndART Software Ltd.",,"2012 16th European Conference on Software Maintenance and Reengineering, CSMR 2012","27 March 2012 through 30 March 2012","Szeged",89560,"Conference Paper","Scopus"
"Training of requirements analysis modeling with UML-based prototype generation tool","Ogata S., Matsuura S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858323898&doi=10.1145%2f2134254.2134273&partnerID=40&md5=d35b7f25489333a43dc9135766cd86af",2012,"Proceedings of the 5th India Software Engineering Conference, ISEC'12","To develop high quality software products efficiently, the requirements analysis is the most important phase in a software development process, but it is generally difficult to achieve the goal sufficiently. To master requirements analysis skills, the training of defining a desirable requirements specification should be conducted to the education at the university to train students to become high quality systems engineers. We have proposed a method of model-driven requirements analysis using Unified Modeling Language. Based on our requirement analysis method and the supporting tool, this paper proposes a way how to train the modeling ability of graduate students in our university. Â© 2012 ACM.","prototyping; training of requirements analysis; UML","IBM;Infosys;intel;Microsoft Research;SIEMENS",,"5th India Software Engineering Conference, ISEC 2012","22 February 2012 through 25 February 2012","Kanpur",89003,"Conference Paper","Scopus"
"How we refactor, and how we know it","Murphy-Hill E., Parnin C., Black A.P.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856553512&doi=10.1109%2fTSE.2011.41&partnerID=40&md5=f8be859c4147bef9a7df678a1bdb5e77",2012,"IEEE Transactions on Software Engineering","Refactoring is widely practiced by developers, and considerable research and development effort has been invested in refactoring tools. However, little has been reported about the adoption of refactoring tools, and many assumptions about refactoring practice have little empirical support. In this paper, we examine refactoring tool usage and evaluate some of the assumptions made by other researchers. To measure tool usage, we randomly sampled code changes from four Eclipse and eight Mylyn developers and ascertained, for each refactoring, if it was performed manually or with tool support. We found that refactoring tools are seldom used: 11 percent by Eclipse developers and 9 percent by Mylyn developers. To understand refactoring practice at large, we drew from a variety of data sets spanning more than 39,000 developers, 240,000 tool-assisted refactorings, 2,500 developer hours, and 12,000 version control commits. Using these data, we cast doubt on several previously stated assumptions about how programmers refactor, while validating others. Finally, we interviewed the Eclipse and Mylyn developers to help us understand why they did not use refactoring tools and to gather ideas for future research. Â© 2006 IEEE.","floss refactoring; Refactoring; refactoring tools; root-canal refactoring",,,,,,,"Article","Scopus"
"Automated analysis of textual use-cases: Does NLP components and pipelines matter?","Kulkarni N., Parachuri D., Dasa M., Kumar A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874598904&doi=10.1109%2fAPSEC.2012.83&partnerID=40&md5=ee5b805b987467a013318947a7b7a2cf",2012,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC","Significant time is spent by practitioners to analyze use-cases written in Natural Language (NL). With only prescriptive templates to describe complex scenarios, common errors like misinterpretation and oversight can have costly consequence later during system development. A semi-automatic approach based on NL processing can reduce the time spent on requirement analysis and bootstrap design activity. However, linguistic community has adopted pipeline processing to handle NL ambiguities where several sequential tasks aid in solving a bigger task. Choosing NL processing techniques depends on the domain and task to accomplish. As use-cases are domain specific it is crucial to identify suitable pipelines to process them. This is highlighted in our evaluation of two pipelines consisting of syntactic and semantic techniques on use-cases found in theory and practice. We believe, the promising results has opened up the need for exploring more task specific NLP pipelines and evaluation thereof. Â© 2012 IEEE.","Natural Language Processing; Requirement Engineering; Use Cases","ACM Hong Kong Chapter;IEEE Hong Kong Section Computer Society Chapter","IEEE Computer Society","19th Asia-Pacific Software Engineering Conference, APSEC 2012","4 December 2012 through 7 December 2012","Hong Kong",95873,"Conference Paper","Scopus"
"The Role of Visualization in Computer Science Education","Fouh E., Akbar M., Shaffer C.A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860123822&doi=10.1080%2f07380569.2012.651422&partnerID=40&md5=7173b08c163ba9a0755c20a055f98c25",2012,"Computers in the Schools","Computer science core instruction attempts to provide a detailed understanding of dynamic processes such as the working of an algorithm or the flow of information between computing entities. Such dynamic processes are not well explained by static media such as text and images, and are difficult to convey in lecture. The authors survey the history of visualization in computer science education, focusing on artifacts that have a documented positive educational assessment. Changes in how computing technology has affected the development and uptake of such visualization artifacts in computer science education, and how recent technology changes are leading to progress in developing online hypertextbooks are then discussed. Â© 2012 Taylor and Francis Group, LLC.","algorithm visualization; data structure visualization; eTextbooks; hypertextbooks; program visualization",,,,,,,"Article","Scopus"
"Search-based composition, streaming and playback of video archive content","Johansen D., Halvorsen P., Johansen H., Riiser H., Gurrin C., Olstad B., Griwodz C., Kvalnes A., Hurley J., Kupka T.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883694471&doi=10.1007%2fs11042-011-0847-5&partnerID=40&md5=0ba1bc58a9739542448daecbf50e8934",2012,"Multimedia Tools and Applications","Locating content in existing video archives is both a time and bandwidth consuming process since users might have to download and manually watch large portions of superfluous videos. In this paper, we present two novel prototypes using an Internet based video composition and streaming system with a keyword-based search interface that collects, converts, analyses, indexes, and ranks video content. At user requests, the system can automatically sequence out portions of single videos or aggregate content from multiple videos to produce a single, personalized video stream on-the-fly. Â© The Author(s) 2011.","Personalized composition; Segmented adaptive HTTP streaming; Video search engines",,"Kluwer Academic Publishers",,,,,"Article","Scopus"
"GUI tools and generated code: Refactoring to reveal intent","Wainer M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871978553&partnerID=40&md5=2ac49db9f77a7761a66ea67b858019d4",2011,"Proceedings of the ISCA 26th International Conference on Computers and Their Applications, CATA 2011","High quality graphical user interfaces (GUIs), expected of today's applications, can be difficult and tedious to develop. Visual GUI build tools are helpful in this regard, providing a more direct approach to defining interface components than typical text based integrated development environments (IDE). These tools generate code which still must be integrated and maintained with the rest of the application code base. Generated code is observed to contain code smells, such as duplication, which mask the higher level intention of the design. Code refinement often accomplished through a traditional IDE, involve reverse engineering, modifications and refactoring (to evolve the design and remove code smells). As visual tools have improved, they are increasingly able to work with GUI code which has been refined elsewhere giving developers more options to mix tools as the project progresses. Refactorings specific to GUI tool generated code are considered through a focused example. To fully integrate the generated code into the code base, the higher level intent lost by the build tool should be reintroduced into the source code. This will generally be a multistep process which might be accomplished in a variety of ways. A new informal notation is introduced to assist developers in mapping out how to evolve their code to raise its abstraction level and make its intent more obvious. This is of special importance in agile development methodologies where the code itself is seen as the major expression of design. Copyright Â© 2011 by the International Society for Computers and Their Applications (ISCA).","Agile; Code smells; HCI; Refactoring; Reverse engineering; Software","Int. Soc. Comput. Their Appl. (ISCA)",,"26th International Conference on Computers and Their Applications, CATA 2011","23 March 2011 through 25 March 2011","New Orleans, LA",94796,"Article","Scopus"
"A two-stage service matching algorithm based on genetic algorithm","Wang B., Ma Y., Duan Y.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052795442&partnerID=40&md5=0b53a63bbe153b06a2003d2cd41f8978",2011,"Journal of Computational Information Systems","Dynamic service composition is an important research area which aims at supporting reusability and interoperability for service composition. And service matching is one of the important steps. Completely intelligent service composition is extremely complicated. Therefore, many researches focus on local matching method. But this idea is usually not practical because the parameter and situation will be more complex in reality than in experiment. Others propose global service match algorithm which is effective, but they solve the problem just from QoS point of view which is not intelligent enough. This paper proposes a two-stage service match algorithm. In the first stage, we improved classic service semantic matching algorithm to match the interface parameter with service function. And then, genetic algorithm with multi-target and QoS constraints is used to global semantic match. Experiment show this method is available and effective. Â© 2005 by Binary Information Press.","Genetic algorithm; Service matching; Two-stage matching",,,,,,,"Article","Scopus"
"Recommending API methods based on identifier contexts","Heinemann L., Hummel B.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960567215&doi=10.1145%2f1985429.1985430&partnerID=40&md5=a529fdbd6eca27b844fd2b42d4ecf728",2011,"Proceedings - International Conference on Software Engineering","Reuse recommendation systems suggest functions or code snippets that are useful for the programming task at hand within the IDE. These systems utilize different aspects from the context of the cursor position within the source file being edited for inferring which functionality is needed next. Current approaches are based on structural information like inheritance relations or type/method usages. We propose a novel method that utilizes the knowledge embodied in the identifiers as a basis for the recommendation of API methods. This approach has the advantage that relevant recommendations can also be made in cases where no methods are called in the context or if contexts use distinct but semantically similar types or methods. First experiments show, that the correct method is recommended in about one quarter to one third of the cases. Â© 2011 ACM.","recommendation system; software reuse","ACM SIGSOFT;IEEE CS",,"3rd International Workshop on Search-Driven Development: Users, Infrastructure, Tools, and Evaluation, SUITE 2011, Co-located with ICSE 2011","28 May 2011 through 28 May 2011","Waikiki, Honolulu, HI",85672,"Conference Paper","Scopus"
"SMaRT: A Workbench for reporting the monitorability of services from SLAs","Foster H., Spanoudakis G.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959584701&doi=10.1145%2f1985394.1985400&partnerID=40&md5=523d8c236120892aafdfa018b5ae335c",2011,"Proceedings - International Conference on Software Engineering","Service Level Agreements (SLAs) for Software Services aim to clearly identify the service level commitments established between service requesters and providers. A dynamic configuration for the monitoring of these SLAs provides the opportunity for service monitor providers to offer and release monitoring infrastructures for different types of services. Whilst there has been work on automating this monitor matching and configuration, additional support may be needed in the negotiation and provision of monitors for which the current monitoring infrastructure does not provide suitable SLA term monitors. In this paper we describe an approach to effectively report and assist service monitoring support groups in managing this provision. The approach described is illustrated with mechanical support in the form of a SMaRT Workbench Eclipse IDE plug-in for reporting on the monitorability of SLAs for service monitoring infrastructures. Â© 2011 ACM.","Monitorability assessment; Monitoring capabilities; Run-time SLA monitoring","ACM SIGSOFT;IEEE CS",,"3rd International Workshop on Principles of Engineering Service-Oriented Systems, PESOS 2011, Co-located with ICSE 2011","23 May 2011 through 24 May 2011","Waikiki, Honolulu, HI",85275,"Conference Paper","Scopus"
"Intelligent software development environments: Integrating natural language processing with the eclipse platform","Witte R., Sateli B., Khamis N., Rilling J.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957950356&doi=10.1007%2f978-3-642-21043-3-49&partnerID=40&md5=8e84a1191dbb2881010b16d4e3532669",2011,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","Software engineers need to be able to create, modify, and analyze knowledge stored in software artifacts. A significant amount of these artifacts contain natural language, like version control commit messages, source code comments, or bug reports. Integrated software development environments (IDEs) are widely used, but they are only concerned with structured software artifacts - they do not offer support for analyzing unstructured natural language and relating this knowledge with the source code. We present an integration of natural language processing capabilities into the Eclipse framework, a widely used software IDE. It allows to execute NLP analysis pipelines through the Semantic Assistants framework, a service-oriented architecture for brokering NLP services based on GATE. We demonstrate a number of semantic analysis services helpful in software engineering tasks, and evaluate one task in detail, the quality analysis of source code comments. Â© 2011 Springer-Verlag.",,"Canadian Artificial Intelligence Association (CAIAC);Memorial University;Compusult;Palomino System Innovations Inc.;University of Regina",,"24th Canadian Conference on Artificial Intelligence, AI 2011, Collocated with the 37th Graphics Interface Conference, GI 2011 and 8th Canadian Conference on Computer and Robot Vision, CRV 2011","25 May 2011 through 27 May 2011","St. John's, NL",85054,"Conference Paper","Scopus"
"Not my bug! and other reasons for software bug report reassignments","Guo P.J., Zimmermann T., Nagappan N., Murphy B.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955128266&doi=10.1145%2f1958824.1958887&partnerID=40&md5=e970bfe1e04289c2ed30a4fc9e24faea",2011,"Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW","Bug reporting/fixing is an important social part of the software development process. The bug-fixing process inherently has strong inter-personal dynamics at play, especially in how to find the optimal person to handle a bug report. Bug report reassignments, which are a common part of the bug-fixing process, have rarely been studied. In this paper, we present a large-scale quantitative and qualitative analysis of the bug reassignment process in the Microsoft Windows Vista operating system project. We quantify social interactions in terms of both useful and harmful reassignments. For instance, we found that reassignments are useful to determine the best person to fix a bug, contrary to the popular opinion that reassignments are always harmful. We categorized five primary reasons for reassignments: finding the root cause, determining ownership, poor bug report quality, hard to determine proper fix, and workload balancing. We then use these findings to make recommendations for the design of more socially-aware bug tracking systems that can overcome some of the inefficiencies we observed in our study. Copyright 2011 ACM.","Bug reassignment; Bug tracking; Bug triaging","SIGCHI",,"ACM 2011 Conference on Computer Supported Cooperative Work, CSCW 2011","19 March 2011 through 23 March 2011","Hangzhou",84592,"Conference Paper","Scopus"
"AskHERMES: An online question answering system for complex clinical questions","Cao Y., Liu F., Simpson P., Antieau L., Bennett A., Cimino J.J., Ely J., Yu H.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952770126&doi=10.1016%2fj.jbi.2011.01.004&partnerID=40&md5=666f1d30c9842fdabb0341dde7c155dc",2011,"Journal of Biomedical Informatics","Objective: Clinical questions are often long and complex and take many forms. We have built a clinical question answering system named AskHERMES to perform robust semantic analysis on complex clinical questions and output question-focused extractive summaries as answers. Design: This paper describes the system architecture and a preliminary evaluation of AskHERMES, which implements innovative approaches in question analysis, summarization, and answer presentation. Five types of resources were indexed in this system: MEDLINE abstracts, PubMed Central full-text articles, eMedicine documents, clinical guidelines and Wikipedia articles. Measurement: We compared the AskHERMES system with Google (Google and Google Scholar) and UpToDate and asked physicians to score the three systems by ease of use, quality of answer, time spent, and overall performance. Results: AskHERMES allows physicians to enter a question in a natural way with minimal query formulation and allows physicians to efficiently navigate among all the answer sentences to quickly meet their information needs. In contrast, physicians need to formulate queries to search for information in Google and UpToDate. The development of the AskHERMES system is still at an early stage, and the knowledge resource is limited compared with Google or UpToDate. Nevertheless, the evaluation results show that AskHERMES' performance is comparable to the other systems. In particular, when answering complex clinical questions, it demonstrates the potential to outperform both Google and UpToDate systems. Conclusions: AskHERMES, available at http://www.AskHERMES.org, has the potential to help physicians practice evidence-based medicine and improve the quality of patient care. Â© 2011 Elsevier Inc.","Answer presentation; Clinical question answering; Passage retrieval; Question analysis; Summarization",,,,,,,"Article","Scopus"
"ISENGARD: An infrastructure for supporting e-Science and grid application development","Kurniawan D., Abramson D.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951765106&doi=10.1002%2fcpe.1662&partnerID=40&md5=fcdfc451bfa2540d9c306bb88a6b2535",2011,"Concurrency Computation Practice and Experience","Grid computing facilitates the aggregation and coordination of resources that are distributed across multiple administrative domains for large-scale and complex e-Science experiments. Writing, deploying, and testing grid applications over highly heterogeneous and distributed resources are complex and challenging. The process requires grid-enabled programming tools that can handle the complexity and scale of the infrastructure. However, while a large amount of research has been undertaken into grid middleware, little work has been directed specifically at the area of grid application development tools. This paper presents the design and implementation of ISENGARD, an infrastructure for supporting e-Science and grid application development. ISENGARD provides services, tools, and APIs that simplify grid software development. Â© 2010 John Wiley & Sons, Ltd.","application development; grid computing; IDE",,,,,,,"Article","Scopus"
"The problem of understanding","Porzel R.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650839790&doi=10.1007%2f978-3-642-17396-7-1&partnerID=40&md5=51504d225ee3f33291339cf744665c00",2011,"Cognitive Technologies","Reading the Chung Tzu quote presented above one might initially get the impression that it would suffice to specify the meaning of words in some way then string those meanings together and the problem of language understanding would be solved once and for all. However, as in his paradoxical remark there is a catch which has confounded researchers for some time and thwarted numerous attempts in artificial intelligence research to build systems that interact with humans via natural language. This catch - as will be claimed in this book - has its origin in the inherent context-dependency of natural language use or the use for any other human communicative signals such as gestures or facial expressions for that matter. Â© 2011 Springer-Verlag.",,,,,,,,"Conference Paper","Scopus"
"A feature modelling framework for ubiquitous embodied learning games","Zualkernan I.A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052764352&doi=10.3233%2f978-1-60750-831-1-198&partnerID=40&md5=49c98375066c146eee3fb5a3d8c80c74",2011,"Frontiers in Artificial Intelligence and Applications","Embodied Ubiquitous Learning Games (UELG) represent a new genre of technology-enhanced education in which learners interact with augmented physical environment to learn. The embodiment in such games comes from instrumented clothes based on e-Textiles incorporating sensors, actuators and low-power wireless modules. Designing and building such ubiquitous systems requires a complex interplay of conventional and embedded hardware and software co-design that includes novel elements like e-Textiles. In order to reduce the cost and shorten the development life-cycle this paper proposes the use of Software Product Lines (SPL). Towards this end, this paper borrows ideas from Architecture to present and apply a framework for constructing feature models for a Software Product Line for ULEG. The framework explicitly uses different types of features related to pedagogy, technology and domain of learning. A family of UELG based on a combination of problem posing and problem solving is used as an example to explain various components of the framework. Â© 2011 The authors and IOS Press. All rights reserved.","e-Textiles; Embodied Learning; Feature Models; Game-based Learning; Software Product Line; Ubiquitous Learning",,"IOS Press",,,,,"Article","Scopus"
"Knowledge Annotation: Making Implicit Knowledge Explicit","Dingli A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885633863&doi=10.1007%2f978-3-642-20323-7&partnerID=40&md5=dd55f5777694e33e6fe93638e0d079af",2011,"Intelligent Systems Reference Library","[No abstract available]",,,"Springer Science and Business Media Deutschland GmbH",,,,,"Article","Scopus"
"IDE 2.0: Collective intelligence in software development","Bruch M., Bodden E., Monperrus M., Mezini M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951595779&doi=10.1145%2f1882362.1882374&partnerID=40&md5=9f3918f34c6c7dd73e2749347374c6a7",2010,"Proceedings of the FSE/SDP Workshop on the Future of Software Engineering Research, FoSER 2010","Today's Integrated Development Environments (IDEs) only integrate the tools and knowledge of a single user and workstation. This neglects the fact that the way in which we develop and maintain a piece of software and interact with our IDE provides a rich source of information that can help ourselves and other programmers to avoid mistakes in the future, or improve productivity otherwise. We argue that, in the near future, IDEs will undergo a revolution that will significantly change the way in which we develop and maintain software, through integration of collective intelligence, the knowledge of the masses. We describe the concept of an IDE based on collective intelligence and discuss three example instantiations of such IDEs. Copyright 2010 ACM.","Collective intelligence; IDE 2.0; Software development","ACM Special Interest Group on Software Engineering (SIGSOFT)",,"FSE/SDP Workshop on the Future of Software Engineering Research, FoSER 2010","7 November 2010 through 11 November 2010","Santa Fe, NM",83792,"Conference Paper","Scopus"
"Speculative analysis: Exploring future development states of software","Brun Y., Holmesy R., Ernst M.D., Notkin D.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951608218&doi=10.1145%2f1882362.1882375&partnerID=40&md5=5aaf40703e56e2fe5af123ef65699142",2010,"Proceedings of the FSE/SDP Workshop on the Future of Software Engineering Research, FoSER 2010","Most software tools and environments help developers analyze the present and past development states of their software systems. Few approaches have investigated the potential consequences of future actions the developers may perform. The commoditization of hardware, multi-core architectures, and cloud computing provide new potential for delivering apparently-instantaneous feedback to developers, informing them of the effects of changes that they may be considering to the software. For example, modern IDEs often provide ""quick fix"" suggestions for resolving compilation errors. Developers must scan this list and select the option they think will resolve the problem. Instead, we propose that the IDE should speculatively perform each of the suggestions in the background and provide information that helps developers select the best option for the given context. We believe the feedback enabled by speculative operations can improve developer productivity and software quality. Copyright 2010 ACM.","Developer awareness; IDE; Quick fix; Recommender system; Speculation; Version control","ACM Special Interest Group on Software Engineering (SIGSOFT)",,"FSE/SDP Workshop on the Future of Software Engineering Research, FoSER 2010","7 November 2010 through 11 November 2010","Santa Fe, NM",83792,"Conference Paper","Scopus"
"Automatic programming assessment and test data generation: A review on its approaches","Romli R., Sulaiman S., Zamli K.Z.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049378289&doi=10.1109%2fITSIM.2010.5561488&partnerID=40&md5=d67f594dda7185bc1e746723293f25de",2010,"Proceedings 2010 International Symposium on Information Technology - System Development and Application and Knowledge Society, ITSim'10","Automatic programming assessment has recently become an important method in assisting lecturers and instructors of programming courses to automatically mark and grade students' programming exercises as well as to provide useful feedbacks on students' programming solutions. As part of the method, test data generation process plays as an integral part to perform a dynamic testing on students' programs. To date, various automated methods for test data generation particularly in software testing field are available. Unfortunately, they are seldom used in the context of automatic programming assessment research area. Nevertheless, there have been limited studies taking a stab to integrate both of them due to more useful features and to include a better quality program testing coverage. Thus, this paper provides a review on approaches that have been implemented in various studies with regard to automatic programming assessment, test data generation and integration of both of them. This review is aimed at gathering different techniques that have been employed to forward an interested reader the starting points for finding further information regarding its trends. In addition, the result of the review reveals the main gap that exists within the context of the considered areas which contributes to our main research topic of interest. Â© 2010 IEEE.","Automatic programming assessment; Program testing; Test data generation","IEEE",,"2010 International Symposium on Information Technology, ITSim'10","15 June 2010 through 17 June 2010","Kuala Lumpur",81915,"Conference Paper","Scopus"
"FoodManager: A cooking, eating and appliance controlling support system for the elderly","Iglesias R., Ibarguren I., De Segura N.G., Ugalde J., Coello L., Iturburu M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956277869&doi=10.1145%2f1839294.1839340&partnerID=40&md5=023581378baecf8f97d96522db01afe9",2010,"ACM International Conference Proceeding Series","These days, many support systems are being developed to improve independence and quality of life of elderly and impaired people at home. Most of them have been hitherto focused on providing home healthcare-related services, and little attention has been paid to cooking and eating activities. On the one hand, the role of supporting eating activities is likely to become increasingly important. Since old age is often associated with memory impairments, it would be useful to provide them with menus including a variety of healthier meals. Furthermore, it is of paramount importance that these suggested meals are built up based on disease pathologies, health condition and user preferences. On the other hand, comprehensive cooking guidelines and food and shopping list handling are also needed. With regard to cooking directions, it seems desirable that users do not need to manage complex household appliances like the oven. This can be achievable thanks to the technologies developed in the area of Home Automation. In this paper, a cooking support system, namely FoodManager, is discussed. This was designed and developed to deal with all the requirements described above. Its interface design and navigation was specially designed for the elderly. Testing performed with ten elderly people (ages from 58 to 81) provided some understanding about its usability and simplicity to use. Copyright Â© 2010 ACM.","Ambient assisted living; Cooking; Eating; Elderly people; Smart","Natl. Sci. Found. (NSF), CISE-Hum. Cent. Comput.;Univ. Cent. Greece (Comput. Sci. Biomed. Informatics Dept);University of the Aegean;Technol. Educ. Inst. Athens (Dep. Informatics);DEMOKRITOS National Center for Scientific Research (NCSR)",,"3rd International Conference on PErvasive Technologies Related to Assistive Environments, PETRA 2010","23 June 2010 through 25 June 2010","Pythagorion, Samos",81604,"Conference Paper","Scopus"
"Semi-formal transformation of secure business processes into analysis class and use case models: An MDA approach","RodrÃ­guez A., GuzmÃ¡n I.G.-R.d., FernÃ¡ndez-Medina E., Piattini M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953671395&doi=10.1016%2fj.infsof.2010.03.015&partnerID=40&md5=caf18266cb5c4d729cb310b0213f6257",2010,"Information and Software Technology","Context: Model-Driven Development (MDD) is an alternative approach for information systems development. The basic underlying concept of this approach is the definition of abstract models that can be transformed to obtain models near implementation. One fairly widespread proposal in this sphere is that of Model Driven Architecture (MDA). Business process models are abstract models which additionally contain key information about the tasks that are being carried out to achieve the company's goals, and two notations currently exist for modelling business processes: the Unified Modelling Language (UML), through activity diagrams, and the Business Process Modelling Notation (BPMN). Objective: Our research is particularly focused on security requirements, in such a way that security is modelled along with the other aspects that are included in a business process. To this end, in earlier works we have defined a metamodel called secure business process (SBP), which may assist in the process of developing software as a source of highly valuable requirements (including very abstract security requirements), which are transformed into models with a lower abstraction level, such as analysis class diagrams and use case diagrams through the approach presented in this paper. Method: We have defined all the transformation rules necessary to obtain analysis class diagrams and use case diagrams from SBP, and refined them through the characteristic iterative process of the action-research method. Results: We have obtained a set of rules and a checklist that make it possible to automatically obtain a set of UML analysis classes and use cases, starting from SBP models. Our approach has additionally been applied in a real environment in the area of the payment of electrical energy consumption. Conclusions: The application of our proposal shows that our semi-automatic process can be used to obtain a set of useful artifacts for software development processes. Â© 2010 Elsevier B.V. All rights reserved.","BPMN; MDA; Secure business processes; UML",,,,,,,"Article","Scopus"
"Code bubbles: Rethinking the user interface paradigm of integrated development environments","Bragdon A., Reiss S.P., Zeleznik R., Karumuri S., Cheung W., Kaplan J., Coleman C., Adeputra F., LaViola Jr. J.J.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954725192&doi=10.1145%2f1806799.1806866&partnerID=40&md5=f036ace8d1c1f470c11afdc1f9f75477",2010,"Proceedings - International Conference on Software Engineering","Today's integrated development environments (IDEs) are hampered by their dependence on files and file-based editing. We propose a novel user interface that is based on collections of lightweight editable fragments, called bubbles, which when grouped together form concurrently visible working sets. In this paper we describe the design of a prototype IDE user interface for Java based on working sets. A quantitative evaluation shows that developers could expect to view a sizeable number of functions concurrently with relatively few UI operations. A qualitative user evaluation with 23 professional developers indicates a high level of excitement, interest, and potential benefits and uses. Â© 2010 ACM.","bubbles; concurrent views; debugging; human factors; integrated development environments; navigation; source code; working set","Association for Computing Machinery (ACM);IEEE Computer Society;Technical Council on Software Engineering (tcse);SIGSOFT;Computer Society - South Africa",,"32nd ACM/IEEE International Conference on Software Engineering, ICSE 2010","1 May 2010 through 8 May 2010","Cape Town",81137,"Conference Paper","Scopus"
"Eliminating dead-code from XQuery programs","GenevÃ¨s P., LayaÃ¯da N.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954722631&doi=10.1145%2f1810295.1810363&partnerID=40&md5=93a7a37138be15c702cfb6d4a6535452",2010,"Proceedings - International Conference on Software Engineering","One of the challenges in web software development is to help achieving a good level of quality in terms of code size and runtime performance, for increasingly popular domain specific languages such as XQuery. We present an IDE equipped with static analysis features for assisting the programmer. These features are capable of identifying and eliminating dead code automatically. The tool is based on newly developed formal programming language verification techniques [4, 3], which are now mature enough to be introduced in the process of software development. Copyright 2010 ACM.",,"Association for Computing Machinery (ACM);IEEE Computer Society;Technical Council on Software Engineering (tcse);SIGSOFT;Computer Society - South Africa",,"32nd ACM/IEEE International Conference on Software Engineering, ICSE 2010","1 May 2010 through 8 May 2010","Cape Town",81137,"Conference Paper","Scopus"
"Evaluating consistency between BPEL specifications and functional requirements of complex computing Systems using the NFR approach","Vemulapalli A., Subramanian N.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954435268&doi=10.1109%2fSYSTEMS.2010.5482355&partnerID=40&md5=ce21fc5943856d8908819a646d93546e",2010,"2010 IEEE International Systems Conference Proceedings, SysCon 2010","Complex computing systems are usually composed of several subsystems interacting with each other. The functional requirements of the system being developed can be captured by BPEL (Business Process Execution Language) specifications, which are obtained from UML (Unified Modeling Language) diagrams. The captured BPEL specifications can be simulated in an environment such as NetBeans. The functional requirements of a complex system are known to change throughout the development due to changes in client's needs. Consistency between the BPEL specifications and the functional requirements is important to ensure that the system developed based on BPEL specifications satisfies the client's needs. In this paper we employ the NFR Approach, where NFR stands for non-functional requirements, to evaluate this consistency. We apply this approach to an example complex system, namely, the elevator control system, by first listing the functional requirements, capturing the requirements in BPEL, and evaluating the consistency between them using the NFR Approach. Based on this evaluation we are able to determine changes needed to BPEL specifications to match the functional requirements, justify the reasons for changes to the BPEL, and keep historical record of changes performed to the specifications during the system development process. It is our belief that practitioners in both industry and academia will find the NFR Approach to consistency evaluation a significant aid during complex systems development. Â©2010 IEEE.","BPEL; Consistency; Functional requirements; NFR approach",,,"4th International Systems Conference, SysCon 2010","5 April 2010 through 8 April 2010","San Diego, CA",80932,"Conference Paper","Scopus"
"In situ software visualisation","Harward M., Irwin W., Churcher N.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954339936&doi=10.1109%2fASWEC.2010.18&partnerID=40&md5=63753943cd0982e09e7c15e473b30d06",2010,"Proceedings of the Australian Software Engineering Conference, ASWEC","Software engineers need to design, implement, comprehend and maintain large and complex software systems. Awareness of information about the properties and state of individual artifacts, and the process being enacted to produce them, can make these activities less error-prone and more efficient. In this paper we advocate the use of code colouring to augment development environments with rich information overlays. These in situ visualisations are delivered within the existing IDE interface and deliver valuable information with minimal overhead. We present CODERCHROME, a code colouring plug-in for Eclipse, and describe how it can be used to support and enhance software engineering activities. Â© 2010 IEEE.","Code colouring; Software metrics; Software visualisation",,,"21st Australian Software Engineering Conference, ASWEC 2010","6 April 2010 through 9 April 2010","Auckland",80906,"Conference Paper","Scopus"
"Code bubbles: A working set-based interface for code understanding and maintenance","Bragdon A., Zeleznik R., Reiss S.P., Karumuri S., Cheung W., Kaplan J., Coleman C., Adeputra F., Laviola Jr. J.J.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953966208&doi=10.1145%2f1753326.1753706&partnerID=40&md5=0df7c51f8e2c0d160856457c41c40058",2010,"Conference on Human Factors in Computing Systems - Proceedings","Developers spend significant time reading and navigating code fragments spread across multiple locations. The file-based nature of contemporary IDEs makes it prohibitively difficult to create and maintain a simultaneous view of such fragments. We propose a novel user interface metaphor for code understanding based on collections of lightweight, editable fragments called bubbles, which form concurrently visible working sets. We present the results of a qualitative usability evaluation, and the results of a quantitative study which indicates Code Bubbles significantly improved code understanding time, while reducing navigation interactions over a widely-used IDE, for two controlled tasks. Â© 2010 ACM.","bubbles; java; multi-view; simultaneous views; source code","ACM Spec. Interest Group Comput.-Hum. Interact. (SIGCHI)",,"28th Annual CHI Conference on Human Factors in Computing Systems, CHI 2010","10 April 2010 through 15 April 2010","Atlanta, GA",80836,"Conference Paper","Scopus"
"Refactoring UML models: Using OpenArchitectureWare to measure UML model quality and perform pattern matching on UML models with OCL queries","Enckevort T.V.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-72249120667&doi=10.1145%2f1639950.1639959&partnerID=40&md5=00a177b706029cc73593d80d5902a805",2009,"Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA","In object oriented software development, the Unified Modeling Language (UML) [20] has become the de-facto modeling standard. UML plays an important role for software factories, in which a high quality abstract UML model is the primary source of input used to generate a working system. While there are many tools that enable assisted refactoring of source code, there are few tools that enable assisted refactoring of UML models. In order to determine UML model quality for UML models used in code generation projects, a selection of quality metrics has been made. While there are a large number of metrics available to determine code quality, there are only a limited number of metrics applicable to UML models. Most model quality metrics have been derived from code quality metrics [16]. Syntactic and semantic model check rules have been implemented, that allow detection of undesirablemodel properties. The syntactic model checkers have been derived directly from the UML specification. The semantic model checkers have been derived from a range of anti-pattern descriptions. We have delivered a prototype that detects undesirable model features in order to test the model improvement capabilities. The prototype contains selected model quality metrics, syntactic and semantic model check rules. Both metrics and rules have been formulated in the Object Constraint Language (OCL) [21], which operates on UML models. The system is built using Open Source tools, allowing easy extensions of the prototype. The effects of suggested repair actions on the model are measurable through the selected model quality metrics and by subjective comparison. The prototype was able to improve model quality for four industry models both by metrics and subjective comparison. Copyright Â© 2009 ACM.","Metrics; Model quality; OCL; Semantic rules; Syntactic rules; UML","ACM SIGPLAN",,"OOPSLA 2009 Companion - 24th Annual ACM Conference on Object-Oriented Programming, Systems, Languages and Applications, OOPSLA 2009","25 October 2009 through 29 October 2009","Orlando, FL",78926,"Conference Paper","Scopus"
"Autumn leaves: Curing the window plague in IDEs","RÃ¶thlisberger D., Nierstrasz O., Ducasse S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-73449114968&doi=10.1109%2fWCRE.2009.18&partnerID=40&md5=972619e6e46691d4e2589efc4624b3f4",2009,"Proceedings - Working Conference on Reverse Engineering, WCRE","Navigating large software systems is difficult as the various artifacts are distributed in a huge space, while the relationships between different artifacts often remain hidden and obscure. As a consequence, developers using a modern interactive development environment (IDE) are forced to open views on numerous source artifacts to reveal these hidden relationships, leading to a crowded workspace with many opened windows or tabs. Developers often lose the overview in such a cluttered workspace as IDEs provide little support to get rid of unused windows. AutumnLeaves automatically selects windows unlikely for future use to be closed or grayed out while important ones are displayed more prominently. This reduces the number of windows opened at a time and adds structure to the developer's workspace. We validate AutumnLeaves with a benchmark evaluation using recorded navigation data of various developers to determine the prediction quality of the employed algorithms. Â© 2009 IEEE.","Developer activity analysis; Development environments; Productivity; Program comprehension; Software navigation","Reengineering Forum",,"16th Working Conference on Reverse Engineering, WCRE 2009","13 October 2009 through 16 October 2009","Lille",78829,"Conference Paper","Scopus"
"Intelligent risk management tools for software development","Dhlamini J., Nhamu I., Kachepa A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-71049174205&doi=10.1145%2f1562741.1562745&partnerID=40&md5=faa28485b7ff8fcef973e88f18b4cd47",2009,"Proceedings of the 2009 Annual Conference of the Southern African Computer Lecturers' Association, SACLA 2009","Software tools have been used in software development for a long time now. They are used for, among other things, performance analysis, testing and verification, debugging and building applications. Software tools can be very simple and lightweight, e.g. linkers, or very large and complex, e.g. computer-assisted software engineering ( CASE ) tools and integrated development environments ( IDEs ). Some tools support particular phases of the project cycle while others can be used with a specific software development model or technology. Some aspects of software development, like risk management, are done throughout the whole project from inception to commissioning. The aim of this paper is to demonstrate the need for an intelligent risk assessment and management tool for both agile or traditional ( or their combination ) methods in software development. The authors propose a model, whose development is subject of further research, which can be investigated for use in developing intelligent risk management tools. Â© 2009 ACM.","Intelligent agents; Risk assessment; Software development; Software engineering tools","Centre of Excellence in Distributed Multimedia;Microsoft;Oracle;MasterSkill;ABSA",,"2009 Annual Conference of the Southern African Computer Lecturers' Association, SACLA '09","29 June 2009 through 1 July 2009","Port Alfred, Eastern Cape",77724,"Conference Paper","Scopus"
"Engineering of framework-specific modeling languages","Antkiewicz M., Czarnecki K., Stephan M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-73549111312&doi=10.1109%2fTSE.2009.30&partnerID=40&md5=7b1600161fbe2a1668b0a86d2103e420",2009,"IEEE Transactions on Software Engineering","Framework-specific modeling languages (FSMLs) help developers build applications based on object-oriented frameworks. FSMLs model abstractions and rules of application programming interfaces (APIs) exposed by frameworks and can express models of how applications use APIs. Such models aid developers in understanding, creating, and evolving application code. We present four exemplar FSMLs and a method for engineering new FSMLs. The method was created postmortem by generalizing the experience of building the exemplars and by specializing existing approaches to domain analysis, software development, and quality evaluation of models and languages. The method is driven by the use cases that the FSML under development should support and the evaluation of the constructed FSML is guided by two existing quality frameworks. The method description provides concrete examples for the engineering steps, outcomes, and challenges. It also provides strategies for making engineering decisions. Our work offers a concrete example of software language engineering and its benefits. FSMLs capture existing domain knowledge in language form and support application code understanding through reverse engineering, application code creation through forward engineering, and application code evolution through round-trip engineering. Â© 2009 IEEE.","Application programming interface (API); Domain-specific language; Evolution; Feature model; Forward engineering; Framework-specific model; Framework-specific modeling language; Mapping; Object-oriented framework; Reverse engineering; Round-trip engineering",,,,,,,"Article","Scopus"
"Automating expert-defined tests: A suitable approach for the medical device industry?","Connolly D., Mc Caffery F., Keenan F.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349934189&doi=10.1007%2f978-3-642-04133-4_3&partnerID=40&md5=73937f6be9210e5e6a2d1083137c6901",2009,"Communications in Computer and Information Science","Testing is frequently reported as a crucial stage in the software development process. With traditional approaches acceptance testing is the last stage of the process before release to customer. Acceptance Test Driven Development (ATDD) promotes the role of an expert customer in defining tests and uses tool support to automate and execute these tests. Here the challenge is to support such an expert in the reuse of existing documentation. This paper details an experiment in a generic domain while outlining plans for development of an automated testing model that could assist medical device companies to adhere to regulatory guidelines by providing them with a fully traceable testing artifacts. Â© 2009 Springer Berlin Heidelberg.",,,,,,,,"Conference Paper","Scopus"
"Learning to program with COALA, a distributed computer assisted environment","Jurado F., Molina A.I., Redondo M.A., Ortega M., Giemza A., Bollen L., Hoppe H.U.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-68749094221&partnerID=40&md5=0682e73f2fc1ac28a78ba24cb66cc578",2009,"Journal of Universal Computer Science","Learning to program is an important subject for students of Computer Science. Mentoring these students is a time-consuming and complex task. In this paper, we present a learning and tutoring environment that integrates task/solution delivery, assessment support and tutor's annotations, by extending Eclipse to a ""Real World Integrated Development Environment"". We will present a distributed system that uses Tuple Space architecture to integrate Eclipse with an evaluation module and a hand-writing annotation feature. Â© J.UCS.","Intelligent tutoring system; Learning programming",,,,,,,"Article","Scopus"
"FINDSITELHM: A threading-based approach to ligand homology modeling","Brylinski M., Skolnick J.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650912101&doi=10.1371%2fjournal.pcbi.1000405&partnerID=40&md5=a0196c3c7b0467e2c4d178d1ce830ed4",2009,"PLoS Computational Biology","Ligand virtual screening is a widely used tool to assist in new pharmaceutical discovery. In practice, virtual screening approaches have a number of limitations, and the development of new methodologies is required. Previously, we showed that remotely related proteins identified by threading often share a common binding site occupied by chemically similar ligands. Here, we demonstrate that across an evolutionarily related, but distant family of proteins, the ligands that bind to the common binding site contain a set of strongly conserved anchor functional groups as well as a variable region that accounts for their binding specificity. Furthermore, the sequence and structure conservation of residues contacting the anchor functional groups is significantly higher than those contacting ligand variable regions. Exploiting these insights, we developed FINDSITELHM that employs structural information extracted from weakly related proteins to perform rapid ligand docking by homology modeling. In large scale benchmarking, using the predicted anchor-binding mode and the crystal structure of the receptor, FINDSITE LHM outperforms classical docking approaches with an average ligand RMSD from native of âˆ¼2.5 Ã…. For weakly homologous receptor protein models, using FINDSITELHM, the fraction of recovered binding residues and specific contacts is 0.66 (0.55) and 0.49 (0.38) for highly confident (all) targets, respectively. Finally, in virtual screening for HIV-1 protease inhibitors, using similarity to the ligand anchor region yields significantly improved enrichment factors. Thus, the rather accurate, computationally inexpensive FINDSITELHM algorithm should be a useful approach to assist in the discovery of novel biopharmaceuticals. Â© 2009 Brylinski, Skolnick.",,,,,,,,"Article","Scopus"
"A model-driven approach to dynamic and adaptive service brokering using modes","Foster H., Mukhija A., Rosenblum D.S., Uchitel S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-58049127019&doi=10.1007%2f978-3-540-89652-4-46&partnerID=40&md5=1a353ef138cb6093dff99ed09516ba77",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","Industry and academia are exploring ways to exploit the services paradigm to assist in the challenges of software self-management. In this paper we present a novel approach which aims to bring these two fields closer by specifying the requirements and capabilities within a UML2 model architecture style and illustrating how these model elements are used to generate specifications for dynamic runtime service brokering given different modes of a software system. The approach is implemented in a tool suite integrated into the Eclipse IDE with a prototype runtime service broker engine. Â© 2008 Springer Berlin Heidelberg.",,,,"6th International Conference on Service-Oriented Computing, ICSOC 2008","1 December 2008 through 5 December 2008","Sydney",74899,"Conference Paper","Scopus"
"Jig saw: A tool for the small-scale reuse of source code","Cottrell R., Walker R.J., Denzinger J.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-57349174421&doi=10.1145%2f1370175.1370194&partnerID=40&md5=8dfb003e9d2ac4450a220aba20f88ec1",2008,"Proceedings - International Conference on Software Engineering","Developers perform small-scale reuse tasks to save time and to increase the quality of their code. Due to the small scale of such tasks, the overhead in reusing source code can quickly outweigh the benefits. Existing approaches focus on locating source code for reuse but do not support the integration of the located code within the developer's system, thereby leaving the developer with the burden of performing these steps manually. This paper presents a tool, called Jigsaw, that uses the developer's context to help integrate the reused source code into the developer's own source code.","Anti-unification; Correspondence; Reuse; Similarity","ACM SIGSOFT;IEEE CSE",,"30th International Conference on Software Engineering 2008, ICSE'08","10 May 2008 through 18 May 2008","Leipzig",74416,"Conference Paper","Scopus"
"Developing natural language-based program analyses and tools to expedite software maintenance","Hill E.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-57349190113&doi=10.1145%2f1370175.1370226&partnerID=40&md5=c571f2af3d9b90ac44f58500d67c1c22",2008,"Proceedings - International Conference on Software Engineering","With as much as 60-90% of software life cycle resources spent on program maintenance, there is a critical need for automated software tools to help explore and understand today's large and complex software. One important source of information software maintenance tools can draw from is lexical information in comments and identifiers. Identifier names often communicate a programmer's intent when writing code, and help developers map real-world concepts to code during comprehension. My dissertation will develop specialized information retrieval techniques and natural language analyses for software so that software maintenance tools can take full advantage of the wealth of information in program identifiers, and integrate these techniques into software tools to expedite the maintenance activities of program exploration, concern location, and fault localization.","Human Factors; Reliability","ACM SIGSOFT;IEEE CSE",,"30th International Conference on Software Engineering 2008, ICSE'08","10 May 2008 through 18 May 2008","Leipzig",74416,"Conference Paper","Scopus"
"Supporting system development by novice software engineers using a tutor-based software visualization (TubVis) approach","Sulaiman S., Rashid N.A., Abdullah R., Sulaiman S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-57349124910&doi=10.1109%2fITSIM.2008.4631951&partnerID=40&md5=31f8b8b15708967a5d656517b2faaf63",2008,"Proceedings - International Symposium on Information Technology 2008, ITSim","Most computer-aided software engineering (CASE) products provide visualization utility to view software artefacts developed. Nevertheless, existing methods or approaches in such tools are limited to generating the views or component dependencies that is focusing on twhat' the output ofreverse engineering process. The online help provided by the tools only indicate thow' to use the tools to generate the views. Since existing tools mostly target for experienced software engineers, they tend to overlook the need ofexplaining twhy' the output is recommended or not with regard to software engineering principles. Hence we propose tutor-based software visualization (Tub Vis) approach in So Vis tool that analyses software artefacts pertaining to software engineering best practices inputted by the experts and generate a set of recommendations regarding the design and coding for a novices. We anticipate TubVis can improve the quality of software design and program comprehension by combining practical and theoretical aspects of software engineering education in a software visualization tool. Â© 2008 IEEE.",,"IEEE",,"International Symposium on Information Technology 2008, ITSim","26 August 2008 through 29 August 2008","Kuala Lumpur",74115,"Conference Paper","Scopus"
"A survey about the intent to use visual defect annotations for software models","Rech J., Spriestersbach A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349866471&doi=10.1007%2f978-3-540-69100-6-29&partnerID=40&md5=859834a3cde7b1668d88dc14b42ef36b",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","Today, many practitioners have consolidated their experience with software models in collections of design flaws, smells, antipatterns, or guidelines that have a negative impact on quality aspects (such as maintainability). Besides these quality defects, many compilability errors or conformance warnings might occur in a software design. Programming IDEs typically present problems regarding compilability in or near the code (e.g., icons at the line or underlining in the code). Modeling IDEs in MDSD follow a visual paradigm and need a similar mechanism for presenting problems in a clear, consistent, and familiar way. In this paper, we present different visualization concepts for visualizing quality defects and other problems in software models. These concepts use different dimensions such as color, size, or icons to present this information to the user. We used a survey to explore the opinions held by practitioners showing that 89.9% want to be informed about potential defects and prefer icon-, view- and underscore-based concepts to other types of concepts. Â© 2008 Springer-Verlag Berlin Heidelberg.","Intelligent Assistance; MDSD; Quality Defects; Software Diagnostics; Software Models; Visual Annotations","Eur. Proj. MODELPLEX (MODELling solut. comPLEX softw. syst.);Int. Bus. Mach. Corp.;Netfective Technol. SA;Objecteering Softw. SA;Testing Technologies IST GmbH",,"4th European Conference on Model Driven Architecture - Foundations and Applications, ECMDA-FA 2008","9 June 2008 through 13 June 2008","Berlin",77483,"Conference Paper","Scopus"
"Semi-automating small-scale source code reuse via structural correspondence","Cottrell R., Walker R.J., Denzinger J.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349995448&doi=10.1145%2f1453101.1453130&partnerID=40&md5=89e9fa256f86146e5ae5a151c66ae437",2008,"Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering","Developers perform small-scale reuse tasks to save time and to increase the quality of their code, but due to their small scale, the costs of such tasks can quickly outweigh their benefits. Existing approaches focus on locating source code for reuse but do not support the integration of the located code within the developer's system, thereby leaving the developer with the burden of performing integration manually. This paper presents an approach that uses the developer's context to help integrate the reused source code into the developer's own source code. The approach approximates a theoretical framework (higher-order anti-unification modulo theories), known to be undecidable in general, to determine candidate correspondences between the source code to be reused and the developer's current (incomplete) system. This approach has been implemented in a prototype tool, called Jigsaw, that identifies and evaluates candidate correspondences greedily with respect to the highest similarity. Situations involving multiple candidate correspondences with similarities above a defined threshold are presented to the developer for resolution. Two empirical evaluations were conducted: an experiment comparing the quality of Jigsaw's results against suspected cases of small-scale reuse in an industrial system; and case studies with two industrial developers to consider its practical usefulness and usability issues. Â© 2008 ACM.","Jigsaw; Semiautomation; Small-scale source code reuse; Structural correspondences","ACM SIGSOFT",,"16th ACM SIGSOFT International Symposium on the Foundations of Software Engineering, SIGSOFT 2008/FSE-16","9 November 2008 through 14 November 2008","Atlanta, GA",79694,"Conference Paper","Scopus"
"The doctoral program of management in information technology at six","Steenkamp A.L.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870322308&partnerID=40&md5=6d31a76564868eb3532b7794614549cd",2008,"Proceedings of the Information Systems Education Conference, ISECON","Experience with the Doctor of Management in Information Technology, offered in the College of Management, Lawrence Technological University since 2002 is described. The mission of the program is to offer doctoral level education for working professionals with high levels of managerial and IT expertise. With a number of students passing through the research phase of the program the focus on innovation within enterprises has become a priority, as well as on IT leadership, based on state-of-the-art knowledge of ITs. The paper describes the approach to outcomes assessment in terms of defined criteria. Some operating results are presented, including data on student performance. Lessons learned during six years of offering the DMIT have impacted a number of aspects of the program, such as offering hybrid modes of delivery for some courses made possible by new education technologies, continuous updating of course syllabi to reflect the latest ideas and practices in consultation with the DMIT Advisory Board, modified approaches to preparing students for the comprehensive examinations, standardizing the research process, refinements in faculty approaches to supervising the dissertation research projects, and improved program governance. The program has enhanced the quality of faculty scholarship in the College of Management, and is attractive for LTU adjunct faculty, graduate students and alumni who are interested in pursuing doctoral studies. The research agenda and completed research topics are included in the paper. Â© 2008 EDSIG.","Curriculum; Dissertation research; Doctoral program; Information technology education; Information technology management; Outcomes assessment",,,"25th Information Systems Education Conference, ISECON 2008","6 November 2008 through 9 November 2008","Phoenix, AZ",94060,"Conference Paper","Scopus"
"Restricting IEC 61131-3 programming languages for use on high integrity applications","De Sousa M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349109497&doi=10.1109%2fETFA.2008.4638419&partnerID=40&md5=1393eff83422cabfd44f115ae1eea5f6",2008,"IEEE International Conference on Emerging Technologies and Factory Automation, ETFA","When developing applications with high integrity requirements, the choice of programming language is of utmost importance. In fact, IEC 61508 includes a table with suggested programming languages for each SIL (Safety Integrity Level). The languages defined in IEC 61131-3 are included in this table, but are only highly recommended for the higher SILs if a subset of the languages are used. In this paper we propose restrictions and changes to the IEC 61131-3 languages that constitute a subset of the languages with the objective of making them appropriate for the development of high integrity applications. Â© 2008 IEEE.",,"Institute of Electrical and Electronics Engineers (IEEE);IEEE Industrial Electronics Society (IES);Helmut Schmidt Universitat;ABB Stotz-Kontakt",,"13th IEEE International Conference on Emerging Technologies and Factory Automation, ETFA 2008","15 September 2008 through 18 September 2008","Hamburg",74209,"Conference Paper","Scopus"
"An empirical study of function overloading in C++","Wang C., Hou D.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-56349137141&doi=10.1109%2fSCAM.2008.25&partnerID=40&md5=060f78f93eff40cc27757b7fd30ce5ab",2008,"Proceedings - 8th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2008","The usefulness and usability of programming tools (for example, languages, libraries, and frameworks) may greatly impact programmer productivity and software quality. Ideally, these tools should be designed to be both useful and usable. But in reality, there always exist some tools or features whose essential characteristics can be fully understood only after they have been extensively used. The study described in this paper is focused on discovering how C++'s function overloading is used in production code using an instrumented g++ compiler. Our principal finding for the system studied is that the most 'advanced' subset of function overloading tends to be defined in only a few utility modules, which are probably developed and maintained by a small number of programmers, the majority of application modules use only the 'easy' subset of function overloading when overloading names, and most overloaded names are used locally within rather than across module interfaces. We recommend these as guidelines to software designers. Â© 2008 IEEE.",,"IEEE Comput. Soc. Tech. Council on Software Engineering;Semantic Designs Inc.;SAP;Reengineering Forum (REF);CREST",,"8th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2008","28 September 2008 through 29 September 2008","Beijing",74211,"Conference Paper","Scopus"
"Genesys: Service-oriented construction of property conform code generators","JÃ¶rges S., Margaria T., Steffen B.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049177579&doi=10.1007%2fs11334-008-0071-2&partnerID=40&md5=832dff1395f28317e4014a86c6e01620",2008,"Innovations in Systems and Software Engineering","This paper presents Genesys, a framework for the high-level construction of property conform code generators. Genesys is an integral part of jABC, a flexible framework designed to enable a systematic model-driven development of systems and applications on the basis of an (extensible) library of well-defined, reusable building blocks. Within jABC, Genesys manages the construction of code generators for jABC's models. So far, Genesys has been used to develop code generators for a variety of different target platforms, like a number of Java-based platforms, mobile devices, BPEL engines, etc. Since the code generators are themselves built within the jABC in a model-driven way, concepts like bootstrapping and reuse of existing components enable a fast evolution of Genesys' code generation library, and a high degree of self-application. Due to its increasing complexity and its high degree of reuse, Genesys profits from model checking-based verification. This way, jABC's models of code generators can be automatically checked wrt. well-formedness properties, to ensure that the models do indeed only consist of building blocks which are suitable for the considered target platforms, and whose versions are mutually compatible. It can be also be verified that the code generation process only starts after a successful initialization phase, and that the generated code is always packaged with all the required libraries. We will illustrate the ease of extension and flexibility of the Genesys framework by describing the intended coverage of diversity while highlighting the high potential of reuse. Â© Springer-Verlag London Limited 2008.",,,,,,,,"Article","Scopus"
"Recommending tags for pictures based on text, visual content and user context","Lindstaedt S., Pammer V., MÃ¶rzinger R., Kern R., MÃ¼lner H., Wagner C.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-51049123134&doi=10.1109%2fICIW.2008.26&partnerID=40&md5=07c9fbddd312ad0534d4c2a3973d00dc",2008,"Proceedings - 3rd International Conference on Internet and Web Applications and Services, ICIW 2008","Imagine you are member of an online social system and want to upload a picture into the community pool. In current social software systems, you can probably tag your photo, share it or send it to a photo printing service and multiple other stuff. The system creates around you a space full of pictures, other interesting content (descriptions, comments) and full of users as well. The one thing current systems do not do, is understand what your pictures are about. We present here a collection of functionalities that make a step in that direction when put together to be consumed by a tag recommendation system for pictures. We use the data richness inherent in social online environments for recommending tags by analysing different aspects of the same data (text, visual content and user context). We also give an assessment of the quality of thus recommended tags. Â© 2008 IEEE.",,"IARIA",,"3rd International Conference on Internet and Web Applications and Services, ICIW 2008","8 June 2008 through 13 June 2008","Athens",73382,"Conference Paper","Scopus"
"Leveraging source code search for reuse","Happel H.-J., Schuster T., Szulman P.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-50949133938&doi=10.1007%2f978-3-540-68073-4_38&partnerID=40&md5=2e760348fd217a15aaa6c7151d1bfc6c",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","The importance of search as a central support activity for information handling and software reuse has been highlighted by several authors. Although it is one of the most dominant daily activities of developers, it is not a first order concern of most development tools. Recently a number of specialized search engines for source code emerged that enable access to reusable assets from both the web and within organizations. We argue that those source code search engines can play a key role for information access throughout the software development lifecycle. In this paper we present an analysis of existing approaches and tools. Furthermore we point out several shortcomings and provide a roadmap for future enhancements. Â© 2008 Springer-Verlag Berlin Heidelberg.",,"Corporate Technology, Siemens Ltd.",,"10th International Conference on Software Reuse, ICSR 2008","25 May 2008 through 29 May 2008","Beijing",73475,"Conference Paper","Scopus"
"Corpus tools and methods, today and tomorrow: Incorporating linguists' manual annotations","Smith N., Hoffmann S., Rayson P.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-44449122882&doi=10.1093%2fllc%2ffqn004&partnerID=40&md5=03200a385c153a041363cdf546c5f16b",2008,"Literary and Linguistic Computing","Today's corpus tools offer the user a wide range of features that greatly facilitate the linguistic analysis of large amounts of authentic language data (e.g. frequency distributions, collocations, keywords, etc.). However, these tools typically fail to address the fundamental need of the linguist to add interpretive information to a concordance or query result, by coding individual concordance lines for structural, functional, discoursal, and other features in a flexible way. The ability to add such qualitative data is indispensable to a fuller understanding of the phenomenon under investigation as it allows the linguist to produce more rigorous descriptions - and theories - about language in use. Our article has two aims: first, to assess the merits and drawbacks of existing solutions, by surveying what can be achieved using state-of-the-art corpus tools and generic database software; second, we draw up a set of desiderata and recommendations for the incorporation of flexible encoding features into future corpus tools. We describe an initial step in this direction, with a recent enhancement to the BNCweb corpus analysis software. More generally, we hope our suggestions will lead to linguists and software developers working together more closely to ensure that the needs of the former are provided for by the available technology. Â© The Author 2008. Published by Oxford University Press on behalf of ALLC and ACH. All rights reserved.",,,,,,,,"Article","Scopus"
"User acceptance of the intelligent fridge: Empirical results from a simulation","Rothensee M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-41549141264&doi=10.1007%2f978-3-540-78731-0_8&partnerID=40&md5=02658748f006a0efd88ffdf44a0c45e4",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","The smart fridge has often been considered a prototypical example of applications of the Internet of Things for the home. However, very little research has been conducted on functions desired by prospective users, and how users will eventually use the fridge. A simulation of a smart fridge was developed and tested within a controlled laboratory between-subjects experiment with 105 participants. Four different assistance functions were tested. It was found that generally a smart fridge is evaluated as moderately useful, easy to use and people would tend to buy it, if it was already available. Emotional responses differed between the assistance functions. Displaying information on durability of products, as well as giving feedback on nutrition health and economics are the most appreciated applications. Structurally, overall usefulness ratings of the device are the strongest predictors for the intention to use a smart fridge, but the emotional response to the product was also an important explanatory variable. Results are not influenced by technical competence, gender, or sense of presence in the simulation. Regression models confirmed that the simulation-based results explained 20% more variance in product acceptance than written scenarios. An outlook is given on future questions to be answered using the simulation. Â© 2008 Springer-Verlag Berlin Heidelberg.",,,,"1st International Conference on the Internet of Things, IOT 2008","26 March 2008 through 28 March 2008","Zurich",71723,"Conference Paper","Scopus"
"Contract specification in java: Classification, characterization, and a new marker method","Chen C.-T., Cheng Y.C., Hsieh C.-Y.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-68749110707&doi=10.1093%2fietisy%2fe91-d.11.2685&partnerID=40&md5=3a5ba07c52b9383d34b66769bedc8a7b",2008,"IEICE Transactions on Information and Systems","Design by Contract (DBC), originated in the Eiffel programming language, is generally accepted as a practical method for building reliable software. Currently, however, few languages have built-in support for it. In recent years, several methods have been proposed to support DBC in Java. We compare eleven DBC tools for Java by analyzing their impact on the developer's programming activities, which are characterized by seven quality attributes identified in this paper. It is shown that each of the existing tools fails to achieve some of the quality attributes. This motivates us to develop ezContract, an open source DBC tool for Java that achieves all of the seven quality attributes. ezContract achieves streamlined integration with the working environment. Notably, standard Java language is used and advanced IDE features that work for standard Java programs can also work for the contract-enabled programs. Such features include incremental compilation, automatic refactoring, and code assist. Copyright Â© 2008 The Institute of Electronics, Information and Communication Engineers.","Characterization; Classification; Contract specification; Design by Contract; Java; Quality attribute",,"Institute of Electronics, Information and Communication, Engineers, IEICE",,,,,"Article","Scopus"
"Software & systems engineering process and tools for the development of autonomous driving intelligence","Basarke C., Berger C., Rumpe B.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38549101566&doi=10.2514%2f1.33453&partnerID=40&md5=cf3cdf66299b03a82543d4e6e5fbae0f",2007,"Journal of Aerospace Computing, Information and Communication","When a large number of people with heterogeneous knowledge and skills run a project together, it is important to use a sensible engineering process. This especially holds for a project building an intelligent autonomously driving car to participate in the 2007 DARPA Urban Challenge. In this article, we present essential elements of a software and systems engineering process for the development of artificial intelligence capable of driving autonomously in complex urban situations. The process includes agile concepts, like test first approach, continuous integration of every software module and a reliable release and configuration management assisted by software tools in integrated development environments. However, the most important ingredients for an efficient and stringent development are the ability to efficiently test the behavior of the developed system in a flexible and modular simulator for urban situations.",,,,,,,,"Article","Scopus"
"Harnessing theories for tool support?","Liu Z., Mencl V., Ravn A.P., Lu Y.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956433437&doi=10.1109%2fISoLA.2006.49&partnerID=40&md5=e799e1838432b97cb6ab744ce37e88e3",2007,"Proceedings - ISoLA 2006: 2nd International Symposium on Leveraging Applications of Formal Methods, Verification and Validation","Software development tools need to support more and more phases of the entire development process, because applications must be developed more correctly and efficiently. The tools therefore need to integrate sophisticated checkers, generators and transformations. A feasible approach to ensure high quality of such add-ins is to base them on sound formal foundations. In order to know where such add-ins will fit, we investigate the use of an existing successful commercial tool and identify suitable places for adding formally supported checking, transformation and generation modules. The paper concludes with a discussion of feasibility of developing the proposed add-ins and how to give conditions such that they will actually be used. Â© 2007 IEEE.","Formal methods; Software development tool; Software process; Tool design","EASST (European Association of Software Science and Technology);University of Cyprus;ATHK/CYTA (Cyprus Telecom-munications Authority);ARTIST 2 Network of Excellence;University of Dortmund;et. al.",,"2nd International Symposium on Leveraging Applications of Formal Methods, Verification and Validation, ISoLA 2006","15 November 2006 through 19 November 2006","Paphos",81670,"Conference Paper","Scopus"
"JACK - A tool for validation of security and behaviour of java applications","Barthe G., Burdy L., Charles J., GrÃ©goire B., Huisman M., Lanet J.-L., Pavlova M., Requet A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38549137266&partnerID=40&md5=483f576af0a392215f054924ff364ecd",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","We describe the main features of JACK (Java Applet Correctness Kit), a tool for the validation of Java applications, annotated with JML specifications. JACK has been especially designed to improve the quality of trusted personal device applications. JACK is fully integrated with the IDE Eclipse, and provides an easily accessible user interface. In particular, it allows to inspect the generated proof obligations in a Java syntax, and to trace them back to the source code that gave rise to them. Further, JACK provides support for annotation generation, and for interactive verification. The whole platform works both for source code and for bytecode, which makes it particularly suitable for a proof carrying code scenario. Â© Springer-Verlag Berlin Heidelberg 2007.",,,,"5th International Symposium on Formal Methods for Components and Objects, FMCO 2006","7 November 2006 through 10 November 2006","Amsterdam",71280,"Conference Paper","Scopus"
"Unanticipated reuse of large-scale software features","Holmes R.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247170026&partnerID=40&md5=fe2c7ae914cc907c4ff99a8b9ad58e77",2006,"Proceedings - International Conference on Software Engineering","Software reuse has been endorsed as a way to reduce development times and costs while increasing software quality and reliability. Techniques designed to encourage software reuse have concentrated on creating reusable software in the form of frameworks, reuse repositories, and component libraries. These approaches do not help a developer who wants to leverage, from an existing system, a complex feature that was not designed to be reusable. We propose an approach that allows developers to investigate the reuse potential of a feature within an existing system, to create a plan for reusing the feature, and to support the transformation of the feature to the developer's project. We believe that by providing explicit support for the reuse of large-scale source code features, the reuse process - and its benefits - can be made accessible to developers.","Integrated development environment; Reuse; Software feature; Software structure","ACM Special Interest Group on Software Engineering, SIGSOFT",,"28th International Conference on Software Engineering 2006, ICSE '06","20 May 2006 through 28 May 2006","Shanghai",69469,"Conference Paper","Scopus"
"User evaluation of FÃ­schlÃ¡r-News: An automatic broadcast news delivery system","Lee H., Smeaton A.F., O'Connor N.E., Smyth B.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746814898&doi=10.1145%2f1148020.1148021&partnerID=40&md5=e4ad597bc5ec9cbf5723c1a28bfb25d9",2006,"ACM Transactions on Information Systems","Technological developments in content-based analysis of digital video information are undergoing much progress, with ideas for fully automatic systems now being proposed and demonstrated. Yet because we do not yet have robust operational video retrieval systems that can be deployed and used, the usual HCI practise of conducting a usage study and an informed iterative system design is thus not possible. FÃ­schlÃ¡r-News is one of the first automatic, content-based broadcast news analysis and archival systems that process broadcast news video so that users can search, browse, and play it in an easy-to-use manner with a conventional web browser. The system incorporates a number of state-of-the-art research components, some of which are not yet considered mature technology, yet it has been built to be robust enough to be deployed to users who are interested in access to daily news throughout a university campus. In this article we report and discuss a user-evaluation study conducted with 16 users, each of whom utilized the system freely for a one month period. Results from a detailed qualitative analysis are presented, looking at collected questionnaires, incident diaries, and interaction-log data. The findings suggest that our users employed the system in conjunction with their other news update methods, such as watching TV news at home and browsing online news websites at their workplace, their major concerns being up-to-dateness and coverage of the news content. They tried to accommodate the system to fit their established web browsing habits, and they found local news content and the ability to play selfcontained news stories on their desktop as major values of the system. Our study also resulted in a detailed wishlist of new features which will help in the further development of both our and others' systems. Â© 2006 ACM.","Content-based video retrieval; Usage analysis; User-evaluation",,,,,,,"Article","Scopus"
"An automated procedure for the extraction of metabolic network information from time series data","Marino S., Voit E.O.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33748511015&doi=10.1142%2fS0219720006002259&partnerID=40&md5=ac730135c539c101279a6954062ae911",2006,"Journal of Bioinformatics and Computational Biology","Novel high-throughput measurement techniques in vivo are beginning to produce dense high-quality time series which can be used to investigate the structure and regulation of biochemical networks. We propose an automated information extraction procedure which takes advantage of the unique S-system structure and supports model building from time traces, curve fitting, model selection, and structure identification based on parameter estimation. The procedure comprises of three modules: model Generation, parameter estimation or model Fitting, and model Selection (GFS algorithm).The GFS algorithm has been implemented in MATLAB and returns a list of candidate S-systems which adequately explain the data and guides the search to the most plausible model for the time series under study. By combining two strategies (namely decoupling and limiting connectivity) with methods of data smoothing, the proposed algorithm is scalable up to realistic situations of moderate size. We illustrate the proposed methodology with a didactic example. Â© 2006 Imperial College Press.","Biochemical systems theory; Inverse problem; Metabolic profile; Parameter estimation; Pathway identification; S-system; Time series",,,,,,,"Article","Scopus"
"Refactoring a legacy component for reuse in a software product line: A case study","Kolb R., Muthig D., Patzke T., Yamauchi K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646408526&doi=10.1002%2fsmr.329&partnerID=40&md5=501ee942227e40e04ac86fcd865112dd",2006,"Journal of Software Maintenance and Evolution","Product lines are a promising approach to improve conceptually the productivity of the software development process and thus to reduce both the cost and time of developing and maintaining increasingly complex systems. An important issue in the adoption of the product-line approach is the migration of legacy software components, which have not been designed for reuse, systematically into reusable product-line components. This article describes activities performed to improve systematically the design and implementation of an existing software component in order to reuse it in a software product line. The activities are embedded in the application of Fraunhofer PuLSEâ„¢-DSSA - an approach for defining domain-specific software architectures (DSSA) and product-line architectures. The component under investigation is the so-called Image Memory Handler (IMH), which is used in Ricoh's current products of office appliances such as copier machines, printers, and multi-functional peripherals. It is responsible for controlling memory usage and compressing and decompressing image data. Improvement of both the component's design and implementation are based on a systematic analysis and focused on increasing maintainability and reusability and hence suitability for use in a product line. As a result of the analysis and refactoring activities, the documentation and implementation of the component has been considerably improved as shown by quantitative data collected at the end of the activities. Despite a number of changes to the code, the external behavior of the component has been preserved without significantly affecting the performance. Copyright Â© 2006 John Wiley & Sons, Ltd.","Code analysis; Legacy component; Refactoring; Reuse; Software product line; Variability",,,,,,,"Conference Paper","Scopus"
"A software package to improve image quality and isolation of objects of interest for quantitative stereology studies of rat hepatocarcinogenesis","Xu Y., Pitot H.C.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646022591&doi=10.1016%2fj.cmpb.2005.11.009&partnerID=40&md5=12d7fd6d604144233ca570cd7cc0c0b5",2006,"Computer Methods and Programs in Biomedicine","In the studies of quantitative stereology of rat hepatocarcinogenesis, we have used image analysis technology (automatic particle analysis) to obtain data such as liver tissue area, size and location of altered hepatic focal lesions (AHF), and nuclei counts. These data are then used for three-dimensional estimation of AHF occurrence and nuclear labeling index analysis. These are important parameters for quantitative studies of carcinogenesis, for screening and classifying carcinogens, and for risk estimation. To take such measurements, structures or cells of interest should be separated from the other components based on the difference of color and density. Common background problems seen on the captured sample image such as uneven light illumination or color shading can cause severe problems in the measurement. Two application programs (BK_Correction and Pixel_Separator) have been developed to solve these problems. With BK_Correction, common background problems such as incorrect color temperature setting, color shading, and uneven light illumination background, can be corrected. With Pixel_Separator different types of objects can be separated from each other in relation to their color, such as seen with different colors in immunohistochemically stained slides. The resultant images of such objects separated from other components are then ready for particle analysis. Objects that have the same darkness but different colors can be accurately differentiated in a grayscale image analysis system after application of these programs. Â© 2006 Elsevier Ireland Ltd. All rights reserved.","Automatic particle analysis; Color pixel separation; Image analysis; Image background problem; Quantitative stereology",,,,,,,"Article","Scopus"
"Evaluation of integrated software development environments: Challenges and results from three empirical studies","Kline R.B., Seffah A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-27644518394&doi=10.1016%2fj.ijhcs.2005.05.002&partnerID=40&md5=ae35b9ce22adeed338911ff784487879",2005,"International Journal of Human Computer Studies","Evidence shows that integrated development environments (IDEs) are too often functionality-oriented and difficult to use, learn, and master. This article describes challenges in the design of usable IDEs and in the evaluation of the usability of such tools. It also presents the results of three different empirical studies of IDE usability. Different methods are sequentially applied across the empirical studies in order to identify increasingly specific kinds of usability problems that developers face in their use of IDEs. The results of these studies suggest several problems in IDE user interfaces with the representation of functionalities and artifacts, such as reusable program components. We conclude by making recommendations for the design of IDE user interfaces with better affordances, which may ameliorate some of most serious usability problems and help to create more human-centric software development environments. Â© 2005 Elsevier Ltd. All rights reserved.","CASE tools; Integrated development environment (IDE); Software development environment; Usability; User interfaces; User-centered design",,,,,,,"Article","Scopus"
"Quality of service-driven requirements analyses for component composition: A two-level grammar++ approach","Liu S.-H., Cao F., Bryant B.R., Gray J., Raje R.R., Olson A.M., Auguston M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548096730&partnerID=40&md5=fdb84a5ead90d00a2d3fc57cc9076d31",2005,"17th International Conference on Software Engineering and Knowledge Engineering, SEKE 2005","Component-based software engineering offers the opportunity to assemble entire systems from components. When applied to Distributed Real-Time and Embedded (DRE) systems, which components to assemble and how to assemble them are determined not only from functional correctness criteria but also assurance of the system's quality of service (QoS). This paper presents a grammatical QoSdriven approach to optimize component assembly by reducing the search space of assembly alternatives by eliminating infeasible components, with feasible components selected based on reasoning about non-functional requirements. The reasoning is realized by a rule engine with a knowledge base derived from the requirements phase of the software lifecycle. In addition, the grammatical approach introduces well-defined semantics among the components being composed. The semantics assist in precisely and efficiently evaluating the individual component QoS, as well as systemwide QoS in a programmable fashion. The result is to facilitate straightforward and manageable component composition analyses from the perspective of QoS requirements.",,,,"17th International Conference on Software Engineering and Knowledge Engineering, SEKE 2005","14 July 2005 through 16 July 2005","Taipei",100110,"Conference Paper","Scopus"
"Improving design quality using meta-pattern transformations: A metric-based approach","Tahvildari L., Kontogiannis K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-4344606635&partnerID=40&md5=32f7ca9343de63070bed9d0f980df01e",2004,"Journal of Software Maintenance and Evolution","Improving the design quality of large object-oriented systems during maintenance and evolution is widely regarded as a high-priority objective. Furthermore, for such systems that are subject to frequent modifications, detection and correction of design defects may easily become a very complex task that is even not tractable for manual handling. Therefore, the use of automatic or semi-automatic detection and correction techniques and tools can assist reengineering activities. This paper proposes a framework whereby object-oriented metrics can be used as indicators for automatically detecting situations for particular transformations to be applied in order to improve specific design quality characteristics. The process is based both on modeling the dependencies between design qualities and source code features, and on analyzing the impact that various transformations have on software metrics that quantify the design qualities being improved. Copyright Â© 2004 John Wiley & Sons, Ltd.","Design flaws; Non-functional requirements; Object-oriented metrics; Program transformation; Software reengineering",,,,,,,"Conference Paper","Scopus"
"Plugging-in Visualization: Experiences Integrating a Visualization Tool with Eclipse","Lintern R., Michaud J., Storey M.-A., Wu X.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0141998633&partnerID=40&md5=5a760d6582a6b79f187546b62811f955",2003,"Proceedings of ACM Symposium on Software Visualization","The Eclipse platform presents an opportunity to openly collaborate and share visualization tools amongst the research community and with developers. In this paper, we present our own experiences of ""plugging-in"" our visualization tool, SHriMP Views, into this environment. The Eclipse platform's Java Development Tools (JDT) and CVS plug-ins provide us with invaluable information on software artifacts relieving us from the burden of creating this functionality from scratch. This allows us to focus our efforts on the quality of our visualizations and, as our tool is now part of a full-featured Java IDE, gives us greater opportunities to evaluate our visualizations. The integration process required us to re-think some of our tool's architecture, strengthening its ability to be plugged into other environments. We step through a real-life scenario, using our newly integrated tool to aid us in merging of two branches of source code. Finally we detail some of the issues we have encountered in this integration and provide recommendations for other developers of visualization tools considering integration with the Eclipse platform.","Configuration management and version control; Integration; Software development environment; Software visualization","ACM SIGCHI",,"Proceedings of the ACM 2003 Symposium on Software Visualization (SoftVis 2003)","11 June 2003 through 13 June 2003","San Diego, CA",61593,"Conference Paper","Scopus"
"Telemonitoring of patients at home: A software agent approach","Rialle V., Lamy J.-B., Noury N., Bajolle L.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0141992831&doi=10.1016%2fS0169-2607%2802%2900161-X&partnerID=40&md5=afcd9bcad4c5ab0ade75ca481de995f3",2003,"Computer Methods and Programs in Biomedicine","To address the issue of the increasing social, economical and medical needs of maintaining at home people in loss of autonomy while preserving privacy and quality of life, the authors present a software agent based telemonitoring and alarm raising system. The article describes the overall architecture, the various components of the model, and the methodology that has been used. It specifically addresses the issue of reflecting in the object oriented model of the system various dimensions including: the physical world of in-home bio-signal sensors, the numerical world of software agents and Internet-related technologies, and the medical and social worlds of patients, physicians and caregivers. In the model, the main stream of information goes from the biophysical world of patients at home to the socio-medical world of carers through a chain of devices including in-home sensors, local area network, home computer, remote server, and carers' computers. Each device hosts software agents with different levels of knowledge and complexity. Internet and Java technologies provide the building blocks of the designed telemonitoring software. Laboratory experiments have been realized using a fully equipped 'smart' demonstration home for telecare. The study takes place into a more general research project on 'smart' homes for telecare conducted at the Hospital Centre of Grenoble, France. Â© 2003 Elsevier Science Ireland Ltd. All rights reserved.","'Smart' homes; Distributed systems; Intelligent monitoring; Software agents; Telemedicine",,"Elsevier Ireland Ltd",,,,,"Article","Scopus"
"Current utilization of CASE technology: Lessons from the field","McMurtrey M.E., Teng J.T.C., Grover V., Kher H.V.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033707477&doi=10.1108%2f02635570010273027&partnerID=40&md5=2ce8fd6bf4a5ed22a7bcb79209348d14",2000,"Industrial Management and Data Systems","Since its inception, CASE (computer-aided software engineering) tools have been hailed as the `silver bullet' of applications development. Although these tools have failed to live up to such an advance billing (as do most `fix-all' solutions), these products remain a viable option for practitioners of modern applications development. This study comments on the use of CASE in modern IS installations, using the results of an in-depth survey completed by 226 IS professionals from over 30 Fortune 500-type companies. First, the study identifies the most popular features possessed by respondents' CASE toolsets. Next, we comment on the `gap' perceived to exist between CASE features actually possessed, and those features needed by these professionals in the performance of their job duties. Finally, implications for practice and research are presented.",,,"MCB Univ Press Ltd, Bradford",,,,,"Article","Scopus"
"Tool integration in evolving information systems environments","Jarke M., Nissen H.W., Pohl K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028876283&partnerID=40&md5=eb3f744fa202403635d315ad1f655172",1994,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","Evolution is a fact of life in information systems. Not only systems evolve but also their development processes. IS environments must therefore be designed for accommodating and managing change. The management of process meta models in repositories is one important step; we show how process traceability models and process guidance models can be developed and related in a standard repository framework. In addition, the currently available tool integration along the presentation, data, and control perspectives have to be augmented for process integration. In our process-adaptable and interoperable tool concept, tool behavior is directly influenced by the process guidance model and automatically traced according to the traceability model. The approach is demonstrated with a prototype requirements engineering environment developed in ESPRIT project NATURE. Â© Springer-Verlag Berlin Heidelberg 1994.",,,"Springer Verlag","3rd Workshop on Information Systems and Artificial Intelligence, 1994","28 February 1994 through 2 March 1994",,171529,"Conference Paper","Scopus"
"Use of hypertext for teaching and training: A bibliography","Ramaiah C.K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77950023880&partnerID=40&md5=dace87af91ef9f9bab05e5f91b562ae6",1993,"Electronic Library","This bibliography was originally compiled for the purpose of a Doctoral degree submitted to Loughborough University of Technology in March 1993. The information in this bibliography, which was started in the last quarter of 1989, was continuously updated by collecting data from all the important current journals and abstracting and indexing sources. It covers all of hypertext, including HyperCard and other hypertext/hypermedia systems which are being used for teaching and training. These systems are also used for conducting research in this field. Full efforts were made to cover all the publications such as periodical articles, conference papers/Proceedings, books and reports that were published until the first quarter of 1993.",,,,,,,,"Article","Scopus"
"An Automated Software Design Assistant","Karimi J., Konsynski B.R.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023961494&doi=10.1109%2f32.4638&partnerID=40&md5=d4f524ce69253c7ef4b84f832d5f4dbc",1988,"IEEE Transactions on Software Engineering","An automated software design assistant was implemented as part of long term project, with the objective of applying the Computed-aided technique to the tools in a software engineering environment. A set of quantitive measures are derived based on the degree to which a particular design satisfied the attributes associated with a structured software design. The measures are then used as a decision rules for a computer-aided methodology for structured design. The feasibility of the approach is also demonstrated by a case study using a small application system design problem. Â© 1988 IEEE","Cohesion; coupling; modularization; structured design",,,,,,,"Article","Scopus"
"ReFit: A Fit test maintenance plug-in for the Eclipse refactoring plug-in","Druk M., Kropp M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889074947&doi=10.1109%2fTOPI.2013.6597187&partnerID=40&md5=fed7fe5551a9b37c7cb09418d7fabbb2",2013,"International Workshop on Developing Tools as Plug-Ins, TOPI","The Fit framework is a widely established tool for automated acceptance test-driven development (ATDD). Fit stores the test specification separate from the test fixture code in an easily human readable and editable tabular form in HTML format. Additional tools like the FitPro plugin or FitN esse support the writing of test specifications and test fixtures from within the Eclipse IDE or the Web. With the increasing popularity of agile test-driven software development, maintenance of the evolving and growing test base has become an important issue. However, there has been no support yet for automated refactoring of Fit test cases. In a recent research project, we developed the Eclipse plugin ReFit for automated refactoring of Fit test cases. Fit test refactoring can occur due to changing requirements or changing Java code, which in either case means a cross-language refactoring to keep test specification and test fixture in sync. In this paper the concept for the development of the ReFit Eclipse Plugin is described, which significantly reduces the effort for Fit test maintenance and makes refactoring less error prone. Besides a tight integration into the existing Eclipse refactoring plugin, major goals of the plugin were to make it easy extensible for additional refactorings, new fixture types and further test specification file formats. Challenges faced when adding new and modifying existing Eclipse refactoring behavior are described and are due to the strong dependency on the Eclipse JDK and LTK features, and the solutions developed are presented. Â© 2013 IEEE.","Acceptance Testing; Maintenance; Plugin; Refactoring Automation; Tools",,,"3rd International Workshop on Developing Tools as Plug-Ins, TOPI 2013","21 May 2013 through 21 May 2013","San Francisco, CA",101037,"Conference Paper","Scopus"
"A rigorous methodology for analyzing and designing plug-ins","Fasie M.V., Haxthausen A.E., Kiniry J.R.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889056032&doi=10.1109%2fTOPI.2013.6597194&partnerID=40&md5=baacb3eea8365cbe039ead853f6d320d",2013,"International Workshop on Developing Tools as Plug-Ins, TOPI","Today, GUI plug-ins development is typically done in a very ad-hoc way, where developers dive directly into implementation. Without any prior analysis and design, plug-ins are often flaky, unreliable, difficult to maintain and extend with new functionality, and have inconsistent user interfaces. This paper addresses these problems by describing a rigorous methodology for analyzing and designing plug-ins. The methodology is grounded in the Extended Business Object Notation (EBON) and covers informal analysis and design of features, GUI, actions, and scenarios, formal architecture design, including behavioral semantics, and validation. The methodology is illustrated via a case study whose focus is an Eclipse environment for the RAISE formal method's tool suite. Â© 2013 IEEE.",,,,"3rd International Workshop on Developing Tools as Plug-Ins, TOPI 2013","21 May 2013 through 21 May 2013","San Francisco, CA",101037,"Conference Paper","Scopus"
"A CPI breakdown model plug-in for optimizing application performance","Araujo R.F., Barboza D., Sene R., Teixeira R., JoÃ£o R., Moschetta W.S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889073025&doi=10.1109%2fTOPI.2013.6597191&partnerID=40&md5=67510d1279fd68997b1a9b7016589a75",2013,"International Workshop on Developing Tools as Plug-Ins, TOPI","Effectiveness and quality are fundamental characteristics for the development of a product. In order to support them, one needs to ensure that an application optimization level is at its best. The most widely used metric for evaluating an application performance is the CPI (Cycles Per Instruction), i.e., the number of clock cycles that takes place when an instruction is executed. We have developed a CPI Breakdown Model Plug-in, that automates the profiling of an application in the Power architecture, breaking it down into several groups of CPI events and metrics in order to identify possible bottlenecks. When analyzing such events and metrics the user can become aware of which operations are causing the processor to stall, and consequently enhance the application source code. We have discussed the adaptation of some command-line tools to be used by our CPI plug-in, which is integrated into the IBM Software Development Toolkit for PowerLinux, an Eclipse based IDE comprising a set of mainstream C/C++ development tools along with several in-house IBM ones. A case study that shows the usefulness of our approach is presented and details on how to optimize an application are discussed. Â© 2013 IEEE.","CPI; Eclipse; Optimization; Performance; plug-ins; Power Architecture; PowerLinux",,,"3rd International Workshop on Developing Tools as Plug-Ins, TOPI 2013","21 May 2013 through 21 May 2013","San Francisco, CA",101037,"Conference Paper","Scopus"
"A visual studio plug-in for CProver","Seghir M.N., Kroening D.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889042565&doi=10.1109%2fTOPI.2013.6597193&partnerID=40&md5=efbc02560ef0d6ea2c39dd297dc17600",2013,"International Workshop on Developing Tools as Plug-Ins, TOPI","In recent years, automatic software verification has emerged as a complementary approach to program testing for enhancing software quality. Finding bugs is the ultimate aim of software verification tools. How do we best support the programmer who has to diagnose and understand those bugs? Unfortunately, most of the existing tools do not offer enough support for error diagnosis. We have developed a plug-in which implements a graphical user interface for the CProver tools within the Visual Studio IDE. Our plug-in enables visual debugging and error trace simulating within C programs as well as co-debugging C programs in tandem with wave-form views of hardware designs. Another feature of our plug-in is background verification. Each time a program source is saved, the verification process is silently triggered in background. If an error is found, its location is highlighted in the program. The user interacts directly with the program source to obtain information about the error. Â© 2013 IEEE.","Co-Verification; Plug-in; Program Verification; Software Model Checking; Visual Studio",,,"3rd International Workshop on Developing Tools as Plug-Ins, TOPI 2013","21 May 2013 through 21 May 2013","San Francisco, CA",101037,"Conference Paper","Scopus"
"Integrating S6 code search and Code Bubbles","Reiss S.P.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888994977&doi=10.1109%2fTOPI.2013.6597190&partnerID=40&md5=99095d6d30a58aae1b9a4405601d0ee5",2013,"International Workshop on Developing Tools as Plug-Ins, TOPI","We wanted to provide a tool for doing code search over open source repositories as part of the Code Bubbles integrated development environment. Integrating code search as a plug-in to Code Bubbles required substantial changes to the S6 code search engine and the development of appropriate user interfaces in Code Bubbles. After briefly reviewing Code Bubbles and the S6 search engine, this paper describes the integration strategy, the front end for code search, the modifications to the code search engine to handle context-based search, and the user interface for handling the results of the search. Â© 2013 IEEE.","Code search; code search in context; integrated development environments; test-based search",,,"3rd International Workshop on Developing Tools as Plug-Ins, TOPI 2013","21 May 2013 through 21 May 2013","San Francisco, CA",101037,"Conference Paper","Scopus"
"Lightweight tool coordination: Path* - A minimal framework for tool coordination","McKenzie R.D., Perry D.E.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889031552&doi=10.1109%2fTOPI.2013.6597195&partnerID=40&md5=761fc464e2b3de5053f83078457f227a",2013,"International Workshop on Developing Tools as Plug-Ins, TOPI","We present a lightweight tool for coordinating tool usage in a structured and unobtrusive manner allowing for the formal description and implementation of development cycles with minimal human intervention. This tool, here and after referred to as ""Path*"" seeks to provide a minimal yet powerful framework for tool coordination by scripting actions to be triggered on events such as disk writes to a project directory and version control system commits. These events execute user-defined scripts for the purpose of automating tasks such as partial rebuilds and style checking in an IDE and platform independent framework. Â© 2013 IEEE.","Plug-in; Process Automation; Process Guidance; Process Scripting; Tool Coordination",,,"3rd International Workshop on Developing Tools as Plug-Ins, TOPI 2013","21 May 2013 through 21 May 2013","San Francisco, CA",101037,"Conference Paper","Scopus"
"TerraME GIMS: An Eclipse plug-in for environmental modeling","Lima T., Carneiro T., Faria S., Silva P., Pessoa M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888995091&doi=10.1109%2fTOPI.2013.6597192&partnerID=40&md5=26920867fff85c4e3257cacc0346e3f6",2013,"International Workshop on Developing Tools as Plug-Ins, TOPI","TerraME is a platform for modeling and simulation of environmental systems that offers a conceptual basis and services to build environmental models through a high-level programming language called Terra Modeling Language. However, the use of a programming language is still a limiting factor since its main users are researchers with different backgrounds who usually lack basic knowledge of algorithms and programming techniques. So, this work presents the development of TerraME GIMS, an Eclipse plug-in for environmental systems modeling through visual metaphors that graphically represent the model. Â© 2013 IEEE.","Eclipse; environmental modeling; graphical user interface; plug-in; TerraME GIMS; Visual programming",,,"3rd International Workshop on Developing Tools as Plug-Ins, TOPI 2013","21 May 2013 through 21 May 2013","San Francisco, CA",101037,"Conference Paper","Scopus"
"Challenges in developing a software architecture evolution tool as a plug-in","Barnes J.M., Garlan D.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889040013&doi=10.1109%2fTOPI.2013.6597188&partnerID=40&md5=2c84d2776e117beb24d251a95b63cb6a",2013,"International Workshop on Developing Tools as Plug-Ins, TOPI","Recent research has developed a theoretical basis for providing software architects with better support for planning and carrying out major evolutions of software systems. However, these theoretical models have yet to be implemented in a complete, integrated tool. In this paper, we consider some of the challenges involved in developing such a tool as a plug-in to an existing architecture modeling framework. We present an experience report based on the development of a prototype architecture evolution tool as a plug-in to MagicDraw, a commercial UML tool. We find that there are many practical obstacles to developing tools for architecture evolution modeling as plug-ins, and we discuss some of the key factors that plug-in developers should weigh when considering frameworks. Â© 2013 IEEE.",,,,"3rd International Workshop on Developing Tools as Plug-Ins, TOPI 2013","21 May 2013 through 21 May 2013","San Francisco, CA",101037,"Conference Paper","Scopus"
"Towards accessible integration and deployment of formal tools and techniques","Lapets A., Skowyra R., Bestavros A., Kfoury A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889044958&doi=10.1109%2fTOPI.2013.6597189&partnerID=40&md5=a5a37817c18d29a14fbfe3c1a29c6f65",2013,"International Workshop on Developing Tools as Plug-Ins, TOPI","Computer science researchers in the programming languages and formal verification communities, among others, have produced a variety of automated assistance and verification tools and techniques for formal reasoning. While there have been notable successes in utilizing these tools on the development of safe and secure software and hardware, these leading-edge advances remain largely underutilized by large populations of potential users that may benefit from them. In particular, we consider researchers, instructors, students, and other end users that may benefit from instant feedback from lightweight modeling and verification capabilities when exploring system designs or formal arguments. We describe Aartifact, a supporting infrastructure that makes it possible to quickly and easily assemble interacting collections of small domain-specific languages, as well as translations between those languages and existing tools (e.g., Alloy, SPIN, Z3) and techniques (e.g., evaluation, type checking, congruence closure); the infrastructure also makes it possible to compile and deploy these translators in the form of a cloud-based web application with an interface that runs inside a standard browser. This makes more manageable the process of exposing a limited, domain-specific, and logistically accessible subset of the capabilities of existing tools and techniques to end users. This infrastructure can be viewed as a collection of modules for defining interfaces that turn third-party formal modeling and verification tools and techniques into plug-ins that can be integrated within web-based interactive formal reasoning environments. Â© 2013 IEEE.",,,,"3rd International Workshop on Developing Tools as Plug-Ins, TOPI 2013","21 May 2013 through 21 May 2013","San Francisco, CA",101037,"Conference Paper","Scopus"
"Developing a domain-specific plug-in for a modelling platform: The good, the bad, the ugly","Montrieux L., Yu Y., Wermelinger M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889042547&doi=10.1109%2fTOPI.2013.6597186&partnerID=40&md5=a4bfe5e8ceb9f08da6c0bf8191b740ce",2013,"International Workshop on Developing Tools as Plug-Ins, TOPI","Domain-Specific Modelling Languages (DSML) allow software engineers to use the techniques and tools of Model-Driven Engineering (MDE) to express, represent and analyse a particular domain. By defining DSMLs as UML profiles, i.e. domain-specific extensions of the UML metamodel, development time for DSMLs can be greatly reduced by extending existing UML tools. In this paper, we reflect on our own experience in building rbacUML, a DSML for Role-Based Access Control modelling and analysis, as a plugin for a UML modelling platform. We describe what motivated our choice, and discuss the advantages and drawbacks of using an existing platform to develop a DSML on top of UML and additional analysis tooling. Â© 2013 IEEE.","Eclipse; MDE; Modelling; OCL; Plugin; RBAC",,,"3rd International Workshop on Developing Tools as Plug-Ins, TOPI 2013","21 May 2013 through 21 May 2013","San Francisco, CA",101037,"Conference Paper","Scopus"
"Plugging in and into code bubbles","Reiss S.P.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864253101&doi=10.1109%2fTOPI.2012.6229811&partnerID=40&md5=0e73358c5afd444d087008dd98d75523",2012,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012 - Proceedings","Code Bubbles is an attempt to redefine the user interface for an integrated programming environment. As it represents a whole new user interface, implementing it as a plug-in is inherently difficult. We get around this difficulty by combining two different plug-in architectures, a standard one based on registrations and callbacks, and a message-based one that puts the plug-in at arms length and defines a narrower two-way interface. This paper describes both how we have implemented Code Bubbles as a plug-in to Eclipse and how Code Bubbles itself is implemented as a set of plugins representing the different aspects of the environment, using both traditional and message-based plug-in architectures as appropriate. Â© 2012 IEEE.","plug-in architectures; publish-subscribe",,,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012","3 June 2012 through 3 June 2012","Zurich",91298,"Conference Paper","Scopus"
"The EventB2Dafny Rodin plug-in","CataÃ±o N., Leino K.R.M., Rivera V.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864225584&doi=10.1109%2fTOPI.2012.6229810&partnerID=40&md5=863c851abc6ce9228d8bb6d38e666a79",2012,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012 - Proceedings","This paper presents a translation of Rodin proof-obligations into the input language of Dafny, and the implementation of the translation as the EventB2Dafny Rodin plug-in. Rodin is a platform that provides support for Event-B. The paper uses a simplified Event-B model for social-networking to illustrate the translation and to describe the generated Dafny model. EventB2Dafny supports the full Event-B syntax and its full source code is available online. Â© 2012 IEEE.","Boogie; Dafny; Event-B; Proof Obligations; Refinement Calculus; Rodin",,,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012","3 June 2012 through 3 June 2012","Zurich",91298,"Conference Paper","Scopus"
"Rumadai: A plug-in to record and replay client-side events of web sites with dynamic content","Yildiz A., Aktemur B., SÃ¶zer H.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864264432&doi=10.1109%2fTOPI.2012.6229819&partnerID=40&md5=e418a015dc06bb5f083f7680a0c7cf6c",2012,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012 - Proceedings","Reproducing user events when testing web pages is challenging because of the dynamic nature of the web content and potential dependency on third party content providers. We present Rumadai, a Visual Studio plug-in, that helps web programmers test web pages by recording and replaying clientside events. Rumadai injects code into web pages to be deployed at servers. The injected code, written in JavaScript, records user events as well as client-side dynamic content requests (e.g. via Ajax or Jsonp) and their responses. Recorded events and responses are then sent to a remote database via HTTP POST. Web page developers can query the saved client data, again using Rumadai seamlessly from Visual Studio, to replay all or a subset of events on a browser. Â© 2012 IEEE.","JavaScript; Plug-in; User event record&replay; Web testing",,,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012","3 June 2012 through 3 June 2012","Zurich",91298,"Conference Paper","Scopus"
"A reference architecture for integrated development and run-time environments","Tajalii H., MedvidoviÄ‡ N.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864272289&doi=10.1109%2fTOPI.2012.6229804&partnerID=40&md5=9a33c2448c830462698097f51343209a",2012,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012 - Proceedings","The need to support different stages of a software system's life-cycle in a software development environment has resulted in the emergence of plug-ins that can monitor and adapt the resulting system's run-time environment. In several recent self-adaptive systems, the existence of these plug-ins has resulted in the integration of the development and runtime environments. We introduce a novel reference architecture that captures the architectures of disparate development environments - from those, such as traditional development environments, that have no run-time plug-ins, to the ones, such as the recently emerging development environments, that are fully integrated with the run-time environments. This reference architecture aids the study and understanding of development environments. More importantly, adherence to the architecture improves the run-time availability of certain critical system facilities while reducing the resource consumption of the integrated development and run-time environments. Â© 2012 IEEE.","integrated development environments; life-cycle; Reference architecture; run-time environments; run-time plug-ins; self-adaptation; tools",,,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012","3 June 2012 through 3 June 2012","Zurich",91298,"Conference Paper","Scopus"
"Playing cupid: The IDE as a matchmaker for plug-ins","Schiller T.W., Lucia B.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864198413&doi=10.1109%2fTOPI.2012.6229805&partnerID=40&md5=4d92234a8d6782a5b8fd54e9116fff73",2012,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012 - Proceedings","We describe a composable, data-driven, plug-in ecosystem for IDEs. Inspired by Unix's and Windows Power-Shell's pipeline communication models, each plug-in declares data-driven capabilities. Developers can then seamlessly mix, match, and combine plug-in capabilities to produce new insight, without modifying the plug-ins. We formalize the architecture using the polymorphic lambda calculus, with special types for source and source locations; the type system prevents nonsensical plug-in combinations, and helps to inform the design of new tools and plug-ins. To illustrate the power of the formalism, we describe several synergies between existing plug-ins (and tools) made possible by the ecosystem. Â© 2012 IEEE.",,,,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012","3 June 2012 through 3 June 2012","Zurich",91298,"Conference Paper","Scopus"
"IBM software development kit for PowerLinux","Araujo R.F., Barboza D.H., Pontes O.B., Teixeira R.M., JoÃ£o R.S., Moschetta W.S., Durelli V.H.S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864231119&doi=10.1109%2fTOPI.2012.6229818&partnerID=40&md5=c6dd2341d4fb6635e93f056c00a81cd6",2012,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012 - Proceedings","Over the years, many software engineering activities have relied on the automated support afforded by tools. In order to maximize the benefits provided by tools, they are often retrofitted to pre-existing development environments that enable them to capitalize on facilities provided by compilers, debuggers, and profilers. Integrated Development Environments (IDEs), for instance, comprise a myriad of tightly-knit tools (i.e., plugins) designed to boost programmer productivity. Due to the advantages that such integrated environments have brought to the mainstream, they have become a de facto standard to implement complex software systems. Eclipse is one of the most widely used contemporary IDEs. In order to integrate a set of mainstream C/C++ development tools along with several in-house IBM tools, we have used Eclipse a as basis to bring together a SDK comprising the necessary tools to assist in the development and analysis of C/C++ programs for Power Architecture, namely, the IBMÂ® Software Development Kit for PowerLinuxâ„¢. In this paper, we describe the approach we used to port such tools into plugins, the main technical hurdles we ran into during the porting, the workarounds we used to deal with such issues, and some quirks of the target platform (i.e., Power machines). Â© 2012 IEEE.","C/C++ development; Eclipse; Linux Tools; plugins; Power Architecture; PTP",,,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012","3 June 2012 through 3 June 2012","Zurich",91298,"Conference Paper","Scopus"
"Simplicity principles for plug-in development: The jABC approach","Naujokat S., Lamprecht A.-L., Steffen B., JÃ¶rges S., Margaria T.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864218724&doi=10.1109%2fTOPI.2012.6229816&partnerID=40&md5=5916c4f1b82650238b2ef0e8564ee23c",2012,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012 - Proceedings","In this paper we present our experiences from a decade of plug-in development in the jABC framework, that is characterized by rigorous application of simplicity principles in two dimensions. First, the scope of the plug-in development is clearly defined: The jABC readily provides a sophisticated graphical user interface, which has been tailored to working with all kinds of directed graphs. Within this scope, plug-in development can deliberately focus on the actual functionality, like providing semantics to graphs, without having to deal with tedious but semantically irrelevant issues like user interfaces. Second, plug-in functionality can be itself conveniently modeled as a workflow within the jABC. We illustrate our approach by means of two mature plug-ins: Genesys, a plug-in that adds arbitrary code generator functionality to the jABC, and PROPHETS, a plug-in that eases user-level definition of workflows by completing model sketches by means of synthesis capabilities, so that they become complete and executable. We summarize our experience so far and derive general design principles for ""lightweight plug-in development"", that we are going to realize in the next generation of the jABC, which will be implemented itself as a collection of Eclipse plug-ins. Â© 2012 IEEE.",,,,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012","3 June 2012 through 3 June 2012","Zurich",91298,"Conference Paper","Scopus"
"IDEs need become open data platforms (as need languages and VMs)","Kuhn A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864258165&doi=10.1109%2fTOPI.2012.6229807&partnerID=40&md5=897b949930f4998ffd86d91978a10e69",2012,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012 - Proceedings","Development environments have put quite some effort into being open platforms, however rewriting our own tools as plug-ins we found that while host platform and other plug-ins are open for addition of feature they are typically closed to accessing data. We report from our own experience rewriting the Codemap tool as an Eclipse plug-in as well as from maintaining the Moose platform and argue why IDEs need to adopt an Open Data paradigm to become valuable targets for development of novel tools. We propose that data created by plug-ins need to default to being openly shared with other plug-ins through uniform data structures. Adding appropriate meta-information to this setup will allow other plug-ins to even make use of this data in unanticipated ways. We further argue that opening up data should not stop at the development platform but that programing languages and even virtual machines should also open up their internal data for the greater benefit of development tool and API designers. Â© 2012 IEEE.","Development Tool Design; Experience Report; Integrated Development Environment (IDE); Lessons Learned; Open Data; Programing Languages; Virtual Machines",,,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012","3 June 2012 through 3 June 2012","Zurich",91298,"Conference Paper","Scopus"
"XML development with plug-ins as a service","Karus S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864249071&doi=10.1109%2fTOPI.2012.6229806&partnerID=40&md5=fb5dc8aee1a67899f60daf5c00ee9866",2012,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012 - Proceedings","XML has quickly become a mainstream language in software development. Not only is it used for message and document interchange, it is also used to define application logic and interfaces. However, modern general purpose integrated development environments have rather limited support for XML development. The wide variety of XML based languages makes it a challenge to build tools for comprehensive support of XML development. In this paper, we present a library exposed as an add-in for Microsoft Visual Studio and a command line tool to improve the experience of editing XML files by providing access to subscribable service-based pluggable helper tools. The tools offer developers new means to check their XML against good and bad practices and possibly even automatically fix errors in XML or improve the files conformance with development guidelines. Â© 2012 IEEE.","automation; IDE; quality assurance; rules; services; Visual Studio; XML",,,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012","3 June 2012 through 3 June 2012","Zurich",91298,"Conference Paper","Scopus"
"A modular environment for software development and re-engineering","Campana S., Poli A., Spalazzi L., Spegni F.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864183007&doi=10.1109%2fTOPI.2012.6229820&partnerID=40&md5=8573d187be9f8f10bee2b1d2583efee6",2012,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012 - Proceedings","Traditional software development processes are designed to deal with the construction of new software systems. We believe the software development methodologies should include from the beginning the possibility of a re-engineering phase. With our work we identify the main characteristics that make software (re)engineering tools useful and usable Developing them in a modular architecture allows for a better integration with the developer's working habits. Â© 2012 IEEE.",,,,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012","3 June 2012 through 3 June 2012","Zurich",91298,"Conference Paper","Scopus"
"An architectural blueprint for a pluggable version control system for software (evolution) analysis","Ghezzi G., WÃ¼rsch M., Giger E., Gall H.C.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864258448&doi=10.1109%2fTOPI.2012.6229803&partnerID=40&md5=6a43608d46bdd79a3912aac9aa61f778",2012,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012 - Proceedings","Current version control systems are not built to be systematically analyzed. They have greatly evolved since their first appearance, but their focus has always been towards supporting developers in forward engineering activities. Supporting the analysis of the development history has so far been neglected. A plethora of third party applications have been built to fill this gap. To extract the data needed, they use interfaces that were not built for that. Drawing from our experience in mining and analyzing version control repositories, we propose an architectural blueprint for a plug-in based version control system in which analyses can be directly plugged into it in a flexible and lightweight way, to support both developers and analysts. We show the potential of this approach in three usage scenarios and we also give some examples for these analysis plug-ins. Â© 2012 IEEE.","mining software repositories; software evolution; version control systems",,,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012","3 June 2012 through 3 June 2012","Zurich",91298,"Conference Paper","Scopus"
"Integrating a set of contract checking tools into Visual Studio","FÃ¤hndrich M., Barnett M., Leijen D., Logozzo F.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864197260&doi=10.1109%2fTOPI.2012.6229809&partnerID=40&md5=47111fc226005c119c3d3f9b07d2e465",2012,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012 - Proceedings","Integrating tools and extensions into existing languages, compilers, debuggers, and IDEs can be difficult, work-intensive, and often results in a one-off integration. In this paper, we report on our experience of building and integrating the CodeContract tool set into an existing programming environment. The CodeContract tools enable 1) authoring of contracts (preconditions, postconditions, and object invariants), 2) instrumenting contract checks into code, 3) statically checking code against contracts, and 4) visualizing contracts and results. We identify three characteristics of our integration that allowed us to reuse existing compilers and IDEs, increase the reach of our tools to multiple languages and target platforms, and maintain the tools over three consecutive versions of C# and Visual Studio with little effort. These principles are 1) use source embedding for new language features, 2) use target analysis and rewriting, and 3) use generic plug-ins to isolate tools from the IDE. Â© 2012 IEEE.","contracts; IDE; plug-ins; tools",,,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012","3 June 2012 through 3 June 2012","Zurich",91298,"Conference Paper","Scopus"
"TacoPlug: An eclipse plug-in for TACO","Chicote M., Galeotti J.P.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864265939&doi=10.1109%2fTOPI.2012.6229808&partnerID=40&md5=8121a2b24d2acba490bc1432844308e8",2012,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012 - Proceedings","In this work we present TacoPlug, an Eclipse plugin that lets users explore error traces output by the bounded verifier TACO. TacoPlug uses and extends TACO to provide a better debugging experience. TacoPlug interface allows the user to verify an annotated software using the TACO verifier. If TACO finds a violation to the specification, TacoPlug presents it in terms of the annotated source code. TacoPlug features several views of the error trace to facilitate fault understanding. It resembles any software debugger, but the debugging occurs statically without executing the program. We show the usability of our tool by means of a motivational example taken from a real-life software error. Â© 2012 IEEE.","bounded verification; Eclipse plug-in; Static analysis; TACO",,,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012","3 June 2012 through 3 June 2012","Zurich",91298,"Conference Paper","Scopus"
"OLAP2DataCube: An Ontowiki plug-in for statistical data publishing","Salas P.E.R., Martin M., Da Mota F.M., Auer S., Breitman K.K., Casanova M.A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864194185&doi=10.1109%2fTOPI.2012.6229815&partnerID=40&md5=5a108d6c13ea836ba004f61a5f6906d8",2012,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012 - Proceedings","Statistical data is one of the most important sources of information, relevant for large numbers of stakeholders in the governmental, scientific and business domains alike. In this article, we introduce an Ontowiki plugin that extracts and publishes statistical data in RDF. We illustrate the plugin with a comprehensive use case reporting on the extraction and publishing on the Web of statistical data about 10 years of Brazilian government. Â© 2012 IEEE.",,,,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012","3 June 2012 through 3 June 2012","Zurich",91298,"Conference Paper","Scopus"
"Surfacing scientific and financial data with the Xcel2RDF plug-in","Pesce M.L., Breitman K.K., Casanova M.A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864254326&doi=10.1109%2fTOPI.2012.6229814&partnerID=40&md5=9c12622a7915affdc318dc87040e298d",2012,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012 - Proceedings","Given the astounding amount of data stored in spreadsheets and relational databases, a critical requirement for the evolution of the Semantic Web (SW) is the ability to convert data to SW compatible formats, such as RDF and OWL. The process by which data is transformed into RDF is known as triplification. This paper introduces Xcel2RDF, an MS Excel plug-in to support the triplification of spreadsheets, which minimizes the learning curve, as it is integrated into a widely used spreadsheet software tool. The plug-in is user-friendly, does not depend on the installation of additional software and does not require the user to leave his familiar environment, thereby avoiding problems reported as the major drawbacks of existing spreadsheet to RDF conversion tools. Finally, as a proof of concept, the paper illustrates how to use the tool to triplify statistical data. Â© 2012 IEEE.","Excel; Linked Open Data; RDF; Spreadsheet",,,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012","3 June 2012 through 3 June 2012","Zurich",91298,"Conference Paper","Scopus"
"Developing a plug-in tool to make OneNote an E-textbook","Cristy J., Tront J.G.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864265218&doi=10.1109%2fTOPI.2012.6229817&partnerID=40&md5=d5ee6e87e3250151e79dc1be7812b7ab",2012,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012 - Proceedings","E-textbooks encourage the user to perform all of the operations typically performed with a hardcopy text in addition to some functions not possible with paper books. This project works to implement an e-textbook with as much capability as possible using software tools that are already in place. Because Microsoft OneNote already implements some of the desired functions of e-textbooks, it is a suitable software foundation for this work. This paper will detail the implementation of the e-textbook plug-in for OneNote and Windows. Specifically, it will discuss the methods of harnessing the built-in capabilities of OneNote such as page content creation and modification, specialized controls creation, and interaction with other programs. Â© 2012 IEEE.","e-Textbook; Education; eTextbook; OneNote",,,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012","3 June 2012 through 3 June 2012","Zurich",91298,"Conference Paper","Scopus"
"SAML goes eclipse - Combining model-based safety analysis and high-level editor support","Lipaczewski M., Struck S., Ortmeier F.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864189566&doi=10.1109%2fTOPI.2012.6229813&partnerID=40&md5=685aed4b3c8eb074f2924048264e02ff",2012,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012 - Proceedings","Software-intensive systems become more and more important in safety critical applications, mainly because of the rising number and complexity of embedded system. Many traditional safety analysis techniques where developed decades ago and thus cannot cope with the complexity of modern systems. Model based analysis techniques where developed to deal with the complexity of software-intensive systems. However, due to the lack of tool support these techniques are currently limited to highly skilled experts. Thus model-based analysis is rarely used by system engineers. Based on the safety analysis modeling language (SAML) framework we propose the S 3E, which integrates a complete safety analysis environment into the eclipse platform. S 3E covers the whole safety analysis work flow. This implies a powerful editor for model creation, a seamless integration of model-analysis tools and presentation as well as evaluation of the analysis results into one environment. In this paper we present the current state of S 3E and first experiences with the eclipse plug-in development. Â© 2012 IEEE.",,,,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012","3 June 2012 through 3 June 2012","Zurich",91298,"Conference Paper","Scopus"
"SSELab: A plug-in-based framework for web-based project portals","Herrmann C., Kurpick T., Rumpe B.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864240959&doi=10.1109%2fTOPI.2012.6229812&partnerID=40&md5=5853f4ddb1eba8172335fd1b50cc5278",2012,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012 - Proceedings","Tools are an essential part of every software engineering project. But the number of tools that are used in all phases of the software development life-cycle and their complexity is growing continually. Consequently, the setup and maintenance of current tool chains and development environments requires much effort and consumes a lot of time. One approach to counter this, is to employ web-based systems for development tasks, because centralized systems simplify the administration and the deployment of new features. But desktop IDEs play an important role in software development projects today, and will not be replaced entirely by web-based environments in the near future. Therefore, supporting a mixture of hosted tools and tools integrated into desktop IDEs is a sensible approach. In this paper, we present the SSELab, a framework for web-based project portals that attempts to migrate more software development tools from desktop to server environments, but still allows their integration into modern desktop IDEs. It supports the deployment of tools as hosted services using plug-in systems on the server-side. Additionally, it provides access to these tools by a set of clients that can be used in different contexts, either from the command line, from within IDEs such as Eclipse, or from web pages. In the paper, we discuss the architecture and the extensibility of the SSELab framework. Furthermore, we share our experiences with creating an instance of the framework and integrating various tools for our own software development projects. Â© 2012 IEEE.",,,,"2012 2nd International Workshop on Developing Tools as Plug-Ins, TOPI 2012","3 June 2012 through 3 June 2012","Zurich",91298,"Conference Paper","Scopus"
"Platform support for developing testing and analysis plug-ins","Choudhary S.R., Duvall J., Jin W., Zhao D., Orso A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959905174&doi=10.1145%2f1984708.1984714&partnerID=40&md5=77a29b8dc4a4e8eaab786f471478f356",2011,"Proceedings - International Conference on Software Engineering","Plug-ins have become an important part of today's integrated development environments (IDEs). They are useful for extending the functionality of these environments and customizing them for different types of projects. In this paper, we discuss some features that should be provided by IDEs to support the development of a specific kind of plug-ins - plug-ins that support program analysis and software testing techniques. To guide the discussion, we leverage our experience in building a plug-in for two different platforms and generalize from that experience. Copyright 2011 ACM.","Integrated development environments; Plug-ins; Program analysis; Software testing","ACM SIGSOFT;IEEE CS",,"1st Workshop on Developing Tools as Plug-ins, TOPI 2011, Co-located with ICSE 2011","28 May 2011 through 28 May 2011","Waikiki, Honolulu, HI",85372,"Conference Paper","Scopus"
"ICE: Circumventing meltdown with an advanced binary analysis framework","Pucsek D., Wall J., Gibbs C., Baldwin J., Salois M., Coady Y.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959874568&doi=10.1145%2f1984708.1984731&partnerID=40&md5=d8e347a6606576af391976e398e96bda",2011,"Proceedings - International Conference on Software Engineering","In this paper we propose ICE, an Integrated Comprehension Environment, designed to facilitate advanced binary analysis through an extensible framework. ICE makes extensive use of modules and a flexible intermediate representation to enable seamless integration of instruction set architectures, platforms, and analysis techniques. Copyright 2011 ACM.","Extensibility; Frameworks; Intermediate languages","ACM SIGSOFT;IEEE CS",,"1st Workshop on Developing Tools as Plug-ins, TOPI 2011, Co-located with ICSE 2011","28 May 2011 through 28 May 2011","Waikiki, Honolulu, HI",85372,"Conference Paper","Scopus"
"IDE support to facilitate the transition from rapid prototyping to robust software production","Ortin F., Morant A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959896043&doi=10.1145%2f1984708.1984720&partnerID=40&md5=dfbbcdc95eaec3e81e556e562845725d",2011,"Proceedings - International Conference on Software Engineering","Dynamic languages are becoming increasingly popular for different software development scenarios such as rapid prototyping because of the flexibility and agile interactive development they offer. The benefits of dynamic languages are, however, counteracted by many limitations produced by the lack of static typing. In order to obtain the benefits of both approaches, some programming languages offer a hybrid dynamic and static type system. The existing IDEs for these hybrid typing languages do not provide any type-based feature when dynamic typing is used, lacking important IDE facilities offered for statically typed code. We have implemented a constraint-based type inference system that gathers type information of dynamic references at compile time. Using this type information, we have extended a professional IDE to offer those type-based features missed for dynamically typed code. Following the Separation of Concerns principle, the IDE has also been customized to facilitate the conversion of dynamically typed code into statically typed one, and vice versa. Copyright 2011 ACM.","Autocomplete; Hybrid dynamic and static typing; IDE support; Separation of concerns; Type inference","ACM SIGSOFT;IEEE CS",,"1st Workshop on Developing Tools as Plug-ins, TOPI 2011, Co-located with ICSE 2011","28 May 2011 through 28 May 2011","Waikiki, Honolulu, HI",85372,"Conference Paper","Scopus"
"AODVis: Leveraging eclipse plugins to reverse engineer and visualize aspectJ/java source code","Koch J., Bohra S., Goel R., Pagade S., Cooper K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959863224&doi=10.1145%2f1984708.1984716&partnerID=40&md5=9fc9ae473140ad27926366fd3dcdd8b3",2011,"Proceedings - International Conference on Software Engineering","AspectJ reverse engineering and visualization remains a challenge at the architectural and design levels, with fewer tools available for reverse engineers compared to other languages such as Java. As part of our AODVis (Aspect-Oriented Development Visualization) framework, we are developing Eclipse plugins to reverse-engineer and visualize AspectJ projects as 3D UML-based detailed design, architecture, and analysis level models. The AODVis plugins leverage several existing Eclipse plugins to extract program facts, create and manipulate UML, transform program facts into models, and generate visualizations of the models. Although integration can be challenging, the broad range of plugins are invaluable in providing solutions for extending and integrating existing tools such as compilers and IDEs. The plugins and the Eclipse plugin architecture also allowed us to concentrate on our specific research problem instead of developing our own tool support from scratch. Copyright 2011 ACM.","AspectJ; Eclipse; Plugin; Reverse engineering; Visualization","ACM SIGSOFT;IEEE CS",,"1st Workshop on Developing Tools as Plug-ins, TOPI 2011, Co-located with ICSE 2011","28 May 2011 through 28 May 2011","Waikiki, Honolulu, HI",85372,"Conference Paper","Scopus"
"Four Generic issues for tools-as-plugins illustrated by the distributed editor Saros","Prechelt L., Beecher K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959916412&doi=10.1145%2f1984708.1984712&partnerID=40&md5=0080b3772b2ea06bbbdb17767e60217d",2011,"Proceedings - International Conference on Software Engineering","Saros is an Eclipse plugin for multi-writer, real-time, distributed collaborative text editing that also includes VoIP, chat, whiteboard, and screen sharing functionality. We present four problematic issues we encountered in the development of Saros: Providing portability, choosing a metaphor, handling clashes in display markups, and attributing incompatibilities correctly to their source. These issues will apply to many other plugins similarly. For three of them, no generic solution approach yet exists but should be worked out. Copyright 2011 ACM.","Conflict; Incompatibility; Metaphor; Portability; Saros","ACM SIGSOFT;IEEE CS",,"1st Workshop on Developing Tools as Plug-ins, TOPI 2011, Co-located with ICSE 2011","28 May 2011 through 28 May 2011","Waikiki, Honolulu, HI",85372,"Conference Paper","Scopus"
"Architecting a plug-in based steam turbine design tool","Zachariadis S., Cianchi T.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959883326&doi=10.1145%2f1984708.1984726&partnerID=40&md5=1ea670e80285258f7e49e753b762b721",2011,"Proceedings - International Conference on Software Engineering","At a leading manufacturer of equipment for power generation, the engineers currently design a steam turbine, a key component of a power plant, using a large number of disjoint legacy tools written mostly in Fortran; These tools encapsulate significant engineering know how and are vital to the successful operation of the company. Their age and state pose a number of challenges, including difficulty in adapting to new methods, maintenance costs and lack of integration; the cost of replacing them all in one go however, has been deemed to be prohibitively expensive. In this experience report we describe the Turbine Design Tool (TDT), our approach in developing a plug-in based design tool that encapsulates and integrates the legacy tools into a single, component-based, extendable environment that offers the advantages of an integrated solution while minimising the cost and disruption to the business and that allows for the gradual replacement of the tools. Copyright 2011 ACM.","Dynamic graph; Eclipse RCP; OSGi; Turbine engineering","ACM SIGSOFT;IEEE CS",,"1st Workshop on Developing Tools as Plug-ins, TOPI 2011, Co-located with ICSE 2011","28 May 2011 through 28 May 2011","Waikiki, Honolulu, HI",85372,"Conference Paper","Scopus"
"A cloud-aware API for semi-structured BLOB databases addressing data overflow","Da Silveira Jr. J.V., Breitman K.K.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959909421&doi=10.1145%2f1984708.1984723&partnerID=40&md5=1e3260a4cabad9e5e6bbece8b218696d",2011,"Proceedings - International Conference on Software Engineering","Cloud computing is rapidly becoming an important platform for research in Software Engineering. Despite the vibe and huge literature on commercial cloud environments, there is, however, very little research on how to capture, model, design and implement new software applications that can make intelligent use of the cloud. In this paper we propose a new abstraction that explores a fundamental aspect of cloud systems - data elasticity. The Container Database (CDB) abstraction provides a cloudbased solution for scenarios where device local storage is not sufficient for manipulating data. To demonstrate the viability of the proposed approach we present an implementation of the CDB abstraction as an API designed to work with a Visual Studio 2010 plug-in, using Windows Azure Cloud services. Copyright 2011 ACM.","Cloud computing; Data elasticity; Databases; Metadata; Microsoft Azure; Plugin; Visual studio 2010","ACM SIGSOFT;IEEE CS",,"1st Workshop on Developing Tools as Plug-ins, TOPI 2011, Co-located with ICSE 2011","28 May 2011 through 28 May 2011","Waikiki, Honolulu, HI",85372,"Conference Paper","Scopus"
"An OpenGL-based eclipse plug-in for visual debugging","Riboira A., Abreu R., Rodrigues R.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959865267&doi=10.1145%2f1984708.1984729&partnerID=40&md5=1940ede1f7a0de63e1eb741c59839cba",2011,"Proceedings - International Conference on Software Engineering","Locating components which are responsible for observed failures is the most expensive, error-prone phase in the software development life cycle. We present an Eclipse Plug-in that aims to fill some of the automatic debugging tools gaps: the lack of a visualization tool that provides intuitive feedback about the defect distribution over the code base, and easy access to the faulty locations. Copyright 2011 ACM.","Diagnosis; Eclipse plug-in; Visual techniques","ACM SIGSOFT;IEEE CS",,"1st Workshop on Developing Tools as Plug-ins, TOPI 2011, Co-located with ICSE 2011","28 May 2011 through 28 May 2011","Waikiki, Honolulu, HI",85372,"Conference Paper","Scopus"
"eCLAIM - An eclipse plug-in for mobile MAS applications","Bruno D.S., Breitman K., El Fallah Seghrouchni A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959865835&doi=10.1145%2f1984708.1984730&partnerID=40&md5=318e7afe2b23db7bc3bf8a80712d1b8b",2011,"Proceedings - International Conference on Software Engineering","The eCLAIM plug-in provides a solid environement for design and implementation of Multi-Agent Systems (MAS). Based on the agent oriented language CLAIM [2], it eases the development of software agents that are at the same time mobile and intelligent. Copyright 2011 ACM.","Eclipse plug-in; Mobile agents; Multi-Agent systems","ACM SIGSOFT;IEEE CS",,"1st Workshop on Developing Tools as Plug-ins, TOPI 2011, Co-located with ICSE 2011","28 May 2011 through 28 May 2011","Waikiki, Honolulu, HI",85372,"Conference Paper","Scopus"
"Resource usage contracts for .NET","Tapicer J., Garbervetsky D., Rouaux M.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959883336&doi=10.1145%2f1984708.1984725&partnerID=40&md5=b9928ee3b61ade423b962272ddac30f6",2011,"Proceedings - International Conference on Software Engineering","CODE CONTRACTS [2] is a tool that allows the specification and verification of contracts (pre, post-condition, invariants) in all .NET based programming languages. RESOURCE CONTRACTS is an extension of this language to specify resource usage in .NET programs. The new annotations, initially focussed on dynamic memory, enable modular analysis of both memory consumption and lifetime properties. They are checked by relying on the own CODE CONTRACTS static verifier and a points-to analysis. This approach is implemented as a VISUAL STUDIO extension1, providing facilities such us autocompletion and verification at build time. Copyright 2011 ACM.","Resource usage annotations; Static verification","ACM SIGSOFT;IEEE CS",,"1st Workshop on Developing Tools as Plug-ins, TOPI 2011, Co-located with ICSE 2011","28 May 2011 through 28 May 2011","Waikiki, Honolulu, HI",85372,"Conference Paper","Scopus"
"Pest: From the lab to the classroom","De Caso G., Garbervetsky D., GorÃ­n D.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959913914&doi=10.1145%2f1984708.1984711&partnerID=40&md5=90bdea82ef7e0efafafabb42b8fd1cb2",2011,"Proceedings - International Conference on Software Engineering","Automated software verification is an active field of research which has made enormous progress both in theoretical and practical aspects. In recent years, an important effort has been put into applying these techniques on top of mainstream programming languages. These languages typically provide powerful features such as reflection, aliasing and polymorphism which are handy for practitioners but, in contrast, make verification a real challenge. The Pest programming language, on the other hand, was conceived with verifiability as one of its main design drivers. Although its main purpose is to serve as a test bed for new language features, its bare-bones syntax and strong support for annotations suggested early on in its development that it could also serve as a teaching tool for first-year undergraduate students. Developing an Eclipse plug-in for Pest proved to be both cost-effective and a key part to its adoption in the classroom. In this paper, we report on this experience. Copyright 2011 ACM.","Eclipse plug-in; Language design; Teaching; Verifiability","ACM SIGSOFT;IEEE CS",,"1st Workshop on Developing Tools as Plug-ins, TOPI 2011, Co-located with ICSE 2011","28 May 2011 through 28 May 2011","Waikiki, Honolulu, HI",85372,"Conference Paper","Scopus"
"Ginga-NCL architecture for plug-ins","Moreno M.F., Marinho R.S., Soares L.F.G.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959863522&doi=10.1145%2f1984708.1984713&partnerID=40&md5=1c2ef6a008a78c58715e414448626666",2011,"Proceedings - International Conference on Software Engineering","Ginga-NCL is the declarative environment of the Ginga middleware, an ITU-T Recommendation for IPTV services and ITU-R Recommendation for terrestrial digital TV. This paper discusses the two-way solution Ginga proposes for its plug-ins. Ginga defines an API that allows third party tools as NCL (the declarative language of Ginga) player's plug-ins for specific media-object type exhibition that composes a DTV application presentation in its whole. As NCL allows nested NCL applications, an NCL application itself acts as a plug-in of another parent NCL application, therefore obeying the same plug-in API previously mentioned. In general, the same NCL plug-in API can be used to allow applications specified in other languages to be embedded in NCL applications, as well as to allow NCL applications to be embedded in other presentation environments, in particular the Ginga-NCL environment. This two-way bridge is exemplified in this paper between NCL and HTML applications. Copyright 2011 ACM.","Digital TV; Ginga-NCL; Middleware; NCL; Plug-in","ACM SIGSOFT;IEEE CS",,"1st Workshop on Developing Tools as Plug-ins, TOPI 2011, Co-located with ICSE 2011","28 May 2011 through 28 May 2011","Waikiki, Honolulu, HI",85372,"Conference Paper","Scopus"
"Reconciling the 3-layer architectural style with the eclipse plug-in-based architecture","Ameller D., Collell O., Franch X.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959894235&doi=10.1145%2f1984708.1984715&partnerID=40&md5=e100b3c8264a46c96cc0bb7fde5ae2cc",2011,"Proceedings - International Conference on Software Engineering","Software architecture construction is the result of a complex decision-making process, in which competing alternatives need to be compared. For example, deciding between a web-based application or a plug-in-based application has a significant impact on the architecture, therefore in order to make the right choice all possible tradeoffs between them must be considered. Decisions need to be made in all architectural views, from the logical view in which architectural styles are chosen, to the development view in which types of modules are decided, to the deployment view where physical allocation is determined. In this paper we analyze the interactions between a 3-layer architecture at the logical view, and a plug-in-based development view implemented in Eclipse, focusing on the difficulties we overcome in a research project in order to make it work. Copyright 2011 ACM.","3-layer; Architectural knowledge; Plug-in development","ACM SIGSOFT;IEEE CS",,"1st Workshop on Developing Tools as Plug-ins, TOPI 2011, Co-located with ICSE 2011","28 May 2011 through 28 May 2011","Waikiki, Honolulu, HI",85372,"Conference Paper","Scopus"
"RDB2RDF plugin: Relational databases to RDF plugin for eclipse","Salas P.E., Marx E., Mera A., Viterbo J.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959893929&doi=10.1145%2f1984708.1984717&partnerID=40&md5=146fc8a1d285891c6683402a8fe428ef",2011,"Proceedings - International Conference on Software Engineering","RDB2RDF is the process by which a relational database schema (RDB) is transformed into a set of RDF triples. A major step in this process is deciding how to represent database schema concepts in terms of RDF classes and properties. This correlation is described in the RDB2RDF mapping file, which is used as the base for the generation of RDF triples. Most RDB2RDF engines today provide support to the mechanical process of transforming RDB to RDF, each with its own mapping language. Due to this fact, the W3C RDB2RDF Working Group has been working to standardize a language to map relational data to RDF called R2RML. Part of their efforts is directed to fostering the development of methods, tools and techniques to support standard RDB2RDF mapping strategies. In this paper, we introduce an Eclipse plug-in that supports the standard RDB to RDF Mapping Language (R2RML) to produce Direct Mappings in RDF. Copyright 2011 ACM.","Eclipse; RDB2RDF; RDF; Relational databases","ACM SIGSOFT;IEEE CS",,"1st Workshop on Developing Tools as Plug-ins, TOPI 2011, Co-located with ICSE 2011","28 May 2011 through 28 May 2011","Waikiki, Honolulu, HI",85372,"Conference Paper","Scopus"
"Contractor.NET: Inferring typestate properties to enrich code contracts","Zoppi E., Braberman V., De Caso G., Garbervetsky D., Uchitel S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959859908&doi=10.1145%2f1984708.1984721&partnerID=40&md5=35cce7356b3eca5592da385276ef2c32",2011,"Proceedings - International Conference on Software Engineering","In this work we present Contractor.NET, a Visual Studio extension that supports the construction of contract specifications with typestate information which can be used for verification of client code. Contractor.NET uses and extends Code Contracts to provide stronger contract specifications. It features a two step process. First, a class source code is analyzed to extract a finite state behavior model (in the form of a typestate) that is amenable to human-in-theloop validation and refinement. The second step is to augment the original contract specification for the input class with the inferred typestate information, therefore enabling the verification of client code. The inferred typestates are enabledness preserving: a level of abstraction that has been successfully used to validate software artifacts, assisting in the detection of a number of concerns in various case studies including specifications of Microsoft Server protocols. Copyright 2011 ACM.","Contract strengthening; Enabledness abstractions; Typestate inference","ACM SIGSOFT;IEEE CS",,"1st Workshop on Developing Tools as Plug-ins, TOPI 2011, Co-located with ICSE 2011","28 May 2011 through 28 May 2011","Waikiki, Honolulu, HI",85372,"Conference Paper","Scopus"
"Application management plug-ins through dynamically pluggable probes","Gama K., Pedraza G., LÃ©vÃªque T., Donsez D.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959913710&doi=10.1145%2f1984708.1984718&partnerID=40&md5=ec2b117db23e24a7e83d344ed83eadc3",2011,"Proceedings - International Conference on Software Engineering","It is widely recognized that applications need to be administered remotely. In general, application management and monitoring is supported by textual management consoles while graphical user interfaces specialized for their tasks are preferred by average users. Defining what must be monitored and what are the admin actions one wants to perform on an application cannot be defined during the application development due to the fact that these needs evolve after the application deployment as we cannot completely predict the execution environment such as available devices. This paper presents an architecture and the corresponding infrastructure that allow administrators to define what they want to monitor and manage and automate the discovery and deployment of corresponding probes and related management console graphical plug-ins. This work has been validated on two different application. Copyright 2011 ACM.","Management console; Monitoring; OSGi; Plug-in development","ACM SIGSOFT;IEEE CS",,"1st Workshop on Developing Tools as Plug-ins, TOPI 2011, Co-located with ICSE 2011","28 May 2011 through 28 May 2011","Waikiki, Honolulu, HI",85372,"Conference Paper","Scopus"
"Fishtail: From task context to source code examples","Sawadsky N., Murphy G.C.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959906163&doi=10.1145%2f1984708.1984722&partnerID=40&md5=92bb52aced35335a3923f07cc5b4e57d",2011,"Proceedings - International Conference on Software Engineering","Implementing software development tools as integrated development environment (IDE) plugins gives tools direct access to a range of useful representations of the program being created and can improve programmer efficiency. These benefits must be weighed against the effort to integrate the tool into the IDE, effort which may need to be repeated for each IDE targeted. In this paper, we introduce Fishtail, a prototype plugin for the Eclipse IDE, which assists programmers in discovering code examples and documentation on the web relevant to their current task. Fishtail uses a detailed history of programmer interactions with the source code to automatically determine relevant web resources. We describe the key factors that make it attractive to implement Fishtail as a plugin, and the requirements Fishtail imposes on the plugin/IDE interface. To reach a broader user base and understand how well our tool supports different programming styles and IDE architectures, we have recently begun investigating how to make a version of Fishtail available in the Visual Studio IDE. We outline some of the challenges we face in trying to reuse code from the original Eclipse plugin. Copyright 2011 ACM.","Degree-of-interest; Development environment; Example-centric development; Interaction history; Program views","ACM SIGSOFT;IEEE CS",,"1st Workshop on Developing Tools as Plug-ins, TOPI 2011, Co-located with ICSE 2011","28 May 2011 through 28 May 2011","Waikiki, Honolulu, HI",85372,"Conference Paper","Scopus"
"OthelloPlay - A plug-in based tool for requirement formalization and validation","Cavada R., Cimatti A., Micheli A., Roveri M., Susi A., Tonetta S.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959876567&doi=10.1145%2f1984708.1984728&partnerID=40&md5=61e9b8b7547a7aed3daebca4ed282b2a",2011,"Proceedings - International Conference on Software Engineering","Requirement engineering is one of the most important phases in the development process of software and systems. In safety-critical applications, it is important to support the validation of the requirements with formal techniques to identify and remove flaws. However, requirements are often written in textual documents and their formalization and validation is not trivial for non-experts in formal methods. The goal of the OTHELLOPLAY tool is to support formalization of textual requirements and to simplify the use of formal techniques for requirements validation. The tool combines a formal verification engine and the Microsoft WordÂ® editor in a single and consistent environment. A fundamental key in our design approach is a plug-in-based architecture, which uses the Python language in conjunction with a Microsoft WordÂ® Add-In. The user can jump between textual requirements in the Microsoft WordÂ® editor and the corresponding formal requirements model. Copyright 2011 ACM.","Requirements formalization and validation; Tools as plug-ins","ACM SIGSOFT;IEEE CS",,"1st Workshop on Developing Tools as Plug-ins, TOPI 2011, Co-located with ICSE 2011","28 May 2011 through 28 May 2011","Waikiki, Honolulu, HI",85372,"Conference Paper","Scopus"
"Supporting plug-in mashes to ease tool integration","Mariani L., Pastore F.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959896653&doi=10.1145%2f1984708.1984710&partnerID=40&md5=25859d02db11006b52f5f8d96860ec36",2011,"Proceedings - International Conference on Software Engineering","The majority of IDEs implement a concept of plug-in that nicely supports the integration of tools within the IDEs. Plug-ins dramatically simplify the structural integration of multiple tools, but provide little support to the design of the dynamic of the integration, which must be entirely coded by programmers from plug-ins' API. Manually integrating plug-ins is costly, complex and requires a deep understanding of the underlying environment. The implementation of tools as plug-ins and the integration of the results produced by different plug-ins are still difficult, expensive and error-prone activities. This paper presents the concepts of Task Based Plug-in (TB-plug-in) and workflow of TB-plug-ins. In our vision, IDE users must be able to execute plug-ins and integrate their results by designing workflows that can be persisted, executed and re-used in other workflows. We validated our idea by refactoring a set of Eclipse plug-ins for log-file analysis into TB-plug-ins, and designing several workflows that integrate plug-in tasks. We compared the effort necessary to implement these analyses from plug-ins with the effort necessary to design the workflows from TB-plug-ins. We discovered that workflows can be easily designed with little knowledge about the IDE and the plug-ins' API, save significant effort otherwise devoted to the implementation of additional plug-ins and glue-code, and produce analyses that can be quickly modified and reused. Copyright 2011 ACM.","IDE; Task-based plug-in; Tool integration; Workflows","ACM SIGSOFT;IEEE CS",,"1st Workshop on Developing Tools as Plug-ins, TOPI 2011, Co-located with ICSE 2011","28 May 2011 through 28 May 2011","Waikiki, Honolulu, HI",85372,"Conference Paper","Scopus"
"Towards subtyped program generation in F#","Aktemur B.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959916704&doi=10.1145%2f1984708.1984727&partnerID=40&md5=f7b2f0c20d3c7cf33002b90ee8931c1f",2011,"Proceedings - International Conference on Software Engineering","Program Generation is the technique of combining code fragments to construct a program. In this work we report on our progress to extend F# with program generation constructs. Our prototype implementation uses a translation that allows simulating program generators by regular programs. The translation enables fast implementation and experimentation. We state how a further extension with subtyping can be integrated by benefiting from the translation. Copyright 2011 ACM.","F#; Meta-programming; Program generation; Subtyping","ACM SIGSOFT;IEEE CS",,"1st Workshop on Developing Tools as Plug-ins, TOPI 2011, Co-located with ICSE 2011","28 May 2011 through 28 May 2011","Waikiki, Honolulu, HI",85372,"Conference Paper","Scopus"
"SRP-plugin: A strategic release planning plug-in for visual studio 2010","Mohebzada J.G., Ruhe G., Eberlein A.","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959907919&doi=10.1145%2f1984708.1984719&partnerID=40&md5=efdd7bb0e2f04e0ceb4eff6f8b521773",2011,"Proceedings - International Conference on Software Engineering","Strategic release planning (SRP) is a critical step in iterative software development. SRP involves the assignment of features or requirements to releases while considering hard and soft constraints, such as time, effort, quality or resources. ReleasePlannerâ„¢ is a web-based decision support tool that is based on a sound and rigorous formal strategic release planning model. In this paper we describe the integration of ReleasePlannerâ„¢ with Visual Studio in the form of a Visual Studio plug-in called SRPPlugin. The SRP-Plugin is used to demonstrate our hypothesis that tools implemented as plug-ins for widely used development platform (such as Visual Studio) help to increase efficiency of the development process. The plug-in augments the rich Visual Studio environment with advanced release planning capabilities which result in better release planning quality, increased productivity and enhanced communication among project stakeholders. The added value of providing systematic release planning as part of a development is not limited to release planning alone. We have outlined the future work to extend SRP-Plugin to include proactive decision making features such as utilization of sensitivity analysis and machine learning techniques to provide recommendations for the project manager during the challenging task of deciding which sets of features should be offered to whom, when, and why. Copyright 2011 ACM.","Application lifecycle management; Microsoft visual studio; Plug-in tools; Strategic release planning","ACM SIGSOFT;IEEE CS",,"1st Workshop on Developing Tools as Plug-ins, TOPI 2011, Co-located with ICSE 2011","28 May 2011 through 28 May 2011","Waikiki, Honolulu, HI",85372,"Conference Paper","Scopus"
